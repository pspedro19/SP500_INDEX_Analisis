"""
üåü PROCESADOR INTEGRADO: TES + EOE UNIVERSAL
Combina el procesamiento de Tasas Cero Cup√≥n TES y todos los √çndices de Confianza EOE
Mantiene la funcionalidad independiente de cada procesador
"""
import os, fnmatch
import pandas as pd
import numpy as np
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
import os
import re
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')


# =============================================================================
# SECCI√ìN 1: TES PROCESSOR - Tasas Cero Cup√≥n
# =============================================================================
import os
import logging
from datetime import datetime

# üß† Definir logging UTF-8 compatible (antes de usarlo)
def configurar_logging(log_file='logs/eoe_universal_processor.log'):
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # Limpiar handlers previos si existen
    if logger.hasHandlers():
        logger.handlers.clear()

    # Archivo con UTF-8
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    # Consola (no redirige stdout para evitar errores de cierre)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

# =============================================================================
# REEMPLAZA LA CLASE PIBProcessor EXISTENTE CON ESTA VERSI√ìN MEJORADA
# =============================================================================

class EconomiaProcessor:
    """
    üèóÔ∏è ECONOMIA PROCESSOR UNIFICADO - PIB + ISE
    
    ‚úÖ PROCESA AMBOS ARCHIVOS:
    - PIB: anexProduccionConstantesItrim2025.xlsx (Trimestral)
    - ISE: anexISE12actividadesmar2025.xlsx (Mensual)
    
    ‚úÖ GENERA SALIDA UNIFICADA:
    - Ambas series convertidas a frecuencia diaria
    - Forward-fill para completar d√≠as faltantes
    - Excel con m√∫ltiples hojas y estad√≠sticas
    
    üéØ ESTRUCTURA IDENTIFICADA:
    PIB: Fila 11=A√±os, Fila 12=Trimestres, Fila 28="Producto Interno Bruto"
    ISE: Fila 10=A√±os, Fila 11=Meses, Fila 28="Indicador de Seguimiento a la Econom√≠a"
    """
    
    # Mapeos de per√≠odos a fechas
    quarter_map = {
        'I': '01-01', 'II': '04-01', 'III': '07-01', 'IV': '10-01'
    }
    
    month_map = {
        'enero': '01-01', 'febrero': '02-01', 'marzo': '03-01',
        'abril': '04-01', 'mayo': '05-01', 'junio': '06-01',
        'julio': '07-01', 'agosto': '08-01', 'septiembre': '09-01',
        'octubre': '10-01', 'noviembre': '11-01', 'diciembre': '12-01'
    }

    def __init__(self, search_paths=None, log_file='logs/economia_processor.log'):
        """Inicializar procesador unificado - RUTAS CORREGIDAS"""
        # Usar exactamente las mismas rutas que PIBProcessor
        self.search_paths = search_paths or [
            '.', 'data', 'data/0_raw', 'downloads', 'raw_data',
            'Data', 'Data/0_raw', 'Data/0_raw/bond',  # ‚Üê AGREGAR ESTAS RUTAS
            './Data', './Data/0_raw', './Data/0_raw/bond'
        ]
        
        # Configurar logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('EconomiaProcessor')
        
        # Datos internos
        self.pib_df = None
        self.ise_df = None
        self.daily_index = None
        self.final_df = None
        self.stats = {}
        
        self.logger.info("=" * 80)
        self.logger.info("üèóÔ∏è ECONOMIA PROCESSOR UNIFICADO - PIB + ISE")
        self.logger.info(f"üìÖ Fecha de procesamiento: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"üîç Rutas de b√∫squeda: {self.search_paths}")
        self.logger.info("=" * 80)
        
    def find_files(self):
        """üîç Buscar ambos archivos autom√°ticamente - VERSI√ìN CORREGIDA"""
        files_found = {}
        
        # Patrones de b√∫squeda CORREGIDOS - usando los mismos que funcionan individualmente
        patterns = {
            'pib': [
                '*anexProduccionConstantes*trim*.xlsx',  # Patr√≥n m√°s amplio primero
                '*anex-ProduccionConstantes*trim*.xlsx', 
                'anexProduccionConstantesItrim2025.xlsx',  # Espec√≠fico despu√©s
                '*PIB*trim*.xlsx',
                '*Produccion*Constantes*.xlsx'
            ],
            'ise': [
                '*anexISE*actividades*.xlsx',  # Patr√≥n m√°s amplio
                '*anex-ISE*actividades*.xlsx',  # Con gui√≥n
                'anexISE12actividadesmar2025.xlsx',  # Espec√≠fico
                '*ISE*.xlsx'
            ]
        }
        
        self.logger.info("üîç Buscando archivos de econom√≠a...")
        
        for file_type, patterns_list in patterns.items():
            self.logger.info(f"   üîç Buscando {file_type.upper()}...")
            
            for root in self.search_paths:
                if not os.path.exists(root):
                    self.logger.debug(f"      ‚ö†Ô∏è Ruta no existe: {root}")
                    continue
                    
                self.logger.info(f"      üìÅ Explorando: {root}")
                
                for dp, _, files in os.walk(root):
                    for f in files:
                        for pat in patterns_list:
                            if fnmatch.fnmatch(f, pat):
                                full_path = os.path.join(dp, f)
                                files_found[file_type] = full_path
                                self.logger.info(f"‚úÖ {file_type.upper()} encontrado: {full_path}")
                                break
                        if file_type in files_found:
                            break
                    if file_type in files_found:
                        break
                if file_type in files_found:
                    break
            
            if file_type not in files_found:
                self.logger.warning(f"‚ö†Ô∏è {file_type.upper()} no encontrado en:")
                for root in self.search_paths:
                    self.logger.warning(f"   - {root}")
                self.logger.warning(f"   Patrones buscados: {patterns_list}")
        
        # Verificar que encontramos ambos
        missing = []
        if 'pib' not in files_found:
            missing.append('PIB')
        if 'ise' not in files_found:
            missing.append('ISE')
        
        if missing:
            self.logger.error(f"‚ùå No se encontraron: {', '.join(missing)}")
            
            # Mostrar archivos disponibles para debug
            self.logger.info("üîç Archivos disponibles en las rutas:")
            for root in self.search_paths:
                if os.path.exists(root):
                    for dp, _, files in os.walk(root):
                        excel_files = [f for f in files if f.endswith(('.xlsx', '.xls'))]
                        if excel_files:
                            self.logger.info(f"   üìÅ {dp}:")
                            for f in excel_files[:5]:  # Solo mostrar primeros 5
                                self.logger.info(f"      - {f}")
                            if len(excel_files) > 5:
                                self.logger.info(f"      ... y {len(excel_files)-5} m√°s")
            
            return None, None
        
        return files_found.get('pib'), files_found.get('ise')
        
        # Verificar que encontramos ambos
        missing = []
        if 'pib' not in files_found:
            missing.append('PIB (anexProduccionConstantesItrim2025.xlsx)')
        if 'ise' not in files_found:
            missing.append('ISE (anexISE12actividadesmar2025.xlsx)')
        
        if missing:
            self.logger.error(f"‚ùå No se encontraron: {', '.join(missing)}")
            return None, None
        
        return files_found.get('pib'), files_found.get('ise')

    def process_pib_file(self, file_path):
        """üìä Procesar archivo PIB (trimestral)"""
        self.logger.info(f"üìä Procesando PIB: {file_path}")
        
        try:
            # Leer archivo
            raw_df = pd.read_excel(file_path, sheet_name='Cuadro 1', header=None, dtype=object)
            
            # Extraer filas espec√≠ficas para PIB
            a√±os_row = raw_df.iloc[11]      # Fila 11: a√±os
            trimestres_row = raw_df.iloc[12] # Fila 12: trimestres
            pib_row = raw_df.iloc[28]       # Fila 28: PIB
            
            # Verificar concepto
            concepto = str(pib_row.iloc[2]).strip()
            if 'Producto Interno Bruto' not in concepto:
                self.logger.warning(f"‚ö†Ô∏è PIB - Concepto inesperado: '{concepto}'")
            
            # Extraer datos
            fechas_pib = []
            valores_pib = []
            a√±o_actual = None
            
            for col_idx in range(3, len(pib_row)):
                # Obtener a√±o
                a√±o_cell = a√±os_row.iloc[col_idx] if col_idx < len(a√±os_row) else None
                if pd.notna(a√±o_cell) and str(a√±o_cell).strip():
                    import re
                    a√±o_match = re.search(r'(\d{4})', str(a√±o_cell))
                    if a√±o_match:
                        a√±o_actual = int(a√±o_match.group(1))
                
                # Obtener trimestre
                trim_cell = trimestres_row.iloc[col_idx] if col_idx < len(trimestres_row) else None
                trim_str = str(trim_cell).strip() if pd.notna(trim_cell) else None
                
                # Obtener valor
                valor_cell = pib_row.iloc[col_idx] if col_idx < len(pib_row) else None
                
                if (a√±o_actual and trim_str in self.quarter_map and pd.notna(valor_cell)):
                    try:
                        valor_num = float(valor_cell)
                        fecha_str = f"{a√±o_actual}-{self.quarter_map[trim_str]}"
                        fecha_dt = pd.to_datetime(fecha_str)
                        
                        fechas_pib.append(fecha_dt)
                        valores_pib.append(valor_num)
                    except (ValueError, TypeError):
                        continue
            
            if len(fechas_pib) == 0:
                self.logger.error("‚ùå No se pudieron extraer datos del PIB")
                return False
            
            self.pib_df = pd.DataFrame({
                'fecha': fechas_pib,
                'pib': valores_pib
            }).sort_values('fecha').reset_index(drop=True)
            
            self.logger.info(f"‚úÖ PIB procesado: {len(self.pib_df)} trimestres")
            self.logger.info(f"   üìÖ Per√≠odo PIB: {self.pib_df['fecha'].min().strftime('%Y-%m-%d')} "
                           f"a {self.pib_df['fecha'].max().strftime('%Y-%m-%d')}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando PIB: {str(e)}")
            return False

    def process_ise_file(self, file_path):
        """üìä Procesar archivo ISE (mensual)"""
        self.logger.info(f"üìä Procesando ISE: {file_path}")
        
        try:
            # Leer archivo
            raw_df = pd.read_excel(file_path, sheet_name='Cuadro 1', header=None, dtype=object)
            
            # Extraer filas espec√≠ficas para ISE (estructura ligeramente diferente)
            a√±os_row = raw_df.iloc[10]      # Fila 10: a√±os (no 11 como PIB)
            meses_row = raw_df.iloc[11]     # Fila 11: meses
            ise_row = raw_df.iloc[28]       # Fila 28: ISE
            
            # Verificar concepto
            concepto = str(ise_row.iloc[2]).strip()
            if 'Indicador de Seguimiento' not in concepto:
                self.logger.warning(f"‚ö†Ô∏è ISE - Concepto inesperado: '{concepto}'")
            
            # Extraer datos
            fechas_ise = []
            valores_ise = []
            a√±o_actual = None
            
            for col_idx in range(3, len(ise_row)):
                # Obtener a√±o
                a√±o_cell = a√±os_row.iloc[col_idx] if col_idx < len(a√±os_row) else None
                if pd.notna(a√±o_cell) and str(a√±o_cell).strip():
                    import re
                    a√±o_match = re.search(r'(\d{4})', str(a√±o_cell))
                    if a√±o_match:
                        a√±o_actual = int(a√±o_match.group(1))
                
                # Obtener mes
                mes_cell = meses_row.iloc[col_idx] if col_idx < len(meses_row) else None
                mes_str = str(mes_cell).lower().strip() if pd.notna(mes_cell) else None
                
                # Obtener valor
                valor_cell = ise_row.iloc[col_idx] if col_idx < len(ise_row) else None
                
                if (a√±o_actual and mes_str in self.month_map and pd.notna(valor_cell)):
                    try:
                        valor_num = float(valor_cell)
                        fecha_str = f"{a√±o_actual}-{self.month_map[mes_str]}"
                        fecha_dt = pd.to_datetime(fecha_str)
                        
                        fechas_ise.append(fecha_dt)
                        valores_ise.append(valor_num)
                    except (ValueError, TypeError):
                        continue
            
            if len(fechas_ise) == 0:
                self.logger.error("‚ùå No se pudieron extraer datos del ISE")
                return False
            
            self.ise_df = pd.DataFrame({
                'fecha': fechas_ise,
                'ise': valores_ise
            }).sort_values('fecha').reset_index(drop=True)
            
            self.logger.info(f"‚úÖ ISE procesado: {len(self.ise_df)} meses")
            self.logger.info(f"   üìÖ Per√≠odo ISE: {self.ise_df['fecha'].min().strftime('%Y-%m-%d')} "
                           f"a {self.ise_df['fecha'].max().strftime('%Y-%m-%d')}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando ISE: {str(e)}")
            return False

    def generate_unified_daily_series(self):
        """üîó Combinar PIB + ISE en serie diaria unificada"""
        if self.pib_df is None or self.ise_df is None:
            self.logger.error("‚ùå Faltan datos PIB o ISE para combinar")
            return False
        
        self.logger.info("üîó Generando serie diaria unificada PIB + ISE...")
        
        # Determinar rango global de fechas
        min_date = min(self.pib_df['fecha'].min(), self.ise_df['fecha'].min())
        max_date = max(self.pib_df['fecha'].max(), self.ise_df['fecha'].max())
        
        # Generar √≠ndice diario
        self.daily_index = pd.DataFrame({
            'fecha': pd.date_range(start=min_date, end=max_date, freq='D')
        })
        
        self.logger.info(f"üìÖ Rango unificado: {min_date.strftime('%Y-%m-%d')} a {max_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"üìä D√≠as totales: {len(self.daily_index)}")
        
        # Combinar con merge_asof + forward fill
        combined = self.daily_index.copy()
        
        # Agregar PIB
        pib_daily = pd.merge_asof(combined, self.pib_df, on='fecha', direction='backward')
        combined['pib'] = pib_daily['pib'].ffill()
        
        # Agregar ISE
        ise_daily = pd.merge_asof(combined, self.ise_df, on='fecha', direction='backward')
        combined['ise'] = ise_daily['ise'].ffill()
        
        self.final_df = combined
        
        # Estad√≠sticas de cobertura
        pib_coverage = self.final_df['pib'].notna().sum()
        ise_coverage = self.final_df['ise'].notna().sum()
        total_days = len(self.final_df)
        
        self.logger.info(f"‚úÖ Serie unificada creada:")
        self.logger.info(f"   üìä PIB cobertura: {pib_coverage}/{total_days} d√≠as ({pib_coverage/total_days*100:.1f}%)")
        self.logger.info(f"   üìä ISE cobertura: {ise_coverage}/{total_days} d√≠as ({ise_coverage/total_days*100:.1f}%)")
        
        return True

    def save_unified_results(self, output_file='economia_integrada.xlsx'):
        """üíæ Guardar resultados unificados"""
        if self.final_df is None:
            self.logger.error("‚ùå No hay datos para guardar")
            return False
        
        try:
            self.logger.info(f"üíæ Guardando econom√≠a unificada en: {output_file}")
            
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # Hoja 1: Serie diaria unificada
                self.final_df.to_excel(writer, sheet_name='Economia_Diaria', index=False)
                
                # Hoja 2: PIB trimestral original
                if self.pib_df is not None:
                    self.pib_df.to_excel(writer, sheet_name='PIB_Trimestral', index=False)
                
                # Hoja 3: ISE mensual original
                if self.ise_df is not None:
                    self.ise_df.to_excel(writer, sheet_name='ISE_Mensual', index=False)
                
                # Hoja 4: Estad√≠sticas comparativas
                stats_data = [
                    ['M√©trica', 'PIB', 'ISE'],
                    ['Frecuencia original', 'Trimestral', 'Mensual'],
                    ['Total per√≠odos', len(self.pib_df) if self.pib_df is not None else 0, 
                     len(self.ise_df) if self.ise_df is not None else 0],
                    ['Fecha inicio', 
                     self.pib_df['fecha'].min().strftime('%Y-%m-%d') if self.pib_df is not None else 'N/A',
                     self.ise_df['fecha'].min().strftime('%Y-%m-%d') if self.ise_df is not None else 'N/A'],
                    ['Fecha fin',
                     self.pib_df['fecha'].max().strftime('%Y-%m-%d') if self.pib_df is not None else 'N/A',
                     self.ise_df['fecha'].max().strftime('%Y-%m-%d') if self.ise_df is not None else 'N/A'],
                    ['Valor promedio',
                     f"{self.pib_df['pib'].mean():,.0f}" if self.pib_df is not None else 'N/A',
                     f"{self.ise_df['ise'].mean():.1f}" if self.ise_df is not None else 'N/A'],
                    ['D√≠as diarios generados', len(self.final_df), len(self.final_df)],
                    ['Procesado en', datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 
                     datetime.now().strftime('%Y-%m-%d %H:%M:%S')]
                ]
                
                stats_df = pd.DataFrame(stats_data[1:], columns=stats_data[0])
                stats_df.to_excel(writer, sheet_name='Estadisticas_Comparativas', index=False)
            
            self.logger.info(f"‚úÖ Archivo guardado exitosamente: {output_file}")
            self.logger.info(f"üìä Hojas: Economia_Diaria, PIB_Trimestral, ISE_Mensual, Estadisticas_Comparativas")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando resultados: {str(e)}")
            return False

    def run(self, pib_file=None, ise_file=None, output_file='economia_integrada.xlsx'):
        """üöÄ Ejecutar procesamiento unificado completo"""
        start_time = datetime.now()
        
        self.logger.info("üöÄ Iniciando procesamiento unificado PIB + ISE...")
        
        # PASO 1: Encontrar archivos
        if pib_file is None or ise_file is None:
            pib_found, ise_found = self.find_files()
            pib_file = pib_file or pib_found
            ise_file = ise_file or ise_found
            
            if not pib_file or not ise_file:
                return False
        
        # PASO 2: Procesar PIB
        if not self.process_pib_file(pib_file):
            return False
        
        # PASO 3: Procesar ISE
        if not self.process_ise_file(ise_file):
            return False
        
        # PASO 4: Generar serie unificada
        if not self.generate_unified_daily_series():
            return False
        
        # PASO 5: Guardar resultados
        if not self.save_unified_results(output_file):
            return False
        
        # Resumen final
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("üéØ PROCESAMIENTO UNIFICADO COMPLETADO EXITOSAMENTE")
        self.logger.info("=" * 80)
        self.logger.info(f"‚è±Ô∏è  Tiempo total: {duration:.2f} segundos")
        self.logger.info(f"üìÅ PIB procesado: {pib_file}")
        self.logger.info(f"üìÅ ISE procesado: {ise_file}")
        self.logger.info(f"üìä Datos PIB: {len(self.pib_df)} trimestres")
        self.logger.info(f"üìä Datos ISE: {len(self.ise_df)} meses")
        self.logger.info(f"üìÖ Serie diaria: {len(self.final_df)} d√≠as")
        self.logger.info(f"üíæ Archivo guardado: {output_file}")
        self.logger.info("=" * 80)
        
        return True


# =============================================================================
# FUNCI√ìN DE UTILIDAD PARA EL SISTEMA PRINCIPAL
# =============================================================================

def procesar_economia_integrada(pib_file=None, ise_file=None, output_file='economia_integrada.xlsx',
                               search_paths=None, log_file='logs/economia_processor.log'):
    """
    üöÄ Funci√≥n principal para procesar PIB + ISE de forma unificada
    
    Args:
        pib_file (str, optional): Ruta al archivo PIB
        ise_file (str, optional): Ruta al archivo ISE  
        output_file (str): Archivo Excel de salida unificado
        search_paths (list, optional): Rutas de b√∫squeda
        log_file (str): Archivo de log
    
    Returns:
        bool: True si el procesamiento fue exitoso
        
    Ejemplo de uso:
        # B√∫squeda autom√°tica de ambos archivos
        success = procesar_economia_integrada()
        
        # Archivos espec√≠ficos
        success = procesar_economia_integrada(
            pib_file='anexProduccionConstantesItrim2025.xlsx',
            ise_file='anexISE12actividadesmar2025.xlsx'
        )
    """
    processor = EconomiaProcessor(search_paths, log_file)
    return processor.run(pib_file, ise_file, output_file)
# =============================================================================
# FUNCI√ìN DE UTILIDAD PARA EL SISTEMA PRINCIPAL
# =============================================================================

def procesar_pib_trimestral(file_path=None, output_file='pib_diario_procesado.xlsx',
                           search_paths=None, log_file='logs/pib_processor.log'):
    """
    üöÄ Funci√≥n principal para procesar el PIB trimestral
    
    Args:
        file_path (str, optional): Ruta espec√≠fica al archivo PIB
        output_file (str): Archivo Excel de salida
        search_paths (list, optional): Rutas de b√∫squeda personalizadas
        log_file (str): Archivo de log
    
    Returns:
        bool: True si el procesamiento fue exitoso
        
    Ejemplo de uso:
        # B√∫squeda autom√°tica
        success = procesar_pib_trimestral()
        
        # Archivo espec√≠fico
        success = procesar_pib_trimestral('data/anexProduccionConstantesItrim2025.xlsx')
        
        # Personalizado
        success = procesar_pib_trimestral(
            output_file='mi_pib_data.xlsx',
            search_paths=['downloads', 'data/raw']
        )
    """
    processor = PIBProcessor(search_paths, log_file)
    return processor.run(file_path, output_file)


# =============================================================================
# EJEMPLO DE USO
# =============================================================================



    # Opci√≥n 2: Archivo espec√≠fico (descomenta para probar)
    # print("\n2Ô∏è‚É£ Procesamiento de archivo espec√≠fico:")
    # success2 = procesar_pib_trimestral(
    #     file_path='anexProduccionConstantesItrim2025.xlsx',
    #     output_file='pib_custom.xlsx'
    # )

class RemesasProcessor:
    """
    Procesador de Excel de remesas: lee un archivo con columnas Fecha, Descripci√≥n y VALOR_REMESA,
    convierte la fecha, genera un √≠ndice diario, hace forward-fill y guarda resultados.
    """
    def __init__(self, log_file='logs/remesas_processor.log'):
        # Configurar logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('RemesasProcessor')
        self.df = None
        self.daily_index = None
        self.final_df = None

    def read_file(self, file_path):
        """
        Lee el Excel de remesas y normaliza nombres de columnas de forma din√°mica,
        soportando tanto archivos perfectamente tabulados como aquellos que vienen
        en una sola columna con tabuladores internos.
        """
        self.logger.info(f"üì• Leyendo archivo: {file_path}")

        # 1) Leer sin header para inspeccionar
        raw = pd.read_excel(file_path, header=None, sheet_name=0, dtype=str)

        # 2) Si solo hay UNA columna pero contiene tabuladores, expl√≥dela
        if raw.shape[1] == 1 and raw.iloc[:,0].str.contains('\t').any():
            exploded = raw[0].str.split('\t', expand=True)
            self.logger.info("üîç Detected single-column with tabs ‚Üí expanding")
        else:
            exploded = raw

        # 3) Encontrar la fila real de encabezado (donde aparezca 'Fecha')
        header_idx = None
        for i in range(min(10, len(exploded))):
            row = exploded.iloc[i].astype(str).str.lower()
            if 'fecha' in row.values:
                header_idx = i
                self.logger.info(f"‚úÖ Header encontrado en fila {i}")
                break
        if header_idx is None:
            header_idx = 0
            self.logger.warning("‚ö†Ô∏è No encontr√© fila con 'Fecha', usando fila 0 como header")

        # 4) Extraer encabezado y datos
        header = exploded.iloc[header_idx].astype(str).str.strip()
        df = exploded.iloc[header_idx+1 : ].copy().reset_index(drop=True)
        df.columns = header

        # 5) Mapeo din√°mico de columnas a nuestro est√°ndar
        col_map = {}
        for col in df.columns:
            low = col.lower()
            if 'fecha' in low:
                col_map[col] = 'fecha'
            elif 'descrip' in low:
                col_map[col] = 'descripcion'
            elif 'valor' in low or 'remesa' in low:
                col_map[col] = 'valor_remesa'

        df = df.rename(columns=col_map)

        # 6) Verificar que ahora tengamos las 3 columnas
        expected = ['fecha', 'descripcion', 'valor_remesa']
        missing = [c for c in expected if c not in df.columns]
        if missing:
            self.logger.error(f"‚ùå Faltan columnas tras mapeo: {missing}")
            raise KeyError(f"Faltan columnas: {missing}")

        # 7) Dejar solo las que nos interesan y asignar a self.df
        self.df = df[expected]
        self.logger.info(f"‚úÖ Columnas normalizadas: {self.df.columns.tolist()}")

    def convert_dates(self):
        """
        Convierte la columna 'fecha' de cadenas como 'MM/DD/YYYY hh:mm:ss.fff AM/PM' a datetime.date.
        """
        self.logger.info("üîÑ Convirtiendo fechas...")
        # Ejemplo de formato: '04/30/2025 12:00:00.000 AM'
        self.df['fecha'] = pd.to_datetime(
            self.df['fecha'],
            format='%m/%d/%Y %I:%M:%S.%f %p',
            errors='coerce'
        )
        nulldates = self.df['fecha'].isna().sum()
        if nulldates:
            self.logger.warning(f"‚ö†Ô∏è {nulldates} fechas no pudieron convertirse y ser√°n descartadas")
        # Eliminar filas sin fecha v√°lida
        self.df = self.df.dropna(subset=['fecha'])
        # Normalizar a fecha (sin hora)
        self.df['fecha'] = self.df['fecha'].dt.normalize()
        self.logger.info(f"‚úÖ Fechas convertidas. Rango: {self.df['fecha'].min().date()} a {self.df['fecha'].max().date()}")

    def process(self, file_path):
        """
        Ejecuta lectura y conversi√≥n de datos.
        """
        self.read_file(file_path)
        self.convert_dates()
        # Renombrar columnas a nombres m√°s amigables
        self.df = self.df.rename(columns={'descripci√≥n': 'descripcion', 'valor_remesa': 'valor_remesa'})
        # Ordenar por fecha
        self.df = self.df.sort_values('fecha').reset_index(drop=True)
        return self.df

    def generate_daily_index(self):
        """
        Genera un √≠ndice diario desde la fecha m√≠nima a la m√°xima.
        """
        if self.df is None:
            self.logger.error("‚ùå DataFrame no procesado a√∫n")
            return
        start = self.df['fecha'].min()
        end = self.df['fecha'].max()
        self.daily_index = pd.DataFrame({
            'fecha': pd.date_range(start=start, end=end, freq='D')
        })
        self.logger.info(f"üìÖ √çndice diario generado: {len(self.daily_index)} d√≠as desde {start.date()} hasta {end.date()}")
        return self.daily_index

    def create_daily_series(self):
        """
        Fusiona los datos originales con el √≠ndice diario y hace forward-fill de valores.
        """
        if self.daily_index is None:
            self.generate_daily_index()
        self.logger.info("üîó Combinando datos y aplicando forward-fill...")
        combined = pd.merge_asof(
            self.daily_index,
            self.df[['fecha', 'valor_remesa']],
            on='fecha',
            direction='backward'
        )
        combined['valor_remesa'] = combined['valor_remesa'].ffill()
        self.final_df = combined
        self.logger.info(f"‚úÖ Series diarias creadas: {len(self.final_df)} registros")
        return self.final_df

    def save_results(self, output_file='remesas_diarias.xlsx'):
        """
        Guarda el DataFrame final en Excel.
        """
        if self.final_df is None:
            self.logger.error("‚ùå No hay datos diarios generados para guardar")
            return False
        self.logger.info(f"üíæ Guardando resultados en: {output_file}")
        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
            self.final_df.to_excel(writer, sheet_name='DatosDiarios', index=False)
        self.logger.info("‚úÖ Archivo guardado exitosamente")
        return True

    def find_remesas_file(self, search_paths=None):
        """
        Busca autom√°ticamente un archivo de remesas en los directorios indicados.
        """
        if search_paths is None:
            search_paths = ['.', 'data', 'data/0_raw']
        patterns = ['*remesas*.xlsx', '*remesas*.xls']
        for root in search_paths:
            for dirpath, _, files in os.walk(root):
                for f in files:
                    for pat in patterns:
                        if fnmatch.fnmatch(f.lower(), pat.lower()):
                            full = os.path.join(dirpath, f)
                            self.logger.info(f"‚úÖ Archivo de remesas hallado: {full}")
                            return full
        self.logger.error("‚ùå No se encontr√≥ ning√∫n archivo de remesas autom√°ticamente")
        return None


    def run(self, file_path, output_file='remesas_diarias.xlsx'):
        """
        Ejecuta todo el proceso: lectura, conversi√≥n, fusi√≥n diaria y guardado.
        """
        self.logger.info("üöÄ Iniciando procesamiento de remesas...")

        if file_path is None:
            file_path = self.find_remesas_file()
            if file_path is None:
                 return False
        try:
            self.process(file_path)
            self.generate_daily_index()
            self.create_daily_series()
            success = self.save_results(output_file)
            self.logger.info("üéØ Procesamiento completado" if success else "‚ùå Error en guardado final")
            return success
        except Exception as e:
            self.logger.error(f"üí• Error en procesamiento: {e}")
            return False


import fnmatch

class IEDProcessor:
    """
    Procesa el Excel de IED trimestral y luego genera una serie diaria,
    rellenando hacia adelante (forward-fill) los valores faltantes.
    """
    quarter_map = {
        'Q1': '-01-01',
        'Q2': '-04-01',
        'Q3': '-07-01',
        'Q4': '-10-01',
    }

    def __init__(self, search_paths=None, log_file='logs/ied_processor.log'):
        self.search_paths = search_paths or ['data/0_raw', 'data/0_raw/bond', '.']
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('IEDProcessor')
        self.quarterly_df = None
        self.daily_index = None
        self.final_df = None

    def find_ied_file(self):
        patterns = ['*IED*.xls*', '*Inversion_Extranjera*.xls*']
        for root in self.search_paths:
            for dp, _, files in os.walk(root):
                for f in files:
                    for pat in patterns:
                        if fnmatch.fnmatch(f, pat):
                            full = os.path.join(dp, f)
                            self.logger.info(f"‚úÖ IED hallado: {full}")
                            return full
        self.logger.error("‚ùå No se encontr√≥ archivo IED autom√°ticamente")
        return None

    def read_and_normalize(self, file_path):
        """Lee el Excel, extrae fechas trimestrales y el valor de la √∫ltima columna."""
        self.logger.info(f"üì• Leyendo IED: {file_path}")

        # Leemos con doble header para capturar pa√≠s y r√≥tulos
        df2 = pd.read_excel(file_path, header=[2,3], dtype=object)

        # A√±o en la primera columna (puede venir NaN en filas de trimestre)
        years = pd.to_datetime(df2.iloc[:, 0], errors='coerce').dt.year

        # Trimestre en la segunda columna
        quarters = df2.iloc[:, 1].astype(str).str.strip().str.upper()

        # Valor en la √∫ltima columna
        values = pd.to_numeric(df2.iloc[:, -1], errors='coerce')

        temp = pd.DataFrame({
            'year': years,
            'quarter': quarters,
            'valor_ied': values
        })

        # Filtrar solo Q1‚ÄìQ4 y rellenar a√±os faltantes
        temp = temp[temp['quarter'].isin(['Q1','Q2','Q3','Q4'])].copy()
        temp['year'] = temp['year'].ffill().astype(int)

        # Construir fecha para el primer d√≠a de cada trimestre
        temp['fecha'] = pd.to_datetime(
            temp['year'].astype(str) +
            temp['quarter'].map(self.quarter_map)
        )

        self.quarterly_df = temp[['fecha', 'valor_ied']].sort_values('fecha').reset_index(drop=True)
        self.logger.info(f"‚úÖ IED trimestral normalizado: {len(self.quarterly_df)} registros")

    def generate_daily_index(self):
        """Genera un √≠ndice diario entre la primera y √∫ltima fecha trimestral."""
        if self.quarterly_df is None:
            self.logger.error("‚ùå No hay datos trimestrales para generar √≠ndice")
            return
        start = self.quarterly_df['fecha'].min()
        end   = self.quarterly_df['fecha'].max()
        self.daily_index = pd.DataFrame({'fecha': pd.date_range(start, end, freq='D')})
        self.logger.info(f"üìÖ √çndice diario generado: {len(self.daily_index)} d√≠as ({start.date()} ‚Üí {end.date()})")

    def create_daily_series(self):
        """Combina √≠ndice diario con trimestral usando merge_asof + forward-fill."""
        if self.daily_index is None:
            self.generate_daily_index()
        self.logger.info("üîó Combinando trimestral con diario y forward-fill...")
        df_daily = pd.merge_asof(
            self.daily_index,
            self.quarterly_df,
            on='fecha',
            direction='backward'
        )
        df_daily['valor_ied'] = df_daily['valor_ied'].ffill()
        self.final_df = df_daily
        self.logger.info(f"‚úÖ Serie diaria creada: {len(self.final_df)} registros")

    def save(self, output_file='ied_diario.xlsx'):
        """Guarda la serie diaria en Excel."""
        if self.final_df is None:
            self.logger.error("‚ùå No hay serie diaria para guardar")
            return False
        self.final_df.to_excel(output_file, index=False)
        self.logger.info(f"üíæ Guardado IED diario en: {output_file}")
        return True

    def run(self, file_path=None, output_file='ied_diario.xlsx'):
        """Ejecuta todo el flujo: detecta archivo, normaliza, crea serie diaria y guarda."""
        self.logger.info("üöÄ Iniciando procesamiento de IED (diario)...")
        fp = file_path or self.find_ied_file()
        if not fp:
            return False
        try:
            self.read_and_normalize(fp)
            self.generate_daily_index()
            self.create_daily_series()
            return self.save(output_file)
        except Exception as e:
            self.logger.error(f"‚ùå Error en IEDProcessor: {e}")
            return False
# processor.run('IngresosEgresos de remesas de trabajadores en Colombia.xlsx')

# üß© Clase con logger incorporado
class EOEUniversalProcessor:
    """
    üåü PROCESADOR UNIVERSAL EOE - Todos los √çndices de Confianza
    
    ‚úÖ DETECTA AUTOM√ÅTICAMENTE: ICI, ICCO, ICC, IEC, ICE y cualquier √≠ndice similar
    ‚úÖ MANEJA M√öLTIPLES VARIABLES: Archivos con 2, 3, 4+ columnas de datos
    ‚úÖ T√âRMINOS UNIVERSALES: "confianza", "expectativa", "opini√≥n empresarial"  
    ‚úÖ PATRONES FLEXIBLES: IC*, I*CO, cualquier sigla de 3-4 letras
    ‚úÖ M√öLTIPLES FORMATOS: Maneja diferentes layouts y estructuras
    ‚úÖ COMPATIBLE TOTAL: Funciona con toda la familia EOE
    """
    
        # Redefinir stdout en UTF-8 para consola


        # Configurar logging manualmente para usar UTF-8


    def __init__(self, data_root='data/0_raw', log_file='logs/eoe_universal_processor.log'):
        self.data_root = data_root
        self.logger = configurar_logging(log_file)
        self.global_min_date = None
        self.global_max_date = None
        self.daily_index = None
        self.processed_data = {}
        self.final_df = None
        self.stats = {}
        self.discovered_files = []
        
        # Mapeo de meses en espa√±ol a n√∫meros
        self.meses_map = {
            'ene': 1, 'feb': 2, 'mar': 3, 'abr': 4, 'may': 5, 'jun': 6,
            'jul': 7, 'ago': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dic': 12
        }
        
        # Patrones de detecci√≥n universales
        self.terminos_confianza = [
            'confianza', 'expectativa', 'opini√≥n empresarial', 'encuesta',
            'ici', 'icco', 'icc', 'iec', 'ice', 'eoe', 'consumidor', 'industrial', 'comercial'
        ]
        
        self.patrones_siglas = [
            r'\bici\b', r'\bicco\b', r'\bicc\b', r'\biec\b', r'\bice\b',
            r'\bi[a-z]{1,3}[co]?\b'  # Patr√≥n flexible para √≠ndices
        ]
        
        # Mapeo espec√≠fico de variables conocidas
        self.variable_mapping = {
            'icc': {'name': 'Indice_Confianza_Consumidor', 'sigla': 'ICC', 'type': 'consumer_confidence'},
            'iec': {'name': 'Indice_Expectativas_Consumidores', 'sigla': 'IEC', 'type': 'consumer_confidence'},
            'ice': {'name': 'Indice_Condiciones_Economicas', 'sigla': 'ICE', 'type': 'consumer_confidence'},
            'ici': {'name': 'Indice_Confianza_Industrial', 'sigla': 'ICI', 'type': 'business_confidence'},  
            'icco': {'name': 'Indice_Confianza_Comercial', 'sigla': 'ICCO', 'type': 'business_confidence'}
        }
        
        self.logger.info("=" * 80)
        self.logger.info("üåü EOE UNIVERSAL PROCESSOR - Todos los √çndices de Confianza")
        self.logger.info(f"Directorio de datos: {data_root}")
        self.logger.info(f"T√©rminos detectados: {', '.join(self.terminos_confianza)}")
        self.logger.info(f"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 80)

    def detect_eoe_universal_file(self, file_path):
        """üîç DETECCI√ìN UNIVERSAL: ¬øEs cualquier archivo EOE?"""
        try:
            df = pd.read_excel(file_path, sheet_name=0, header=None, nrows=20)
            indicators = 0
            
            # 1. Buscar t√©rminos de confianza UNIVERSALES en las primeras 5 filas
            for i in range(min(5, len(df))):
                for j in range(min(df.shape[1], 5)):  # Revisar m√°s columnas
                    if i < len(df) and j < df.shape[1]:
                        cell_value = str(df.iloc[i, j]).lower()
                        
                        # Buscar cualquier t√©rmino de confianza
                        for termino in self.terminos_confianza:
                            if termino in cell_value:
                                indicators += 2
                                self.logger.debug(f"Encontrado '{termino}' en celda [{i},{j}]")
                                break
            
            # 2. Buscar siglas de √≠ndices (ICI, ICCO, ICC, etc.)
            siglas_encontradas = []
            for i in range(min(5, len(df))):
                for j in range(min(df.shape[1], 5)):
                    if i < len(df) and j < df.shape[1]:
                        cell_value = str(df.iloc[i, j]).lower()
                        
                        for patron in self.patrones_siglas:
                            matches = re.findall(patron, cell_value)
                            if matches:
                                siglas_encontradas.extend(matches)
                                indicators += 1
            
            if siglas_encontradas:
                indicators += 3  # Bonus por encontrar siglas
                self.logger.debug(f"Siglas encontradas: {set(siglas_encontradas)}")
            
            # 3. Verificar patr√≥n de fechas mes-a√±o (CR√çTICO)
            fechas_validas = 0
            for i in range(2, min(25, len(df))):  # Revisar m√°s filas
                if i < len(df):
                    cell_value = str(df.iloc[i, 0]).lower().strip()
                    if re.match(r'^[a-z]{3}-\d{2}$', cell_value):
                        fechas_validas += 1
            
            if fechas_validas >= 3:
                indicators += 5  # Patr√≥n de fechas es CR√çTICO
                self.logger.debug(f"Patr√≥n de fechas v√°lido: {fechas_validas} fechas encontradas")
            
            # 4. Verificar valores num√©ricos en columnas de datos
            valores_numericos = 0
            for i in range(2, min(15, len(df))):
                if i < len(df):
                    for j in range(1, min(df.shape[1], 5)):  # Revisar m√∫ltiples columnas
                        try:
                            val = df.iloc[i, j]
                            if pd.notna(val) and isinstance(val, (int, float)):
                                valores_numericos += 1
                        except:
                            pass
            
            if valores_numericos >= 5:
                indicators += 3
                self.logger.debug(f"Valores num√©ricos v√°lidos: {valores_numericos}")
            
            # 5. Bonus por estructura t√≠pica EOE
            for i in range(min(3, len(df))):
                cell_content = str(df.iloc[i, 0]).lower()
                if 'eoe' in cell_content or 'encuesta' in cell_content or 'confianza' in cell_content:
                    indicators += 3
                    self.logger.debug("Estructura t√≠pica EOE detectada")
                    break
            
            self.logger.debug(f"Archivo {Path(file_path).name}: {indicators} indicadores EOE Universal")
            return indicators >= 10  # Umbral ajustado para mayor precisi√≥n
            
        except Exception as e:
            self.logger.warning(f"Error detectando archivo EOE Universal {file_path}: {e}")
            return False

    def extract_multiple_variables_info(self, file_path):
        """üìä EXTRACCI√ìN INTELIGENTE de m√∫ltiples variables de un archivo"""
        try:
            df = pd.read_excel(file_path, sheet_name=0, header=None, nrows=10)
            variables_found = []
            
            # Buscar fila con headers de variables (t√≠picamente fila 1 o 2)
            header_row = None
            for i in range(min(5, len(df))):
                row_content = [str(cell).lower() for cell in df.iloc[i].values if pd.notna(cell)]
                row_text = ' '.join(row_content)
                
                # Contar cu√°ntas siglas conocidas hay en esta fila
                siglas_count = 0
                for sigla in ['icc', 'iec', 'ice', 'ici', 'icco']:
                    if sigla in row_text:
                        siglas_count += 1
                
                if siglas_count >= 2:  # Si hay 2+ siglas, probablemente es el header
                    header_row = i
                    break
            
            if header_row is not None:
                self.logger.info(f"Header de variables encontrado en fila {header_row}")
                
                # Extraer variables de la fila header
                for j in range(1, min(df.shape[1], 6)):  # Columnas 1-5 (despu√©s de fechas)
                    if j < df.shape[1]:
                        cell_value = str(df.iloc[header_row, j]).lower()
                        
                        # Identificar variable espec√≠fica
                        variable_info = None
                        for sigla_key, info in self.variable_mapping.items():
                            if sigla_key in cell_value or info['sigla'].lower() in cell_value:
                                variable_info = {
                                    'column_index': j,
                                    'name': info['name'],
                                    'sigla': info['sigla'],
                                    'type': info['type'],
                                    'original_header': str(df.iloc[header_row, j])
                                }
                                variables_found.append(variable_info)
                                break
                        
                        # Si no se encontr√≥ espec√≠ficamente, crear variable gen√©rica
                        if variable_info is None and cell_value.strip() and '√≠ndice' in cell_value:
                            variable_info = {
                                'column_index': j,
                                'name': f'Indice_Confianza_Col{j}',
                                'sigla': f'IC{j}',
                                'type': 'business_confidence',
                                'original_header': str(df.iloc[header_row, j])
                            }
                            variables_found.append(variable_info)
            
            if not variables_found:
                # Fallback: asumir estructura est√°ndar
                self.logger.warning("No se encontraron headers espec√≠ficos, usando estructura por defecto")
                for j in range(1, min(df.shape[1], 4)):
                    variables_found.append({
                        'column_index': j,
                        'name': f'Indice_Confianza_Col{j}',
                        'sigla': f'IC{j}',
                        'type': 'business_confidence',
                        'original_header': f'Columna {j}'
                    })
            
            self.logger.info(f"Variables encontradas: {len(variables_found)}")
            for var in variables_found:
                self.logger.info(f"  - {var['sigla']}: {var['name']} (col {var['column_index']})")
            
            return variables_found, header_row
            
        except Exception as e:
            self.logger.warning(f"Error extrayendo info de variables m√∫ltiples: {e}")
            return [], None

    def auto_discover_files(self):
        """üîç DESCUBRIMIENTO AUTOM√ÅTICO de TODOS los archivos EOE"""
        self.logger.info("üîç Buscando TODOS los archivos EOE (Universal)...")
        
        discovered = []
        
        for root, dirs, files in os.walk(self.data_root):
            for file in files:
                if file.endswith(('.xlsx', '.xls')) and not file.startswith('~'):
                    file_path = os.path.join(root, file)
                    
                    if self.detect_eoe_universal_file(file_path):
                        variable_name = Path(file).stem
                        variables_info, header_row = self.extract_multiple_variables_info(file_path)
                        
                        # Si el archivo tiene m√∫ltiples variables, crear entrada para cada una
                        if len(variables_info) > 1:
                            for var_info in variables_info:
                                discovered.append({
                                    'Variable': f"{variable_name}_{var_info['sigla']}",
                                    'Archivo': file_path,
                                    'TARGET': var_info['name'],
                                    'Sigla': var_info['sigla'],
                                    'Tipo_Macro': var_info['type'],
                                    'Carpeta': os.path.basename(os.path.dirname(file_path)),
                                    'Column_Index': var_info['column_index'],
                                    'Header_Row': header_row,
                                    'Original_Header': var_info['original_header'],
                                    'Multi_Variable': True
                                })
                        else:
                            # Archivo con una sola variable (comportamiento original)
                            if variables_info:
                                var_info = variables_info[0]
                                discovered.append({
                                    'Variable': variable_name,
                                    'Archivo': file_path,
                                    'TARGET': var_info['name'],
                                    'Sigla': var_info['sigla'],
                                    'Tipo_Macro': var_info['type'],
                                    'Carpeta': os.path.basename(os.path.dirname(file_path)),
                                    'Column_Index': var_info['column_index'],
                                    'Header_Row': header_row,
                                    'Original_Header': var_info['original_header'],
                                    'Multi_Variable': False
                                })
        
        self.discovered_files = discovered
        self.logger.info(f"‚úÖ Encontrados {len(discovered)} variables en archivos EOE:")
        
        # Agrupar por archivo para mostrar mejor
        files_summary = {}
        for file_info in discovered:
            archivo = file_info['Archivo']
            if archivo not in files_summary:
                files_summary[archivo] = []
            files_summary[archivo].append(file_info)
        
        for archivo, variables in files_summary.items():
            file_name = Path(archivo).name
            self.logger.info(f"   üìÅ {file_name}:")
            for var in variables:
                self.logger.info(f"      ‚Üí {var['Sigla']}: {var['TARGET']} ({var['Tipo_Macro']})")
        
        return discovered

    def convert_eoe_date(self, date_str):
        """üìÖ Conversi√≥n de fechas EOE: "ene-80" ‚Üí 1980-01-01"""
        if pd.isna(date_str) or not isinstance(date_str, str):
            return None
        
        date_str = date_str.lower().strip()
        
        # Patr√≥n: mes-a√±o (eje: ene-80, feb-25)
        match = re.match(r'^([a-z]{3})-(\d{2})$', date_str)
        if not match:
            return None
        
        mes_str, a√±o_str = match.groups()
        
        if mes_str not in self.meses_map:
            return None
        
        mes = self.meses_map[mes_str]
        a√±o_2d = int(a√±o_str)
        
        # Convertir a√±o de 2 d√≠gitos a 4 d√≠gitos
        # Regla: 80-99 ‚Üí 1980-1999, 00-79 ‚Üí 2000-2079
        if a√±o_2d >= 80:
            a√±o = 1900 + a√±o_2d
        else:
            a√±o = 2000 + a√±o_2d
        
        try:
            return pd.Timestamp(year=a√±o, month=mes, day=1)
        except Exception:
            return None

    def convert_eoe_number(self, value):
        """üí∞ Conversi√≥n de n√∫meros EOE (maneja decimales y negativos)"""
        if pd.isna(value):
            return None
        if isinstance(value, (int, float)):
            # Validar rango l√≥gico para √≠ndices de confianza
            if -200 <= float(value) <= 200:  # Rango razonable para √≠ndices
                return float(value)
            else:
                self.logger.warning(f"Valor fuera de rango esperado: {value}")
                return float(value)  # Incluir de todas formas pero advertir
        if not isinstance(value, str):
            return None
            
        value = str(value).strip()
        if value in ['-', '', 'N/A', 'n/a', 'nan']:
            return None
            
        try:
            num_value = float(value)
            if -200 <= num_value <= 200:
                return num_value
            else:
                self.logger.warning(f"Valor fuera de rango esperado: {num_value}")
                return num_value
        except (ValueError, TypeError):
            return None

    def process_file_automatically(self, file_info):
        """‚ö° Procesamiento autom√°tico de archivo EOE Universal"""
        variable = file_info['Variable']
        target_col = file_info['TARGET']
        sigla = file_info['Sigla']
        macro_type = file_info['Tipo_Macro']
        file_path = file_info['Archivo']
        column_index = file_info['Column_Index']
        header_row = file_info.get('Header_Row', 1)
        
        self.logger.info(f"\nüåü Procesando EOE Universal: {variable}")
        self.logger.info(f"   üìÅ Carpeta: {file_info['Carpeta']}")
        self.logger.info(f"   üéØ TARGET: {target_col}")
        self.logger.info(f"   üè∑Ô∏è  Sigla: {sigla}")
        self.logger.info(f"   üìÇ Tipo: {macro_type}")
        self.logger.info(f"   üìä Columna: {column_index}")
        
        try:
            # Leer Excel completo
            df = pd.read_excel(file_path, sheet_name=0, header=None)
            self.logger.info(f"   üìä Archivo cargado: {len(df)} filas total")
            
            # Determinar fila donde empiezan los datos (despu√©s del header)
            data_start_row = header_row + 1 if header_row is not None else 2
            
            # Filtrar solo filas con datos v√°lidos
            valid_rows = []
            
            for idx in range(data_start_row, len(df)):
                row = df.iloc[idx]
                
                # Verificar si la primera columna tiene formato fecha mes-a√±o
                fecha_cell = str(row.iloc[0]).lower().strip()
                if re.match(r'^[a-z]{3}-\d{2}$', fecha_cell):
                    # Verificar que hay un valor num√©rico en la columna espec√≠fica
                    if len(row) > column_index and pd.notna(row.iloc[column_index]):
                        valid_rows.append({
                            'fecha_str': fecha_cell,
                            'valor': row.iloc[column_index]
                        })
            
            if len(valid_rows) == 0:
                self.logger.error(f"‚ùå No se encontraron datos v√°lidos en {variable}")
                return variable, None
            
            self.logger.info(f"   ‚úÖ {len(valid_rows)} registros v√°lidos encontrados")
            
            # Crear DataFrame resultado
            result_df = pd.DataFrame()
            
            # Convertir fechas y valores
            fechas_convertidas = []
            valores_convertidos = []
            
            for row_data in valid_rows:
                fecha_conv = self.convert_eoe_date(row_data['fecha_str'])
                valor_conv = self.convert_eoe_number(row_data['valor'])
                
                if fecha_conv is not None and valor_conv is not None:
                    fechas_convertidas.append(fecha_conv)
                    valores_convertidos.append(valor_conv)
            
            if len(fechas_convertidas) == 0:
                self.logger.error(f"‚ùå No se pudieron convertir datos v√°lidos en {variable}")
                return variable, None
            
            result_df['fecha'] = fechas_convertidas
            
            # Usar nombre original de la columna del Excel
            nombre_original = file_info.get('Original_Header', sigla)
            # Limpiar el nombre pero mantener la esencia original
            nuevo_nombre = str(nombre_original).strip()
            # Solo limpiar caracteres problem√°ticos pero mantener el texto original
            nuevo_nombre = re.sub(r'[\n\r\t]', ' ', nuevo_nombre)  # Reemplazar saltos de l√≠nea
            nuevo_nombre = re.sub(r'\s+', ' ', nuevo_nombre)  # Normalizar espacios
            nuevo_nombre = nuevo_nombre.strip()
            
            # Si el nombre est√° vac√≠o o es muy gen√©rico, usar la sigla
            if not nuevo_nombre or nuevo_nombre.lower() in ['', 'nan', 'none']:
                nuevo_nombre = sigla
                
            result_df[nuevo_nombre] = valores_convertidos
            
            # Ordenar por fecha
            result_df = result_df.sort_values('fecha').reset_index(drop=True)
            
            # Estad√≠sticas b√°sicas
            valores_stats = result_df[nuevo_nombre]
            self.logger.info(f"   üìà Estad√≠sticas: Min={valores_stats.min():.1f}, "
                          f"Max={valores_stats.max():.1f}, "
                          f"Media={valores_stats.mean():.1f}")
            
            # Actualizar fechas globales
            current_min = result_df['fecha'].min()
            current_max = result_df['fecha'].max()
            
            if self.global_min_date is None or current_min < self.global_min_date:
                self.global_min_date = current_min
            if self.global_max_date is None or current_max > self.global_max_date:
                self.global_max_date = current_max
            
            # Estad√≠sticas (formato compatible pero con nombre original)
            self.stats[variable] = {
                'macro_type': macro_type,
                'target_column': target_col,
                'sigla': sigla,
                'total_rows': len(result_df),
                'valid_values': result_df[nuevo_nombre].notna().sum(),
                'coverage': 100.0,
                'date_min': current_min,
                'date_max': current_max,
                'nombre_columna': nuevo_nombre,  # Nombre original para referencia
                'column_index': column_index,
                'original_header': file_info.get('Original_Header', ''),
                'min_value': valores_stats.min(),
                'max_value': valores_stats.max(),
                'mean_value': valores_stats.mean()
            }
            
            self.logger.info(f"   ‚úÖ {len(result_df)} valores procesados correctamente")
            self.logger.info(f"   üìÖ Per√≠odo: {current_min.strftime('%Y-%m-%d')} a {current_max.strftime('%Y-%m-%d')}")
            
            return variable, result_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando {variable}: {str(e)}")
            import traceback
            self.logger.error(f"   Traceback: {traceback.format_exc()}")
            return variable, None

    def generate_daily_index(self):
        """üìÖ Generar √≠ndice diario (compatible con sistema principal)"""
        if self.global_min_date is None or self.global_max_date is None:
            self.logger.error("‚ùå No se pudieron determinar fechas globales")
            return None
            
        self.daily_index = pd.DataFrame({
            'fecha': pd.date_range(start=self.global_min_date, end=self.global_max_date, freq='D')
        })
        
        self.logger.info(f"üìÖ √çndice diario generado: {len(self.daily_index)} d√≠as")
        self.logger.info(f"   üìÖ Desde: {self.global_min_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"   üìÖ Hasta: {self.global_max_date.strftime('%Y-%m-%d')}")
        return self.daily_index

    def combine_data(self):
        """üîó Combinar datos con merge_asof (compatible con sistema principal)"""
        if self.daily_index is None:
            self.logger.error("‚ùå El √≠ndice diario no ha sido generado")
            return None
            
        combined = self.daily_index.copy()
        
        for variable, df in self.processed_data.items():
            if df is None or df.empty:
                self.logger.warning(f"‚ö†Ô∏è Omitiendo {variable} por falta de datos")
                continue
                
            df = df.sort_values('fecha')
            col_name = self.stats[variable]['nombre_columna']  # Usar nombre original
            
            # merge_asof para asociar cada d√≠a con el valor reportado m√°s reciente
            df_daily = pd.merge_asof(combined[['fecha']], df, on='fecha', direction='backward')
            
            # Forward fill para llenar los d√≠as sin datos
            df_daily[col_name] = df_daily[col_name].ffill()
            
            # Agregar la columna al DataFrame combinado
            combined[col_name] = df_daily[col_name]
                                   
        self.final_df = combined
        self.logger.info(f"üîó Datos combinados: {len(self.final_df)} filas, {len(self.final_df.columns)} columnas")
        
        # Mostrar resumen de columnas
        columnas = [col for col in self.final_df.columns if col != 'fecha']
        self.logger.info(f"üìä Variables en dataset final:")
        for col in columnas:
            # Mostrar nombre original y sigla asociada
            sigla_asociada = "N/A"
            for var, stats in self.stats.items():
                if stats['nombre_columna'] == col:
                    sigla_asociada = stats['sigla']
                    break
            self.logger.info(f"   - {col} ({sigla_asociada})")
        
        return self.final_df

    def analyze_coverage(self):
        """üìä An√°lisis de cobertura (compatible con sistema principal)"""
        if not self.stats or self.daily_index is None:
            return
            
        total_days = len(self.daily_index)
        self.logger.info("\nüìä Resumen de Cobertura EOE Universal:")
        
        for variable, stats in self.stats.items():
            self.logger.info(f"- {variable} ({stats['sigla']}): {stats['coverage']:.1f}% "
                           f"desde {stats['date_min'].strftime('%Y-%m-%d')} "
                           f"a {stats['date_max'].strftime('%Y-%m-%d')} "
                           f"[{stats['min_value']:.1f} a {stats['max_value']:.1f}]")

    def save_results(self, output_file):
        """üíæ Guardar resultados (compatible con sistema principal)"""
        if self.final_df is None:
            self.logger.error("‚ùå No hay datos combinados para guardar")
            return False
            
        try:
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # Hoja 1: Datos Diarios
                self.final_df.to_excel(writer, sheet_name='Datos_Diarios', index=False)
                
                # Hoja 2: Estad√≠sticas detalladas
                df_stats = pd.DataFrame(self.stats).T
                df_stats.to_excel(writer, sheet_name='Estadisticas')
                
                # Hoja 3: Resumen por archivo
                archivos_summary = {}
                for file_info in self.discovered_files:
                    archivo = Path(file_info['Archivo']).name
                    if archivo not in archivos_summary:
                        archivos_summary[archivo] = []
                    archivos_summary[archivo].append(file_info)
                
                summary_data = []
                for archivo, variables in archivos_summary.items():
                    for var in variables:
                        summary_data.append({
                            'Archivo': archivo,
                            'Variable': var['Variable'],
                            'Sigla': var['Sigla'],
                            'Tipo': var['Tipo_Macro'],
                            'Columna': var['Column_Index'],
                            'Header_Original': var.get('Original_Header', ''),
                            'Multi_Variable': var.get('Multi_Variable', False)
                        })
                
                df_summary = pd.DataFrame(summary_data)
                df_summary.to_excel(writer, sheet_name='Resumen_Archivos', index=False)
                
                # Hoja 4: Metadatos del proceso
                meta = {
                    'Proceso': ['EOEUniversalProcessor_v2'],
                    'Fecha_Proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],
                    'Total_Variables': [len(self.stats)],
                    'Indices_Detectados': [', '.join([stats['sigla'] for stats in self.stats.values()])],
                    'Periodo_Global': [f"{self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}"],
                    'Total_Dias': [len(self.daily_index)],
                    'Archivos_Procesados': [len(set([info['Archivo'] for info in self.discovered_files]))],
                    'Variables_Multi_Columna': [sum(1 for info in self.discovered_files if info.get('Multi_Variable', False))]
                }
                pd.DataFrame(meta).to_excel(writer, sheet_name='Metadatos', index=False)
                
            self.logger.info(f"üíæ Archivo guardado exitosamente: {output_file}")
            self.logger.info(f"üìä Hojas creadas: Datos_Diarios, Estadisticas, Resumen_Archivos, Metadatos")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando resultados: {str(e)}")
            import traceback
            self.logger.error(f"   Traceback: {traceback.format_exc()}")
            return False

    def run(self, output_file):
        """üöÄ Ejecutar proceso completo EOE Universal"""
        start_time = time.time()
        
        self.logger.info("üöÄ Iniciando proceso EOE Universal...")
        
        # 1. Descubrir archivos EOE
        discovered = self.auto_discover_files()
        if not discovered:
            self.logger.error("‚ùå No se encontraron archivos EOE Universal")
            return False
        
        # 2. Procesar cada variable identificada
        self.logger.info(f"\n‚ö° Procesando {len(discovered)} variables...")
        successful_count = 0
        
        for file_info in discovered:
            var, df_processed = self.process_file_automatically(file_info)
            self.processed_data[var] = df_processed
            if df_processed is not None:
                successful_count += 1
        
        # 3. Verificar que se proces√≥ al menos una variable
        if successful_count == 0:
            self.logger.error("‚ùå No se proces√≥ ninguna variable correctamente")
            return False
        
        self.logger.info(f"‚úÖ {successful_count}/{len(discovered)} variables procesadas exitosamente")
        
        # 4. Generar √≠ndice diario
        self.logger.info("\nüìÖ Generando √≠ndice diario...")
        self.generate_daily_index()
        
        # 5. Combinar datos
        self.logger.info("\nüîó Combinando datos...")
        self.combine_data()
        
        # 6. Analizar cobertura
        self.analyze_coverage()
        
        # 7. Guardar resultados
        self.logger.info(f"\nüíæ Guardando resultados en {output_file}...")
        result = self.save_results(output_file)
        
        end_time = time.time()
        
        # Resumen final
        self.logger.info(f"\n" + "="*80)
        self.logger.info(f"‚è±Ô∏è RESUMEN DE EJECUCI√ìN EOE UNIVERSAL")
        self.logger.info(f"="*80)
        self.logger.info(f"‚è∞ Tiempo total: {end_time - start_time:.2f} segundos")
        self.logger.info(f"üìÅ Archivos √∫nicos procesados: {len(set([info['Archivo'] for info in self.discovered_files]))}")
        self.logger.info(f"üìä Variables procesadas: {successful_count}")
        self.logger.info(f"üìÖ Per√≠odo de datos: {self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}")
        
        if self.stats:
            self.logger.info(f"üè∑Ô∏è  √çndices detectados:")
            for var, stats in self.stats.items():
                self.logger.info(f"   - {stats['sigla']}: '{stats['nombre_columna']}' ({stats['total_rows']} registros)")
        
        self.logger.info(f"üíæ Archivo de salida: {output_file}")
        self.logger.info(f"üéØ Estado: {'‚úÖ COMPLETADO EXITOSAMENTE' if result else '‚ùå ERROR EN GUARDADO'}")
        self.logger.info(f"="*80)
        
        return result
        

class TESProcessor:
    """
    üè¶ PROCESADOR ESPEC√çFICO para Tasas Cero Cup√≥n TES
    
    Dise√±ado espec√≠ficamente para el archivo: "Tasas Cero Cup√≥n TES.csv"
    Extrae solo las 3 variables solicitadas:
    - TES pesos 1 a√±o
    - TES pesos 5 a√±os  
    - TES pesos 10 a√±os
    """
    
    def __init__(self, log_file='logs/tes_processor.log'):
        self.logger = self.configurar_logging(log_file)
        self.global_min_date = None
        self.global_max_date = None
        self.daily_index = None
        self.processed_data = None
        self.final_df = None
        self.stats = {}
        
        self.logger.info("=" * 80)
        self.logger.info("üè¶ TES PROCESSOR - Tasas Cero Cup√≥n Espec√≠fico")
        self.logger.info(f"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 80)

    def configurar_logging(self, log_file):
        """Configurar sistema de logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger('TESProcessor')

    def find_tes_file(self, search_paths=None):
        """
        üîç Buscar el archivo de TES en m√∫ltiples ubicaciones
        """
        if search_paths is None:
            search_paths = [
                'data/0_raw',
                'data/0_raw/bond',
                'data/raw',
                'raw_data',
                '.',
                'downloads'
            ]
        
        file_patterns = [
            'Tasas Cero Cup√≥n TES.csv',
            'tasas_cero_cupon_tes.csv',
            'TES*.csv',
            '*TES*.csv'
        ]
        
        self.logger.info("üîç Buscando archivo TES...")
        
        for search_path in search_paths:
            if not os.path.exists(search_path):
                continue
                
            self.logger.info(f"   Buscando en: {search_path}")
            
            # B√∫squeda exacta primero
            for pattern in file_patterns:
                files = list(Path(search_path).rglob(pattern))
                if files:
                    found_file = str(files[0])
                    self.logger.info(f"‚úÖ Archivo encontrado: {found_file}")
                    return found_file
        
        self.logger.error("‚ùå Archivo TES no encontrado")
        self.logger.info("üí° Ubicaciones buscadas:")
        for path in search_paths:
            self.logger.info(f"   - {path}")
        self.logger.info("üìù Patrones buscados:")
        for pattern in file_patterns:
            self.logger.info(f"   - {pattern}")
        
        return None

    def convert_date_tes(self, date_str):
        """
        üìÖ Convertir fechas del formato TES: 'MMM DD, AAAA'
        Ejemplos: 'Jan 15, 2023', 'Dec 31, 2024'
        """
        if pd.isna(date_str):
            return None
            
        if isinstance(date_str, (pd.Timestamp, datetime)):
            return pd.Timestamp(date_str)
        
        if not isinstance(date_str, str):
            return None
            
        date_str = str(date_str).strip()
        
        try:
            # Formato directo MMM DD, YYYY
            return pd.to_datetime(date_str, format='%b %d, %Y')
        except:
            try:
                # Formato alternativo con mes completo
                return pd.to_datetime(date_str, format='%B %d, %Y')
            except:
                try:
                    # Conversi√≥n gen√©rica
                    return pd.to_datetime(date_str)
                except:
                    self.logger.warning(f"‚ö†Ô∏è No se pudo convertir fecha: '{date_str}'")
                    return None

    def convert_number_tes(self, value):
        """
        üí∞ Convertir n√∫meros del formato TES
        """
        if pd.isna(value):
            return None
            
        if isinstance(value, (int, float)):
            return float(value)
        
        if not isinstance(value, str):
            return None
            
        value = str(value).strip()
        
        # Valores especiales
        if value.lower() in ['', 'n/a', 'na', '-', 'null']:
            return None
            
        try:
            # Remover caracteres especiales pero mantener punto y coma
            cleaned = re.sub(r'[^\d.,\-]', '', value)
            
            # Convertir formato colombiano si tiene comas
            if ',' in cleaned and '.' in cleaned:
                # Formato: 1.234,56 ‚Üí 1234.56
                cleaned = cleaned.replace('.', '').replace(',', '.')
            elif ',' in cleaned:
                # Solo comas: 1234,56 ‚Üí 1234.56
                cleaned = cleaned.replace(',', '.')
            
            return float(cleaned)
            
        except (ValueError, TypeError):
            self.logger.warning(f"‚ö†Ô∏è No se pudo convertir n√∫mero: '{value}'")
            return None

    def process_tes_file(self, file_path):
        """
        ‚öôÔ∏è Procesar el archivo TES espec√≠fico
        """
        self.logger.info(f"\nüìä Procesando archivo TES: {file_path}")
        
        try:
            # Leer el archivo CSV
            df = pd.read_csv(file_path)
            self.logger.info(f"   üìÅ Archivo cargado: {len(df)} filas, {len(df.columns)} columnas")
            
            # Mostrar columnas encontradas
            self.logger.info("   üìã Columnas encontradas:")
            for i, col in enumerate(df.columns):
                self.logger.info(f"      {i+1}. {col}")
            
            # Crear DataFrame resultado
            result_df = pd.DataFrame()
            
            # PASO 1: Procesar fechas
            date_column = df.columns[0]  # Primera columna es fecha
            self.logger.info(f"   üìÖ Procesando fechas de columna: '{date_column}'")
            
            result_df['fecha'] = df[date_column].apply(self.convert_date_tes)
            
            # Eliminar filas sin fecha v√°lida
            initial_rows = len(result_df)
            result_df = result_df.dropna(subset=['fecha'])
            valid_dates = len(result_df)
            
            self.logger.info(f"   ‚úÖ Fechas v√°lidas: {valid_dates}/{initial_rows}")
            
            if valid_dates == 0:
                self.logger.error("‚ùå No se encontraron fechas v√°lidas")
                return None
            
            # PASO 2: Mapear columnas solicitadas
            target_columns = {
                'tes_1_a√±o': {
                    'pattern': r'.*1\s*a√±o.*pesos.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), pesos - 1 a√±o',
                    'output_name': 'TES_pesos_1a√±o'
                },
                'tes_5_a√±os': {
                    'pattern': r'.*5\s*a√±os.*pesos.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), pesos - 5 a√±os',
                    'output_name': 'TES_pesos_5a√±os'
                },
                'tes_10_a√±os': {
                    'pattern': r'.*10\s*a√±os.*pesos.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), pesos - 10 a√±os',
                    'output_name': 'TES_pesos_10a√±os'
                },
                'tes_uvr_1_a√±o': {
                    'pattern': r'.*1\s*a√±o.*uvr.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), UVR - 1 a√±o',
                    'output_name': 'TES_uvr_1a√±o'
                },
                'tes_uvr_5_a√±os': {
                    'pattern': r'.*5\s*a√±os.*uvr.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), UVR - 5 a√±os',
                    'output_name': 'TES_uvr_5a√±os'
                },
                'tes_uvr_10_a√±os': {
                    'pattern': r'.*10\s*a√±os.*uvr.*',
                    'exact_match': 'Tasa de inter√©s Cero Cup√≥n, T√≠tulos de Tesorer√≠a (TES), UVR - 10 a√±os',
                    'output_name': 'TES_uvr_10a√±os'
                }
            }
            
            # PASO 3: Encontrar y procesar cada columna objetivo
            found_columns = {}
            
            for key, config in target_columns.items():
                column_found = None
                
                # Buscar coincidencia exacta primero
                if config['exact_match'] in df.columns:
                    column_found = config['exact_match']
                    self.logger.info(f"   ‚úÖ Coincidencia exacta: {key} ‚Üí '{column_found}'")
                else:
                    # Buscar con regex
                    pattern = re.compile(config['pattern'], re.IGNORECASE)
                    for col in df.columns:
                        if pattern.search(col):
                            column_found = col
                            self.logger.info(f"   ‚úÖ Coincidencia regex: {key} ‚Üí '{column_found}'")
                            break
                
                if column_found:
                    # Procesar valores num√©ricos
                    values = df[column_found].apply(self.convert_number_tes)
                    result_df[config['output_name']] = values
                    found_columns[key] = column_found
                    
                    # Estad√≠sticas
                    valid_values = values.notna().sum()
                    total_values = len(values)
                    self.logger.info(f"      üìä Valores v√°lidos: {valid_values}/{total_values} ({valid_values/total_values*100:.1f}%)")
                else:
                    self.logger.warning(f"   ‚ö†Ô∏è No encontrada: {key}")
            
            if not found_columns:
                self.logger.error("‚ùå No se encontraron columnas objetivo")
                return None
            
            # PASO 4: Limpiar datos finales
            # Eliminar filas donde TODAS las columnas TES son NaN
            tes_columns = [config['output_name'] for config in target_columns.values() 
                          if config['output_name'] in result_df.columns]
            
            if tes_columns:
                result_df = result_df.dropna(subset=tes_columns, how='all')
            
            # Ordenar por fecha
            result_df = result_df.sort_values('fecha').reset_index(drop=True)
            
            # PASO 5: Calcular estad√≠sticas
            self.global_min_date = result_df['fecha'].min()
            self.global_max_date = result_df['fecha'].max()
            
            self.stats = {
                'archivo_procesado': file_path,
                'filas_totales': len(result_df),
                'periodo_inicio': self.global_min_date,
                'periodo_fin': self.global_max_date,
                'columnas_encontradas': found_columns,
                'columnas_procesadas': tes_columns
            }
            
            self.logger.info(f"\n‚úÖ PROCESAMIENTO EXITOSO:")
            self.logger.info(f"   üìä Filas procesadas: {len(result_df)}")
            self.logger.info(f"   üìÖ Per√≠odo: {self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}")
            self.logger.info(f"   üìà Variables extra√≠das: {len(tes_columns)}")
  
            return result_df  # <-- Nota: cambi√© 'result' por 'result_df'
            
        except Exception as e:
            self.logger.error(f"‚ùå Error procesando archivo: {str(e)}")
            return None

    def generate_daily_index(self):
        """üìÖ Generar √≠ndice diario"""
        if self.global_min_date is None or self.global_max_date is None:
            self.logger.error("‚ùå No se pueden determinar fechas globales")
            return None
            
        self.daily_index = pd.DataFrame({
            'fecha': pd.date_range(start=self.global_min_date, end=self.global_max_date, freq='D')
        })
        
        self.logger.info(f"üìÖ √çndice diario generado: {len(self.daily_index)} d√≠as")
        return self.daily_index

    def create_daily_series(self):
        """üîó Crear series diarias con forward fill"""
        if self.processed_data is None or self.daily_index is None:
            self.logger.error("‚ùå Faltan datos procesados o √≠ndice diario")
            return None
            
        self.logger.info("üîó Creando series temporales diarias...")

        self.processed_data = self.processed_data.sort_values('fecha')
        combined = pd.merge_asof(self.daily_index, self.processed_data, on='fecha', direction='backward')

        tes_columns = [col for col in combined.columns if col.startswith('TES_')]
        for col in tes_columns:
            combined[col] = combined[col].ffill()

        self.final_df = combined
        self.logger.info(f"‚úÖ Series diarias creadas: {len(self.final_df)} d√≠as")

        return self.final_df

    def save_results(self, output_file='datos_tes_procesados.xlsx'):
        """üíæ Guardar resultados procesados"""
        if self.final_df is None:
            self.logger.error("‚ùå No hay datos para guardar")
            return False

        try:
            self.logger.info(f"üíæ Guardando resultados en: {output_file}")

            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                self.final_df.to_excel(writer, sheet_name='TES_Datos_Diarios', index=False)

                stats_df = pd.DataFrame([
                    ['Archivo procesado', self.stats['archivo_procesado']],
                    ['Filas totales', self.stats['filas_totales']],
                    ['Per√≠odo inicio', self.stats['periodo_inicio'].strftime('%Y-%m-%d')],
                    ['Per√≠odo fin', self.stats['periodo_fin'].strftime('%Y-%m-%d')],
                    ['D√≠as totales', len(self.daily_index)],
                    ['Variables extra√≠das', len(self.stats['columnas_procesadas'])]
                ], columns=['M√©trica', 'Valor'])
                stats_df.to_excel(writer, sheet_name='Estadisticas', index=False)

                mapping_data = []
                for key, original_name in self.stats['columnas_encontradas'].items():
                    mapping_data.append([key, original_name])
                mapping_df = pd.DataFrame(mapping_data, columns=['Variable_TES', 'Columna_Original'])
                mapping_df.to_excel(writer, sheet_name='Mapeo_Columnas', index=False)

                tes_cols = [col for col in self.final_df.columns if col.startswith('TES_')]
                self.logger.info(f"   üìä Columnas TES guardadas: {tes_cols}")

            self.logger.info(f"‚úÖ Archivo guardado exitosamente: {output_file}")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Error guardando resultados: {str(e)}")
            return False

    def run(self, file_path=None, output_file='datos_tes_procesados.xlsx'):
        """üöÄ Ejecutar procesamiento completo"""
        start_time = time.time()

        self.logger.info("üöÄ Iniciando procesamiento TES...")

        if file_path is None:
            file_path = self.find_tes_file()
            if file_path is None:
                return False

        self.processed_data = self.process_tes_file(file_path)
        if self.processed_data is None:
            return False

        if self.generate_daily_index() is None:
            return False

        if self.create_daily_series() is None:
            return False

        if not self.save_results(output_file):
            return False

        end_time = time.time()
        self.logger.info("\n" + "=" * 60)
        self.logger.info("üéØ PROCESAMIENTO COMPLETADO")
        self.logger.info("=" * 60)
        self.logger.info(f"‚è±Ô∏è  Tiempo total: {end_time - start_time:.2f} segundos")
        self.logger.info(f"üìÅ Archivo procesado: {file_path}")
        self.logger.info(f"üìä Variables extra√≠das: {self.stats['columnas_procesadas']}")
        self.logger.info(f"üìÖ Per√≠odo: {self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"üíæ Archivo guardado: {output_file}")
        self.logger.info("=" * 60)

        return True
       
        


# =============================================================================
# SECCI√ìN 3: FUNCIONES INTEGRADAS PARA EL SISTEMA PRINCIPAL
# =============================================================================

def procesar_tasas_tes(file_path=None, output_file='datos_tes_procesados.xlsx'):
    """
    üè¶ Funci√≥n principal para procesar Tasas Cero Cup√≥n TES
    
    Args:
        file_path (str, optional): Ruta espec√≠fica al archivo. Si es None, busca autom√°ticamente.
        output_file (str): Nombre del archivo Excel de salida.
    
    Returns:
        bool: True si el procesamiento fue exitoso, False en caso contrario.
    
    Ejemplo de uso:
        # B√∫squeda autom√°tica
        procesar_tasas_tes()
        
        # Archivo espec√≠fico
        procesar_tasas_tes('data/Tasas Cero Cup√≥n TES.csv')
        
        # Con archivo de salida personalizado
        procesar_tasas_tes(output_file='mi_tes_data.xlsx')
    """
    processor = TESProcessor()
    return processor.run(file_path, output_file)

def procesar_pib_trimestral(file_path=None, output_file='pib_diario_procesado.xlsx',
                           search_paths=None, log_file='logs/pib_processor.log'):
    """
    üöÄ Funci√≥n principal para procesar el PIB trimestral
    
    Args:
        file_path (str, optional): Ruta espec√≠fica al archivo PIB
        output_file (str): Archivo Excel de salida
        search_paths (list, optional): Rutas de b√∫squeda personalizadas
        log_file (str): Archivo de log
    
    Returns:
        bool: True si el procesamiento fue exitoso
        
    Ejemplo de uso:
        # B√∫squeda autom√°tica
        success = procesar_pib_trimestral()
        
        # Archivo espec√≠fico
        success = procesar_pib_trimestral('data/anexProduccionConstantesItrim2025.xlsx')
        
        # Personalizado
        success = procesar_pib_trimestral(
            output_file='mi_pib_data.xlsx',
            search_paths=['downloads', 'data/raw']
        )
    """
    processor = PIBProcessor(search_paths, log_file)
    return processor.run(file_path, output_file)

def ejecutar_eoe_universal_processor(
    config_file=None,  # No necesita configuraci√≥n externa
    output_file=None,
    data_root=None,
    log_file=None
):
    """
    üåü PROCESADOR EOE UNIVERSAL v2 - Todos los √çndices de Confianza
    Funci√≥n integrada compatible con el sistema principal
    
    ‚úÖ Detecta autom√°ticamente TODOS los archivos EOE
    ‚úÖ Maneja archivos con M√öLTIPLES VARIABLES (ICC, IEC, ICE en uno solo)
    ‚úÖ Convierte fechas "ene-80" ‚Üí 1980-01-01 autom√°ticamente  
    ‚úÖ Salta filas vac√≠as y headers autom√°ticamente
    ‚úÖ Valida rangos de valores para √≠ndices de confianza
    ‚úÖ Genera Excel con estad√≠sticas detalladas
    """
    
    # Configurar rutas por defecto si no se especifican
    if data_root is None:
        data_root = 'data/0_raw'  # Directorio por defecto
    
    if output_file is None:
        output_file = 'datos_eoe_universal_procesados.xlsx'
    
    if log_file is None:
        log_file = 'logs/eoe_universal_processor.log'
    
    # Crear directorios si no existen
    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)
    os.makedirs(os.path.dirname(log_file) if os.path.dirname(log_file) else '.', exist_ok=True)
    
    # Crear y ejecutar procesador
    processor = EOEUniversalProcessor(data_root, log_file)
    return processor.run(output_file)


def procesar_archivo_eoc_especifico(archivo_path, output_file=None):
    """
    üéØ FUNCI√ìN ESPEC√çFICA para procesar tu archivo EOC_Abril_2025_Hist√≥rico 1.xlsx
    
    Este archivo contiene 3 variables espec√≠ficas:
    - ICC: √çndice de Confianza del Consumidor
    - IEC: √çndice de Expectativas de los Consumidores  
    - ICE: √çndice de Condiciones Econ√≥micas
    """
    
    if output_file is None:
        output_file = 'EOC_procesado_3_variables.xlsx'
    
    log_file = 'logs/eoc_processor.log'
    
    # Configurar logging
    logger = configurar_logging_eoe(log_file)
    
    logger.info("üéØ PROCESAMIENTO ESPEC√çFICO ARCHIVO EOC")
    logger.info(f"üìÅ Archivo: {archivo_path}")
    logger.info(f"üìä Variables esperadas: ICC, IEC, ICE")
    logger.info("="*60)
    
    try:
        # Crear directorio temporal para simular estructura esperada
        temp_dir = 'temp_data'
        os.makedirs(temp_dir, exist_ok=True)
        
        # Copiar archivo al directorio temporal si es necesario
        import shutil
        if not os.path.exists(os.path.join(temp_dir, os.path.basename(archivo_path))):
            shutil.copy2(archivo_path, temp_dir)
        
        # Crear procesador apuntando al directorio temporal


        processor = EOEUniversalProcessor(temp_dir, log_file)
        
        # Ejecutar procesamiento
        result = processor.run(output_file)
        
        # Limpiar directorio temporal
        try:
            shutil.rmtree(temp_dir)
        except:
            pass
        
        if result:
            logger.info("‚úÖ Procesamiento espec√≠fico completado exitosamente")
            logger.info(f"üìä Archivo generado: {output_file}")
            
            # Mostrar informaci√≥n espec√≠fica de las 3 variables
            if processor.stats:
                logger.info("\nüìà RESUMEN DE LAS 3 VARIABLES:")
                for var, stats in processor.stats.items():
                    logger.info(f"   {stats['sigla']}: {stats['total_rows']} datos, "
                              f"rango [{stats['min_value']:.1f}, {stats['max_value']:.1f}], "
                              f"promedio {stats['mean_value']:.1f}")
        else:
            logger.error("‚ùå Error en procesamiento espec√≠fico")
        
        return result
        
    except Exception as e:
        logger.error(f"üí• Error cr√≠tico en procesamiento espec√≠fico: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return False


# =============================================================================
# SECCI√ìN 4: PROCESADOR INTEGRADO PRINCIPAL
# =============================================================================

class IntegratedProcessor:
    """
    üåü PROCESADOR INTEGRADO: TES + EOE UNIVERSAL
    Maneja ambos tipos de archivos de forma unificada
    """
    
    def __init__(self, log_file='logs/integrated_processor.log'):
        self.logger = self.configurar_logging(log_file)
        self.results = {
            'tes': None,
            'eoe': None
        }
        
        self.logger.info("=" * 80)
        self.logger.info("üåü PROCESADOR INTEGRADO: TES + EOE UNIVERSAL")
        self.logger.info(f"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info("=" * 80)
    
    def configurar_logging(self, log_file):
        """Configurar sistema de logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(log_file, mode='w'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger('IntegratedProcessor')
    
    def procesar_tes(self, file_path=None, output_file='datos_tes_procesados.xlsx'):
        """Procesar archivos TES"""
        self.logger.info("\nüè¶ INICIANDO PROCESAMIENTO TES")
        self.logger.info("-" * 60)
        
        result = procesar_tasas_tes(file_path, output_file)
        self.results['tes'] = result
        
        if result:
            self.logger.info("‚úÖ Procesamiento TES completado exitosamente")
        else:
            self.logger.error("‚ùå Error en procesamiento TES")
        
        return result
    
    def procesar_eoe(self, data_root='data/0_raw', output_file='datos_eoe_universal_procesados.xlsx'):
        """Procesar archivos EOE"""
        self.logger.info("\nüåü INICIANDO PROCESAMIENTO EOE UNIVERSAL")
        self.logger.info("-" * 60)
        
        result = ejecutar_eoe_universal_processor(
            data_root=data_root,
            output_file=output_file
        )
        self.results['eoe'] = result
        
        if result:
            self.logger.info("‚úÖ Procesamiento EOE completado exitosamente")
        else:
            self.logger.error("‚ùå Error en procesamiento EOE")
        
        return result
    
    def procesar_todo(self, 
                     tes_file=None, 
                     eoe_data_root='data/0_raw',
                     output_dir='output'):
        """
        Procesar tanto TES como EOE de forma integrada
        
        Args:
            tes_file: Ruta al archivo TES (None para b√∫squeda autom√°tica)
            eoe_data_root: Directorio ra√≠z para buscar archivos EOE
            output_dir: Directorio donde guardar los resultados
        """
        self.logger.info("\nüöÄ PROCESAMIENTO INTEGRADO COMPLETO")
        self.logger.info("=" * 80)
        
        # Crear directorio de salida
        os.makedirs(output_dir, exist_ok=True)
        
        # Procesar TES
        tes_output = os.path.join(output_dir, 'datos_tes_procesados.xlsx')
        tes_success = self.procesar_tes(tes_file, tes_output)
        
        # Procesar EOE
        eoe_output = os.path.join(output_dir, 'datos_eoe_universal_procesados.xlsx')
        eoe_success = self.procesar_eoe(eoe_data_root, eoe_output)
        
        # Resumen final
        self.logger.info("\n" + "=" * 80)
        self.logger.info("üìä RESUMEN DE PROCESAMIENTO INTEGRADO")
        self.logger.info("=" * 80)
        self.logger.info(f"TES: {'‚úÖ EXITOSO' if tes_success else '‚ùå ERROR'}")
        self.logger.info(f"EOE: {'‚úÖ EXITOSO' if eoe_success else '‚ùå ERROR'}")
        self.logger.info(f"Directorio de salida: {output_dir}")
        self.logger.info("=" * 80)
        
        return {
            'tes': tes_success,
            'eoe': eoe_success,
            'output_dir': output_dir
        }


# =============================================================================
# SECCI√ìN 5: FUNCIONES DE UTILIDAD Y MEN√ö PRINCIPAL
# =============================================================================

def mostrar_menu():
    """Mostrar men√∫ interactivo actualizado con Econom√≠a Integrada"""
    print("\n" + "=" * 80)
    print("üåü PROCESADOR INTEGRADO: TES + EOE + PIB + ISE")
    print("=" * 80)
    print("\nOpciones disponibles:")
    print("1. üè¶ Procesar solo TES (Tasas Cero Cup√≥n)")
    print("2. üåü Procesar solo EOE (√çndices de Confianza)")
    print("3. üèóÔ∏è Procesar solo PIB (Trimestral ‚Üí Diario)")
    print("4. üìà Procesar Econom√≠a Integrada (PIB + ISE)")  # ‚Üê NUEVA OPCI√ìN
    print("5. üöÄ Procesar TODO (TES + EOE + PIB + ISE)")
    print("6. üéØ Procesar archivo EOC espec√≠fico (ICC, IEC, ICE)")
    print("7. üí∏ Procesar Remesas + IED")
    print("8. ‚ùå Salir")
    print("-" * 80)
    
    while True:
        opcion = input("\nSeleccione una opci√≥n (1-8): ").strip()
        if opcion in ['1', '2', '3', '4', '5', '6', '7', '8']:
            return opcion
        else:
            print("‚ö†Ô∏è  Opci√≥n inv√°lida. Por favor, seleccione 1-8.")
# =============================================================================
# REEMPLAZA LA FUNCI√ìN ejecutar_opcion_menu() EXISTENTE CON ESTA VERSI√ìN
# =============================================================================

def ejecutar_opcion_menu(opcion):
    """Ejecutar la opci√≥n seleccionada del men√∫"""
    
    if opcion == '1':
        # Procesar solo TES
        print("\nüè¶ PROCESAMIENTO TES")
        print("-" * 60)
        
        archivo = input("Ruta al archivo TES (Enter para b√∫squeda autom√°tica): ").strip()
        if not archivo:
            archivo = None
        
        output = input("Archivo de salida (Enter para 'datos_tes_procesados.xlsx'): ").strip()
        if not output:
            output = 'datos_tes_procesados.xlsx'
        
        success = procesar_tasas_tes(archivo, output)
        
        if success:
            print(f"\n‚úÖ Procesamiento TES completado: {output}")
        else:
            print("\n‚ùå Error en procesamiento TES")
    
    elif opcion == '2':
        # Procesar solo EOE
        print("\nüåü PROCESAMIENTO EOE UNIVERSAL")
        print("-" * 60)
        
        data_root = input("Directorio de datos (Enter para 'data/0_raw'): ").strip()
        if not data_root:
            data_root = 'data/0_raw'
        
        output = input("Archivo de salida (Enter para 'datos_eoe_universal_procesados.xlsx'): ").strip()
        if not output:
            output = 'datos_eoe_universal_procesados.xlsx'
        
        success = ejecutar_eoe_universal_processor(data_root=data_root, output_file=output)
        
        if success:
            print(f"\n‚úÖ Procesamiento EOE completado: {output}")
        else:
            print("\n‚ùå Error en procesamiento EOE")
    
    elif opcion == '3':
        # Procesar solo PIB
        print("\nüèóÔ∏è PROCESAMIENTO PIB TRIMESTRAL")
        print("-" * 60)
        
        archivo = input("Ruta al archivo PIB (Enter para b√∫squeda autom√°tica): ").strip()
        if not archivo:
            archivo = None
        
        output = input("Archivo de salida (Enter para 'pib_diario_procesado.xlsx'): ").strip()
        if not output:
            output = 'pib_diario_procesado.xlsx'
        
        success = procesar_pib_trimestral(archivo, output)
        
        if success:
            print(f"\n‚úÖ Procesamiento PIB completado: {output}")
            print("üìä Serie diaria generada con forward-fill")
        else:
            print("\n‚ùå Error en procesamiento PIB")
    
    elif opcion == '4':
        # Procesar Econom√≠a Integrada (PIB + ISE) - NUEVA OPCI√ìN
        print("\nüìà PROCESAMIENTO ECONOM√çA INTEGRADA (PIB + ISE)")
        print("-" * 60)
        
        pib_file = input("Ruta al archivo PIB (Enter para b√∫squeda autom√°tica): ").strip()
        if not pib_file:
            pib_file = None
        
        ise_file = input("Ruta al archivo ISE (Enter para b√∫squeda autom√°tica): ").strip()
        if not ise_file:
            ise_file = None
        
        output = input("Archivo de salida (Enter para 'economia_integrada.xlsx'): ").strip()
        if not output:
            output = 'economia_integrada.xlsx'
        
        success = procesar_economia_integrada(pib_file, ise_file, output)
        
        if success:
            print(f"\n‚úÖ Procesamiento Econom√≠a Integrada completado: {output}")
            print("üìä PIB (trimestral) + ISE (mensual) ‚Üí Series diarias unificadas")
            print("üìÅ Hojas: Economia_Diaria, PIB_Trimestral, ISE_Mensual, Estadisticas")
        else:
            print("\n‚ùå Error en procesamiento de Econom√≠a Integrada")
    
    elif opcion == '5':
        # Procesar TODO - ACTUALIZAR PARA INCLUIR ECONOM√çA
        print("\nüöÄ PROCESAMIENTO COMPLETO (TES + EOE + ECONOM√çA)")
        print("-" * 60)
        
        processor = IntegratedProcessor()
        
        tes_file = input("Ruta al archivo TES (Enter para b√∫squeda autom√°tica): ").strip()
        if not tes_file:
            tes_file = None
        
        eoe_root = input("Directorio EOE (Enter para 'data/0_raw'): ").strip()
        if not eoe_root:
            eoe_root = 'data/0_raw'
        
        pib_file = input("Ruta al archivo PIB (Enter para b√∫squeda autom√°tica): ").strip()
        if not pib_file:
            pib_file = None
        
        ise_file = input("Ruta al archivo ISE (Enter para b√∫squeda autom√°tica): ").strip()
        if not ise_file:
            ise_file = None
        
        output_dir = input("Directorio de salida (Enter para 'output'): ").strip()
        if not output_dir:
            output_dir = 'output'
        
        # Crear directorio de salida
        os.makedirs(output_dir, exist_ok=True)
        
        # Procesar TES y EOE
        results = processor.procesar_todo(tes_file, eoe_root, output_dir)
        
        # Procesar Econom√≠a Integrada
        economia_output = os.path.join(output_dir, 'economia_integrada.xlsx')
        economia_success = procesar_economia_integrada(pib_file, ise_file, economia_output)
        
        # Mostrar resultados
        print(f"\nüìä Resultados guardados en: {output_dir}")
        print(f"TES: {'‚úÖ EXITOSO' if results['tes'] else '‚ùå ERROR'}")
        print(f"EOE: {'‚úÖ EXITOSO' if results['eoe'] else '‚ùå ERROR'}")
        print(f"ECONOM√çA (PIB+ISE): {'‚úÖ EXITOSO' if economia_success else '‚ùå ERROR'}")
        
        if economia_success:
            print("üìà Serie unificada PIB+ISE generada con √©xito")
    
    elif opcion == '6':
        # Procesar archivo EOC espec√≠fico
        print("\nüéØ PROCESAMIENTO ARCHIVO EOC ESPEC√çFICO")
        print("-" * 60)
        
        archivo = input("Ruta al archivo EOC: ").strip()
        
        if not archivo or not os.path.exists(archivo):
            print("‚ùå Archivo no encontrado")
            return
        
        output = input("Archivo de salida (Enter para 'EOC_procesado_3_variables.xlsx'): ").strip()
        if not output:
            output = 'EOC_procesado_3_variables.xlsx'
        
        success = procesar_archivo_eoc_especifico(archivo, output)
        
        if success:
            print(f"\n‚úÖ Procesamiento EOC completado: {output}")
            print("üìä Variables procesadas: ICC, IEC, ICE")
        else:
            print("\n‚ùå Error en procesamiento EOC")

    elif opcion == '7':
        # Procesar remesas + IED
        print("\nüí∏ PROCESAMIENTO REMESAS + IED")
        print("-" * 60)

        # remesas (auto-detect)
        proc_r = RemesasProcessor()
        ok_r   = proc_r.run(None, 'remesas_diarias.xlsx')

        # IED (auto-detect)
        proc_i = IEDProcessor()
        ok_i   = proc_i.run(None, 'ied_diario.xlsx')

        if ok_r and ok_i:
            print("\n‚úÖ Ambos procesos completados:")
            print("   ‚Ä¢ remesas_diarias.xlsx")
            print("   ‚Ä¢ ied_diario.xlsx")
        else:
            if not ok_r: print("‚ùå Error en remesas")
            if not ok_i: print("‚ùå Error en IED")

    elif opcion == '8':
        # Salir
        print("\nüëã ¬°Hasta luego!")
        return False  # Se√±al para salir del bucle principal
    
    return True  # Continuar en el men√∫
# =============================================================================
# PUNTO DE ENTRADA PRINCIPAL
# =============================================================================

if __name__ == "__main__":
    """
    üöÄ EJECUTAR PROCESADOR INTEGRADO
    """
    
    import sys

    
    # Verificar argumentos de l√≠nea de comandos
    if len(sys.argv) > 1:
        if sys.argv[1] == '--help':
            print("\nüåü PROCESADOR INTEGRADO: TES + EOE UNIVERSAL")
            print("=" * 80)
            print("\nUso:")
            print("  python integrated_processor.py              # Men√∫ interactivo")
            print("  python integrated_processor.py --tes        # Procesar solo TES")
            print("  python integrated_processor.py --eoe        # Procesar solo EOE")
            print("  python integrated_processor.py --all        # Procesar todo")
            print("  python integrated_processor.py --help       # Esta ayuda")
            print("\nEjemplos:")
            print("  python integrated_processor.py --tes archivo.csv")
            print("  python integrated_processor.py --eoe data/0_raw")
            print("  python integrated_processor.py --all")
            
        elif sys.argv[1] == '--tes':
            # Procesamiento directo TES
            file_path = sys.argv[2] if len(sys.argv) > 2 else None
            success = procesar_tasas_tes(file_path)
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == '--eoe':
            # Procesamiento directo EOE
            data_root = sys.argv[2] if len(sys.argv) > 2 else 'data/0_raw'
            success = ejecutar_eoe_universal_processor(data_root=data_root)
            sys.exit(0 if success else 1)
            
        elif sys.argv[1] == '--all':
            # Procesamiento completo
            processor = IntegratedProcessor()
            results = processor.procesar_todo()
            success = results['tes'] and results['eoe']
            sys.exit(0 if success else 1)

        elif sys.argv[1] == '--remesas':
            # Uso:
            #   python integrated_processor.py --remesas <archivo_excel> [<archivo_salida>]
            file_path = sys.argv[2] if len(sys.argv) > 2 else None
            output_file = sys.argv[3] if len(sys.argv) > 3 else 'remesas_diarias.xlsx'
            if not file_path:
                print("‚ùå Indica la ruta al Excel de remesas")
                sys.exit(1)
            proc = RemesasProcessor()
            ok = proc.run(file_path, output_file)
            sys.exit(0 if ok else 1)

        else:
            print(f"‚ùå Opci√≥n no reconocida: {sys.argv[1]}")
            print("Use --help para ver las opciones disponibles")
            sys.exit(1)

    else:
        # Modo interactivo con men√∫
        try:
            while True:
                opcion = mostrar_menu()
                
                if opcion == '6':
                    print("\nüëã ¬°Hasta luego!")
                    break
                
                ejecutar_opcion_menu(opcion)
                
                input("\nüìå Presione Enter para continuar...")
                
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Proceso interrumpido por el usuario")
        except Exception as e:
            print(f"\nüí• Error inesperado: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print("\nüéØ Procesamiento finalizado")
        print("=" * 80)
            

    def generate_daily_index(self):
        """üìÖ Generar √≠ndice diario"""
        if self.global_min_date is None or self.global_max_date is None:
            self.logger.error("‚ùå No se pueden determinar fechas globales")
            return None
            
        self.daily_index = pd.DataFrame({
            'fecha': pd.date_range(start=self.global_min_date, end=self.global_max_date, freq='D')
        })
        
        self.logger.info(f"üìÖ √çndice diario generado: {len(self.daily_index)} d√≠as")
        return self.daily_index

    def create_daily_series(self):
        """üîó Crear series diarias con forward fill"""
        if self.processed_data is None or self.daily_index is None:
            self.logger.error("‚ùå Faltan datos procesados o √≠ndice diario")
            return None
            
        self.logger.info("üîó Creando series temporales diarias...")
        
        # Merge usando merge_asof para forward fill
        self.processed_data = self.processed_data.sort_values('fecha')
        combined = pd.merge_asof(self.daily_index, self.processed_data, on='fecha', direction='backward')
        
        # Forward fill para rellenar valores faltantes
        tes_columns = [col for col in combined.columns if col.startswith('TES_')]
        for col in tes_columns:
            combined[col] = combined[col].ffill()
        
        self.final_df = combined
        self.logger.info(f"‚úÖ Series diarias creadas: {len(self.final_df)} d√≠as")
        
        return self.final_df

    def save_results(self, output_file='datos_tes_procesados.xlsx'):
        """üíæ Guardar resultados procesados"""
        if self.final_df is None:
            self.logger.error("‚ùå No hay datos para guardar")
            return False
            
        try:
            self.logger.info(f"üíæ Guardando resultados en: {output_file}")
            
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # Hoja 1: Datos diarios
                self.final_df.to_excel(writer, sheet_name='TES_Datos_Diarios', index=False)
                
                # Hoja 2: Estad√≠sticas
                stats_df = pd.DataFrame([
                    ['Archivo procesado', self.stats['archivo_procesado']],
                    ['Filas totales', self.stats['filas_totales']],
                    ['Per√≠odo inicio', self.stats['periodo_inicio'].strftime('%Y-%m-%d')],
                    ['Per√≠odo fin', self.stats['periodo_fin'].strftime('%Y-%m-%d')],
                    ['D√≠as totales', len(self.daily_index)],
                    ['Variables extra√≠das', len(self.stats['columnas_procesadas'])]
                ], columns=['M√©trica', 'Valor'])
                
                stats_df.to_excel(writer, sheet_name='Estadisticas', index=False)
                
                # Hoja 3: Mapeo de columnas
                mapping_data = []
                for key, original_name in self.stats['columnas_encontradas'].items():
                    mapping_data.append([key, original_name])
                
                mapping_df = pd.DataFrame(mapping_data, columns=['Variable_TES', 'Columna_Original'])
                mapping_df.to_excel(writer, sheet_name='Mapeo_Columnas', index=False)
                
                # Show column info
                tes_cols = [col for col in self.final_df.columns if col.startswith('TES_')]
                self.logger.info(f"   üìä Columnas TES guardadas: {tes_cols}")
            
            self.logger.info(f"‚úÖ Archivo guardado exitosamente: {output_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Error guardando resultados: {str(e)}")
            return False

    def run(self, file_path=None, output_file='datos_tes_procesados.xlsx'):
        """
        üöÄ Ejecutar procesamiento completo
        """
        start_time = time.time()
        
        self.logger.info("üöÄ Iniciando procesamiento TES...")
        
        # PASO 1: Encontrar archivo
        if file_path is None:
            file_path = self.find_tes_file()
            if file_path is None:
                return False
        
        # PASO 2: Procesar archivo
        self.processed_data = self.process_tes_file(file_path)
        if self.processed_data is None:
            return False
        
        # PASO 3: Generar √≠ndice diario
        if self.generate_daily_index() is None:
            return False
        
        # PASO 4: Crear series diarias
        if self.create_daily_series() is None:
            return False
        
        # PASO 5: Guardar resultados
        if not self.save_results(output_file):
            return False
        
        # Resumen final
        end_time = time.time()
        self.logger.info("\n" + "=" * 60)
        self.logger.info("üéØ PROCESAMIENTO COMPLETADO")
        self.logger.info("=" * 60)
        self.logger.info(f"‚è±Ô∏è  Tiempo total: {end_time - start_time:.2f} segundos")
        self.logger.info(f"üìÅ Archivo procesado: {file_path}")
        self.logger.info(f"üìä Variables extra√≠das: {self.stats['columnas_procesadas']}")
        self.logger.info(f"üìÖ Per√≠odo: {self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}")
        self.logger.info(f"üíæ Archivo guardado: {output_file}")
        self.logger.info("=" * 60)
        
        return True


# =============================================================================
# SECCI√ìN 2: EOE UNIVERSAL PROCESSOR - √çndices de Confianza
# =============================================================================

def configurar_logging_eoe(log_file='logs/eoe_universal_processor.log'):
    """Configuraci√≥n del sistema de logging para EOE"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('EOE_Universal_Processor')


