{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\saule\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\saule\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyInvesting Copy-Paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 11:47:59,338 [INFO] ================================================================================\n",
      "2025-03-31 11:47:59,339 [INFO] INICIANDO PROCESO: EconomicDataProcessor\n",
      "2025-03-31 11:47:59,339 [INFO] Archivo de configuración: Data Engineering.xlsx\n",
      "2025-03-31 11:47:59,341 [INFO] Directorio raíz de datos: data/Macro/raw\n",
      "2025-03-31 11:47:59,341 [INFO] Fecha y hora: 2025-03-31 11:47:59\n",
      "2025-03-31 11:47:59,342 [INFO] ================================================================================\n",
      "2025-03-31 11:47:59,344 [INFO] Leyendo archivo de configuración...\n",
      "2025-03-31 11:47:59,389 [INFO] Se encontraron 21 configuraciones para procesar\n",
      "2025-03-31 11:47:59,391 [INFO] \n",
      "Procesando: US_ISM_Manufacturing (business_confidence)\n",
      "2025-03-31 11:47:59,392 [INFO] - Archivo: US_ISM_Manufacturing.xlsx\n",
      "2025-03-31 11:47:59,392 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,394 [INFO] - Ruta encontrada: data/Macro/raw\\business_confidence\\US_ISM_Manufacturing.xlsx\n",
      "2025-03-31 11:47:59,418 [INFO] - Filas encontradas: 138\n",
      "2025-03-31 11:47:59,429 [INFO] Preferencia de dayfirst para data/Macro/raw\\business_confidence\\US_ISM_Manufacturing.xlsx: True\n",
      "2025-03-31 11:47:59,489 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,493 [INFO] - Valores no nulos en TARGET: 137\n",
      "2025-03-31 11:47:59,493 [INFO] - Periodo: 2013-11-01 a 2025-03-03\n",
      "2025-03-31 11:47:59,495 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,496 [INFO] \n",
      "Procesando: US_ISM_Services (business_confidence)\n",
      "2025-03-31 11:47:59,498 [INFO] - Archivo: US_ISM_Services.xlsx\n",
      "2025-03-31 11:47:59,498 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,499 [INFO] - Ruta encontrada: data/Macro/raw\\business_confidence\\US_ISM_Services.xlsx\n",
      "2025-03-31 11:47:59,516 [INFO] - Filas encontradas: 137\n",
      "2025-03-31 11:47:59,524 [INFO] Preferencia de dayfirst para data/Macro/raw\\business_confidence\\US_ISM_Services.xlsx: True\n",
      "2025-03-31 11:47:59,569 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,571 [INFO] - Valores no nulos en TARGET: 136\n",
      "2025-03-31 11:47:59,572 [INFO] - Periodo: 2013-12-04 a 2025-03-05\n",
      "2025-03-31 11:47:59,574 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,577 [INFO] \n",
      "Procesando: US_Philly_Fed_Index (business_confidence)\n",
      "2025-03-31 11:47:59,577 [INFO] - Archivo: US_Philly_Fed_Index.xlsx\n",
      "2025-03-31 11:47:59,579 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,579 [INFO] - Ruta encontrada: data/Macro/raw\\business_confidence\\US_Philly_Fed_Index.xlsx\n",
      "2025-03-31 11:47:59,600 [INFO] - Filas encontradas: 136\n",
      "2025-03-31 11:47:59,606 [INFO] Preferencia de dayfirst para data/Macro/raw\\business_confidence\\US_Philly_Fed_Index.xlsx: True\n",
      "2025-03-31 11:47:59,665 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,668 [INFO] - Valores no nulos en TARGET: 136\n",
      "2025-03-31 11:47:59,669 [INFO] - Periodo: 2013-12-19 a 2025-03-20\n",
      "2025-03-31 11:47:59,669 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,672 [INFO] \n",
      "Procesando: France_Business_Climate (business_confidence)\n",
      "2025-03-31 11:47:59,674 [INFO] - Archivo: France_Business_Climate.xlsx\n",
      "2025-03-31 11:47:59,675 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,675 [INFO] - Ruta encontrada: data/Macro/raw\\business_confidence\\France_Business_Climate.xlsx\n",
      "2025-03-31 11:47:59,695 [INFO] - Filas encontradas: 137\n",
      "2025-03-31 11:47:59,700 [INFO] Preferencia de dayfirst para data/Macro/raw\\business_confidence\\France_Business_Climate.xlsx: True\n",
      "2025-03-31 11:47:59,743 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,744 [INFO] - Valores no nulos en TARGET: 137\n",
      "2025-03-31 11:47:59,746 [INFO] - Periodo: 2013-12-20 a 2025-03-21\n",
      "2025-03-31 11:47:59,748 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,749 [INFO] \n",
      "Procesando: EuroZone_Business_Climate (business_confidence)\n",
      "2025-03-31 11:47:59,749 [INFO] - Archivo: EuroZone_Business_Climate.xlsx\n",
      "2025-03-31 11:47:59,749 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,751 [INFO] - Ruta encontrada: data/Macro/raw\\business_confidence\\EuroZone_Business_Climate.xlsx\n",
      "2025-03-31 11:47:59,767 [INFO] - Filas encontradas: 138\n",
      "2025-03-31 11:47:59,772 [INFO] Preferencia de dayfirst para data/Macro/raw\\business_confidence\\EuroZone_Business_Climate.xlsx: True\n",
      "2025-03-31 11:47:59,819 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,822 [INFO] - Valores no nulos en TARGET: 137\n",
      "2025-03-31 11:47:59,822 [INFO] - Periodo: 2013-10-30 a 2025-02-27\n",
      "2025-03-31 11:47:59,824 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,826 [INFO] \n",
      "Procesando: U.S. All Car Sales (car_registrations)\n",
      "2025-03-31 11:47:59,827 [INFO] - Archivo: U.S. All Car Sales.xlsx\n",
      "2025-03-31 11:47:59,827 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,827 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\U.S. All Car Sales.xlsx\n",
      "2025-03-31 11:47:59,841 [INFO] - Filas encontradas: 132\n",
      "2025-03-31 11:47:59,845 [INFO] Preferencia de dayfirst para data/Macro/raw\\car_registrations\\U.S. All Car Sales.xlsx: True\n",
      "2025-03-31 11:47:59,902 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,906 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\car_registrations\\U.S. All Car Sales.xlsx\n",
      "2025-03-31 11:47:59,907 [INFO] \n",
      "Procesando: US_Consumer_Confidence (consumer_confidence)\n",
      "2025-03-31 11:47:59,909 [INFO] - Archivo: US_Consumer_Confidence.xlsx\n",
      "2025-03-31 11:47:59,909 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,910 [INFO] - Ruta encontrada: data/Macro/raw\\consumer_confidence\\US_Consumer_Confidence.xlsx\n",
      "2025-03-31 11:47:59,930 [INFO] - Filas encontradas: 136\n",
      "2025-03-31 11:47:59,935 [INFO] Preferencia de dayfirst para data/Macro/raw\\consumer_confidence\\US_Consumer_Confidence.xlsx: True\n",
      "2025-03-31 11:47:59,975 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:47:59,978 [INFO] - Valores no nulos en TARGET: 136\n",
      "2025-03-31 11:47:59,978 [INFO] - Periodo: 2013-12-31 a 2025-03-25\n",
      "2025-03-31 11:47:59,980 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:47:59,980 [INFO] \n",
      "Procesando: China_PMI_Manufacturing (economics)\n",
      "2025-03-31 11:47:59,980 [INFO] - Archivo: China_PMI_Manufacturing.xlsx\n",
      "2025-03-31 11:47:59,981 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:47:59,981 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\China_PMI_Manufacturing.xlsx\n",
      "2025-03-31 11:47:59,995 [INFO] - Filas encontradas: 136\n",
      "2025-03-31 11:48:00,001 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\China_PMI_Manufacturing.xlsx: True\n",
      "2025-03-31 11:48:00,067 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,068 [INFO] - Valores no nulos en TARGET: 135\n",
      "2025-03-31 11:48:00,070 [INFO] - Periodo: 2013-12-31 a 2025-02-28\n",
      "2025-03-31 11:48:00,071 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:00,074 [INFO] \n",
      "Procesando: Singapore_NonOil_Exports_YoY (economics)\n",
      "2025-03-31 11:48:00,076 [INFO] - Archivo: Singapore_NonOil_Exports_YoY.xlsx\n",
      "2025-03-31 11:48:00,076 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,077 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\Singapore_NonOil_Exports_YoY.xlsx\n",
      "2025-03-31 11:48:00,099 [INFO] - Filas encontradas: 138\n",
      "2025-03-31 11:48:00,104 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\Singapore_NonOil_Exports_YoY.xlsx: True\n",
      "2025-03-31 11:48:00,152 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,155 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\Singapore_NonOil_Exports_YoY.xlsx\n",
      "2025-03-31 11:48:00,155 [INFO] \n",
      "Procesando: Japan_M2_MoneySupply_YoY (economics)\n",
      "2025-03-31 11:48:00,156 [INFO] - Archivo: Japan_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,157 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,158 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\Japan_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,172 [INFO] - Filas encontradas: 131\n",
      "2025-03-31 11:48:00,179 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\Japan_M2_MoneySupply_YoY.xlsx: True\n",
      "2025-03-31 11:48:00,221 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,224 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\Japan_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,224 [INFO] \n",
      "Procesando: China_M2_MoneySupply_YoY (economics)\n",
      "2025-03-31 11:48:00,226 [INFO] - Archivo: China_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,226 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,227 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\China_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,244 [INFO] - Filas encontradas: 137\n",
      "2025-03-31 11:48:00,248 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\China_M2_MoneySupply_YoY.xlsx: True\n",
      "2025-03-31 11:48:00,295 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,297 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\China_M2_MoneySupply_YoY.xlsx\n",
      "2025-03-31 11:48:00,298 [INFO] \n",
      "Procesando: Mexico_CPI_MoM (economics)\n",
      "2025-03-31 11:48:00,298 [INFO] - Archivo: Mexico_CPI_MoM.xlsx\n",
      "2025-03-31 11:48:00,298 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,300 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\Mexico_CPI_MoM.xlsx\n",
      "2025-03-31 11:48:00,316 [INFO] - Filas encontradas: 138\n",
      "2025-03-31 11:48:00,321 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\Mexico_CPI_MoM.xlsx: True\n",
      "2025-03-31 11:48:00,366 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,369 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\Mexico_CPI_MoM.xlsx\n",
      "2025-03-31 11:48:00,371 [INFO] \n",
      "Procesando: BOJ_Policy_Rate (economics)\n",
      "2025-03-31 11:48:00,371 [INFO] - Archivo: BOJ_Policy_Rate.xlsx\n",
      "2025-03-31 11:48:00,371 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,372 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\BOJ_Policy_Rate.xlsx\n",
      "2025-03-31 11:48:00,385 [INFO] - Filas encontradas: 111\n",
      "2025-03-31 11:48:00,394 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\BOJ_Policy_Rate.xlsx: True\n",
      "2025-03-31 11:48:00,433 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,434 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\BOJ_Policy_Rate.xlsx\n",
      "2025-03-31 11:48:00,436 [INFO] \n",
      "Procesando: UK_Retail_Sales_MoM (economics)\n",
      "2025-03-31 11:48:00,437 [INFO] - Archivo: UK_Retail_Sales_MoM.xlsx\n",
      "2025-03-31 11:48:00,437 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,439 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\UK_Retail_Sales_MoM.xlsx\n",
      "2025-03-31 11:48:00,460 [INFO] - Filas encontradas: 138\n",
      "2025-03-31 11:48:00,468 [INFO] Preferencia de dayfirst para data/Macro/raw\\economics\\UK_Retail_Sales_MoM.xlsx: True\n",
      "2025-03-31 11:48:00,520 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,522 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\economics\\UK_Retail_Sales_MoM.xlsx\n",
      "2025-03-31 11:48:00,523 [INFO] \n",
      "Procesando: China_Exports (exports)\n",
      "2025-03-31 11:48:00,523 [INFO] - Archivo: China_Exports.xlsx\n",
      "2025-03-31 11:48:00,525 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,525 [INFO] - Ruta encontrada: data/Macro/raw\\exports\\China_Exports.xlsx\n",
      "2025-03-31 11:48:00,540 [INFO] - Filas encontradas: 131\n",
      "2025-03-31 11:48:00,545 [INFO] Preferencia de dayfirst para data/Macro/raw\\exports\\China_Exports.xlsx: True\n",
      "2025-03-31 11:48:00,590 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,592 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\exports\\China_Exports.xlsx\n",
      "2025-03-31 11:48:00,593 [INFO] \n",
      "Procesando: US_Exports (exports)\n",
      "2025-03-31 11:48:00,593 [INFO] - Archivo: US_Exports.xlsx\n",
      "2025-03-31 11:48:00,593 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,595 [INFO] - Ruta encontrada: data/Macro/raw\\exports\\US_Exports.xlsx\n",
      "2025-03-31 11:48:00,607 [INFO] - Filas encontradas: 87\n",
      "2025-03-31 11:48:00,613 [INFO] Preferencia de dayfirst para data/Macro/raw\\exports\\US_Exports.xlsx: True\n",
      "2025-03-31 11:48:00,657 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,660 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\exports\\US_Exports.xlsx\n",
      "2025-03-31 11:48:00,662 [INFO] \n",
      "Procesando: US_Leading_EconIndex (leading_economic_index)\n",
      "2025-03-31 11:48:00,662 [INFO] - Archivo: US_Leading_EconIndex.xlsx\n",
      "2025-03-31 11:48:00,663 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,665 [INFO] - Ruta encontrada: data/Macro/raw\\leading_economic_index\\US_Leading_EconIndex.xlsx\n",
      "2025-03-31 11:48:00,665 [INFO] Utilizando estrategia especial para US_Leading_EconIndex (header=2)\n",
      "2025-03-31 11:48:00,682 [INFO] Columnas leídas: ['Feb 20, 2025 (Jan)', nan, '-0.3%', '-0.1%', '0.1%']\n",
      "2025-03-31 11:48:00,683 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:00,683 [ERROR] No se encontró la columna 'Release Date' en data/Macro/raw\\leading_economic_index\\US_Leading_EconIndex.xlsx\n",
      "2025-03-31 11:48:00,685 [INFO] \n",
      "Procesando: US_ConferenceBoard_LEI (leading_economic_index)\n",
      "2025-03-31 11:48:00,686 [INFO] - Archivo: US_ConferenceBoard_LEI.xlsx\n",
      "2025-03-31 11:48:00,686 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,686 [INFO] - Ruta encontrada: data/Macro/raw\\leading_economic_index\\US_ConferenceBoard_LEI.xlsx\n",
      "2025-03-31 11:48:00,699 [INFO] - Filas encontradas: 135\n",
      "2025-03-31 11:48:00,702 [INFO] Preferencia de dayfirst para data/Macro/raw\\leading_economic_index\\US_ConferenceBoard_LEI.xlsx: True\n",
      "2025-03-31 11:48:00,763 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,764 [INFO] - Valores no nulos en TARGET: 8\n",
      "2025-03-31 11:48:00,766 [INFO] - Periodo: 2014-01-28 a 2014-08-26\n",
      "2025-03-31 11:48:00,766 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:00,769 [INFO] \n",
      "Procesando: Japan_Leading_Indicator (leading_economic_index)\n",
      "2025-03-31 11:48:00,769 [INFO] - Archivo: Japan_Leading_Indicator.xlsx\n",
      "2025-03-31 11:48:00,769 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,769 [INFO] - Ruta encontrada: data/Macro/raw\\leading_economic_index\\Japan_Leading_Indicator.xlsx\n",
      "2025-03-31 11:48:00,803 [INFO] - Filas encontradas: 257\n",
      "2025-03-31 11:48:00,809 [INFO] Preferencia de dayfirst para data/Macro/raw\\leading_economic_index\\Japan_Leading_Indicator.xlsx: True\n",
      "2025-03-31 11:48:00,904 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,910 [INFO] - Valores no nulos en TARGET: 257\n",
      "2025-03-31 11:48:00,912 [INFO] - Periodo: 2014-01-10 a 2025-03-26\n",
      "2025-03-31 11:48:00,912 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:00,915 [INFO] \n",
      "Procesando: China_Unemployment_Rate (unemployment_rate)\n",
      "2025-03-31 11:48:00,917 [INFO] - Archivo: China_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:00,917 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,917 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\China_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:00,929 [INFO] - Filas encontradas: 75\n",
      "2025-03-31 11:48:00,936 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\China_Unemployment_Rate.xlsx: True\n",
      "2025-03-31 11:48:00,973 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:00,976 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\unemployment_rate\\China_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:00,978 [INFO] \n",
      "Procesando: Eurozone_Unemployment_Rate (unemployment_rate)\n",
      "2025-03-31 11:48:00,980 [INFO] - Archivo: Eurozone_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:00,980 [INFO] - Columna TARGET: ACTUAL\n",
      "2025-03-31 11:48:00,981 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\Eurozone_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:01,001 [INFO] - Filas encontradas: 136\n",
      "2025-03-31 11:48:01,006 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\Eurozone_Unemployment_Rate.xlsx: True\n",
      "2025-03-31 11:48:01,058 [WARNING] No se encontró 'ACTUAL', se usará 'Actual'\n",
      "2025-03-31 11:48:01,062 [ERROR] No se encontraron valores válidos para 'Actual' en data/Macro/raw\\unemployment_rate\\Eurozone_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:01,067 [INFO] Índice diario generado: 4166 días desde 2013-10-30 hasta 2025-03-26\n",
      "2025-03-31 11:48:01,103 [WARNING] Omitiendo U.S. All Car Sales por falta de datos\n",
      "2025-03-31 11:48:01,120 [WARNING] Omitiendo Singapore_NonOil_Exports_YoY por falta de datos\n",
      "2025-03-31 11:48:01,121 [WARNING] Omitiendo Japan_M2_MoneySupply_YoY por falta de datos\n",
      "2025-03-31 11:48:01,123 [WARNING] Omitiendo China_M2_MoneySupply_YoY por falta de datos\n",
      "2025-03-31 11:48:01,123 [WARNING] Omitiendo Mexico_CPI_MoM por falta de datos\n",
      "2025-03-31 11:48:01,124 [WARNING] Omitiendo BOJ_Policy_Rate por falta de datos\n",
      "2025-03-31 11:48:01,126 [WARNING] Omitiendo UK_Retail_Sales_MoM por falta de datos\n",
      "2025-03-31 11:48:01,127 [WARNING] Omitiendo China_Exports por falta de datos\n",
      "2025-03-31 11:48:01,127 [WARNING] Omitiendo US_Exports por falta de datos\n",
      "2025-03-31 11:48:01,129 [WARNING] Omitiendo US_Leading_EconIndex por falta de datos\n",
      "2025-03-31 11:48:01,138 [WARNING] Omitiendo China_Unemployment_Rate por falta de datos\n",
      "2025-03-31 11:48:01,138 [WARNING] Omitiendo Eurozone_Unemployment_Rate por falta de datos\n",
      "2025-03-31 11:48:01,139 [INFO] DataFrame final combinado: 4166 filas, 10 columnas\n",
      "2025-03-31 11:48:01,139 [INFO] \n",
      "Resumen de Cobertura:\n",
      "2025-03-31 11:48:01,141 [INFO] - US_ISM_Manufacturing: 100.00% desde 2013-11-01 a 2025-03-03\n",
      "2025-03-31 11:48:01,143 [INFO] - US_ISM_Services: 100.00% desde 2013-12-04 a 2025-03-05\n",
      "2025-03-31 11:48:01,143 [INFO] - US_Philly_Fed_Index: 100.00% desde 2013-12-19 a 2025-03-20\n",
      "2025-03-31 11:48:01,144 [INFO] - France_Business_Climate: 100.00% desde 2013-12-20 a 2025-03-21\n",
      "2025-03-31 11:48:01,144 [INFO] - EuroZone_Business_Climate: 100.00% desde 2013-10-30 a 2025-02-27\n",
      "2025-03-31 11:48:01,146 [INFO] - US_Consumer_Confidence: 100.00% desde 2013-12-31 a 2025-03-25\n",
      "2025-03-31 11:48:01,146 [INFO] - China_PMI_Manufacturing: 100.00% desde 2013-12-31 a 2025-02-28\n",
      "2025-03-31 11:48:01,149 [INFO] - US_ConferenceBoard_LEI: 100.00% desde 2014-01-28 a 2014-08-26\n",
      "2025-03-31 11:48:01,150 [INFO] - Japan_Leading_Indicator: 100.00% desde 2014-01-10 a 2025-03-26\n",
      "2025-03-31 11:48:01,861 [INFO] Archivo guardado exitosamente: datos_economicos_procesados_cp.xlsx\n",
      "2025-03-31 11:48:01,863 [INFO] \n",
      "Resumen de Ejecución:\n",
      "2025-03-31 11:48:01,864 [INFO] Tiempo de ejecución: 2.52 segundos\n",
      "2025-03-31 11:48:01,866 [INFO] Archivos procesados: 21\n",
      "2025-03-31 11:48:01,866 [INFO] Archivo de salida: datos_economicos_procesados_cp.xlsx\n",
      "2025-03-31 11:48:01,866 [INFO] Estado: COMPLETADO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.parser import parse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de logging\n",
    "def configurar_logging(log_file='myinvestingreportcp.log'):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('EconomicDataProcessor')\n",
    "\n",
    "class EconomicDataProcessor:\n",
    "    \"\"\"\n",
    "    Clase para procesar datos macroeconómicos con robustez en el manejo de fechas y \n",
    "    forward filling de series de frecuencia (por ejemplo, mensuales a diarios).\n",
    "\n",
    "    Funcionalidades:\n",
    "      - Conversión robusta de cadenas de fecha usando múltiples estrategias.\n",
    "      - Transformación de series (generalmente mensuales) a datos diarios mediante merge_asof.\n",
    "      - Renombrado de la columna de valores usando el patrón: \n",
    "            {target_col}_{variable}_{tipo_macro}\n",
    "      - Validación y log detallado en cada etapa.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_file, data_root='data/Macro/raw', log_file='myinvestingreportcp.log'):\n",
    "        self.config_file = config_file\n",
    "        self.data_root = data_root\n",
    "        self.logger = configurar_logging(log_file)\n",
    "        self.config_data = None\n",
    "        self.global_min_date = None\n",
    "        self.global_max_date = None\n",
    "        self.daily_index = None\n",
    "        self.processed_data = {}  # Diccionario {variable: DataFrame procesado}\n",
    "        self.final_df = None\n",
    "        self.stats = {}\n",
    "        # Cache para la preferencia de conversión de fecha\n",
    "        self.date_cache = {}\n",
    "\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"INICIANDO PROCESO: EconomicDataProcessor\")\n",
    "        self.logger.info(f\"Archivo de configuración: {config_file}\")\n",
    "        self.logger.info(f\"Directorio raíz de datos: {data_root}\")\n",
    "        self.logger.info(f\"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "\n",
    "    def read_config(self):\n",
    "        try:\n",
    "            self.logger.info(\"Leyendo archivo de configuración...\")\n",
    "            df_config = pd.read_excel(self.config_file)\n",
    "            self.config_data = df_config[\n",
    "                (df_config['Fuente'] == 'Investing Data') &\n",
    "                (df_config['Tipo de Preprocesamiento Según la Fuente'] == 'Copiar y Pegar')\n",
    "            ].copy()\n",
    "            self.logger.info(f\"Se encontraron {len(self.config_data)} configuraciones para procesar\")\n",
    "            return self.config_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer configuración: {e}\")\n",
    "            return None\n",
    "\n",
    "    def robust_parse_date(self, date_str, preferred_dayfirst=None):\n",
    "        \"\"\"\n",
    "        Intenta convertir la cadena de fecha usando múltiples estrategias.\n",
    "        - Primero, busca patrones como \"Apr 01, 2025 (Mar)\".\n",
    "        - Luego utiliza pd.to_datetime con la opción dayfirst según la preferencia.\n",
    "        - Si no se especifica, prueba ambas opciones y elige la que dé una fecha razonable.\n",
    "\n",
    "        Args:\n",
    "            date_str (str): Cadena de fecha.\n",
    "            preferred_dayfirst (bool, opcional): Preferencia de interpretación.\n",
    "\n",
    "        Returns:\n",
    "            pd.Timestamp o None.\n",
    "        \"\"\"\n",
    "        if not isinstance(date_str, str):\n",
    "            return None\n",
    "        date_str = date_str.strip()\n",
    "        if not date_str:\n",
    "            return None\n",
    "\n",
    "        m = re.search(r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})', date_str)\n",
    "        if m:\n",
    "            candidate = m.group(1)\n",
    "            try:\n",
    "                parsed = pd.to_datetime(candidate, errors='coerce')\n",
    "                if parsed is not None:\n",
    "                    return parsed\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error al parsear patrón en '{date_str}': {e}\")\n",
    "\n",
    "        if preferred_dayfirst is not None:\n",
    "            try:\n",
    "                parsed = pd.to_datetime(date_str, dayfirst=preferred_dayfirst, errors='coerce')\n",
    "                threshold = pd.Timestamp.today() + pd.Timedelta(days=30)\n",
    "                if parsed and parsed <= threshold:\n",
    "                    return parsed\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error con dayfirst={preferred_dayfirst} en '{date_str}': {e}\")\n",
    "\n",
    "        try:\n",
    "            parsed_true = pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "            parsed_false = pd.to_datetime(date_str, dayfirst=False, errors='coerce')\n",
    "            threshold = pd.Timestamp.today() + pd.Timedelta(days=30)\n",
    "            valid_true = parsed_true and parsed_true <= threshold\n",
    "            valid_false = parsed_false and parsed_false <= threshold\n",
    "            if valid_true and not valid_false:\n",
    "                return parsed_true\n",
    "            elif valid_false and not valid_true:\n",
    "                return parsed_false\n",
    "            elif valid_true and valid_false:\n",
    "                return parsed_true  # Por defecto dayfirst=True\n",
    "            else:\n",
    "                return parsed_true if pd.notnull(parsed_true) else parsed_false\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error en robust_parse_date para '{date_str}': {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_file(self, config_row):\n",
    "        \"\"\"\n",
    "        Procesa un archivo individual:\n",
    "          - Lee el archivo (se usa estrategia especial para US_Leading_EconIndex).\n",
    "          - Convierte la columna 'Release Date' robustamente.\n",
    "          - Detecta y convierte la columna target a numérico.\n",
    "          - Renombra la columna de valor usando el patrón:\n",
    "                {target_col}_{variable}_{tipo_macro}\n",
    "          - Selecciona solo las columnas 'fecha' y la columna renombrada.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (variable, DataFrame procesado) o (variable, None) en caso de error.\n",
    "        \"\"\"\n",
    "        variable = config_row['Variable']\n",
    "        macro_type = config_row['Tipo Macro']\n",
    "        target_col = config_row['TARGET']\n",
    "\n",
    "        # Construir la ruta (buscando también en subdirectorios)\n",
    "        ruta = os.path.join(self.data_root, macro_type, f\"{variable}.xlsx\")\n",
    "        if not os.path.exists(ruta):\n",
    "            for root, dirs, files in os.walk(self.data_root):\n",
    "                if f\"{variable}.xlsx\" in files:\n",
    "                    ruta = os.path.join(root, f\"{variable}.xlsx\")\n",
    "                    break\n",
    "        if not os.path.exists(ruta):\n",
    "            self.logger.error(f\"Archivo no encontrado: {variable}.xlsx\")\n",
    "            return variable, None\n",
    "\n",
    "        self.logger.info(f\"\\nProcesando: {variable} ({macro_type})\")\n",
    "        self.logger.info(f\"- Archivo: {variable}.xlsx\")\n",
    "        self.logger.info(f\"- Columna TARGET: {target_col}\")\n",
    "        self.logger.info(f\"- Ruta encontrada: {ruta}\")\n",
    "\n",
    "        try:\n",
    "            # Estrategia especial para US_Leading_EconIndex: ajustar header y limpiar columnas\n",
    "            if variable == \"US_Leading_EconIndex\":\n",
    "                self.logger.info(\"Utilizando estrategia especial para US_Leading_EconIndex (header=2)\")\n",
    "                df = pd.read_excel(ruta, header=2, engine='openpyxl')\n",
    "                df.columns = df.columns.str.strip()\n",
    "                self.logger.info(f\"Columnas leídas: {df.columns.tolist()}\")\n",
    "            else:\n",
    "                df = pd.read_excel(ruta, engine='openpyxl')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer {ruta}: {e}\")\n",
    "            return variable, None\n",
    "\n",
    "        self.logger.info(f\"- Filas encontradas: {len(df)}\")\n",
    "        if 'Release Date' not in df.columns:\n",
    "            self.logger.error(f\"No se encontró la columna 'Release Date' en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        # Determinar preferencia de dayfirst (cacheada)\n",
    "        if ruta not in self.date_cache:\n",
    "            sample = df['Release Date'].dropna().head(10)\n",
    "            count_true, count_false = 0, 0\n",
    "            threshold = pd.Timestamp.today() + pd.Timedelta(days=30)\n",
    "            for val in sample:\n",
    "                dt_true = pd.to_datetime(val, dayfirst=True, errors='coerce')\n",
    "                dt_false = pd.to_datetime(val, dayfirst=False, errors='coerce')\n",
    "                if pd.notnull(dt_true) and dt_true <= threshold:\n",
    "                    count_true += 1\n",
    "                if pd.notnull(dt_false) and dt_false <= threshold:\n",
    "                    count_false += 1\n",
    "            preferred = count_true >= count_false\n",
    "            self.date_cache[ruta] = preferred\n",
    "            self.logger.info(f\"Preferencia de dayfirst para {ruta}: {preferred}\")\n",
    "        else:\n",
    "            preferred = self.date_cache[ruta]\n",
    "\n",
    "        df['fecha'] = df['Release Date'].apply(lambda x: self.robust_parse_date(x, preferred_dayfirst=preferred))\n",
    "        df = df.dropna(subset=['fecha'])\n",
    "        df = df.sort_values('fecha')\n",
    "\n",
    "        # Si el target especificado no está, intenta buscar una alternativa\n",
    "        if target_col not in df.columns:\n",
    "            for col in df.columns:\n",
    "                if col.strip().lower() == target_col.strip().lower():\n",
    "                    target_col = col\n",
    "                    self.logger.warning(f\"No se encontró '{config_row['TARGET']}', se usará '{target_col}'\")\n",
    "                    break\n",
    "        if target_col not in df.columns:\n",
    "            self.logger.error(f\"No se encontró columna TARGET ni alternativa en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        # Convertir la columna target a numérico y descartar valores no válidos\n",
    "        df['valor'] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "        df = df.dropna(subset=['valor'])\n",
    "        if df.empty:\n",
    "            self.logger.error(f\"No se encontraron valores válidos para '{target_col}' en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        # Actualizar rango global de fechas\n",
    "        current_min = df['fecha'].min()\n",
    "        current_max = df['fecha'].max()\n",
    "        if self.global_min_date is None or current_min < self.global_min_date:\n",
    "            self.global_min_date = current_min\n",
    "        if self.global_max_date is None or current_max > self.global_max_date:\n",
    "            self.global_max_date = current_max\n",
    "\n",
    "        # Calcular cobertura (puedes ajustar la fórmula si lo deseas)\n",
    "        cobertura = (len(df) / len(df)) * 100\n",
    "\n",
    "        # RENOMBRAR LA COLUMNA: Crear un nombre único\n",
    "        nuevo_nombre = f\"{target_col}_{variable}_{macro_type}\"\n",
    "        df.rename(columns={'valor': nuevo_nombre}, inplace=True)\n",
    "        self.stats[variable] = {\n",
    "            'macro_type': macro_type,\n",
    "            'target_column': target_col,\n",
    "            'total_rows': len(df),\n",
    "            'valid_values': len(df),\n",
    "            'coverage': cobertura,\n",
    "            'date_min': current_min,\n",
    "            'date_max': current_max,\n",
    "            'nuevo_nombre': nuevo_nombre\n",
    "        }\n",
    "        self.logger.info(f\"- Valores no nulos en TARGET: {len(df)}\")\n",
    "        self.logger.info(f\"- Periodo: {current_min.strftime('%Y-%m-%d')} a {current_max.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"- Cobertura: {cobertura:.2f}%\")\n",
    "        return variable, df[['fecha', nuevo_nombre]].copy()\n",
    "\n",
    "    def generate_daily_index(self):\n",
    "        \"\"\"\n",
    "        Genera un DataFrame con un índice diario desde la fecha global mínima hasta la máxima.\n",
    "        \"\"\"\n",
    "        if self.global_min_date is None or self.global_max_date is None:\n",
    "            self.logger.error(\"No se pudieron determinar las fechas globales\")\n",
    "            return None\n",
    "        self.daily_index = pd.DataFrame({\n",
    "            'fecha': pd.date_range(start=self.global_min_date, end=self.global_max_date, freq='D')\n",
    "        })\n",
    "        self.logger.info(f\"Índice diario generado: {len(self.daily_index)} días desde {self.global_min_date.strftime('%Y-%m-%d')} hasta {self.global_max_date.strftime('%Y-%m-%d')}\")\n",
    "        return self.daily_index\n",
    "\n",
    "    def combine_data(self):\n",
    "        \"\"\"\n",
    "        Convierte cada serie (generalmente reportada en frecuencias bajas) a datos diarios usando merge_asof.\n",
    "        Para cada archivo, se asocia el dato reportado más reciente a cada día.\n",
    "        \"\"\"\n",
    "        if self.daily_index is None:\n",
    "            self.logger.error(\"El índice diario no ha sido generado\")\n",
    "            return None\n",
    "\n",
    "        combined = self.daily_index.copy()\n",
    "        for variable, df in self.processed_data.items():\n",
    "            if df is None or df.empty:\n",
    "                self.logger.warning(f\"Omitiendo {variable} por falta de datos\")\n",
    "                continue\n",
    "            df = df.sort_values('fecha')\n",
    "            # merge_asof para asignar cada día con el valor reportado más reciente\n",
    "            df_daily = pd.merge_asof(combined, df, on='fecha', direction='backward')\n",
    "            col_name = self.stats[variable]['nuevo_nombre']\n",
    "            # Como precaución se aplica ffill\n",
    "            df_daily[col_name] = df_daily[col_name].ffill()\n",
    "            combined = combined.merge(df_daily[['fecha', col_name]], on='fecha', how='left')\n",
    "        self.final_df = combined\n",
    "        self.logger.info(f\"DataFrame final combinado: {len(self.final_df)} filas, {len(self.final_df.columns)} columnas\")\n",
    "        return self.final_df\n",
    "\n",
    "    def analyze_coverage(self):\n",
    "        \"\"\"\n",
    "        Genera un resumen de cobertura y estadísticas para cada indicador.\n",
    "        \"\"\"\n",
    "        total_days = len(self.daily_index)\n",
    "        self.logger.info(\"\\nResumen de Cobertura:\")\n",
    "        for variable, stats in self.stats.items():\n",
    "            self.logger.info(f\"- {variable}: {stats['coverage']:.2f}% desde {stats['date_min'].strftime('%Y-%m-%d')} a {stats['date_max'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    def save_results(self, output_file='datos_economicos_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Guarda el DataFrame final combinado y las estadísticas en un archivo Excel.\n",
    "        \"\"\"\n",
    "        if self.final_df is None:\n",
    "            self.logger.error(\"No hay datos combinados para guardar\")\n",
    "            return False\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                self.final_df.to_excel(writer, sheet_name='Datos Diarios', index=False)\n",
    "                df_stats = pd.DataFrame(self.stats).T\n",
    "                df_stats.to_excel(writer, sheet_name='Estadisticas')\n",
    "                meta = {\n",
    "                    'Proceso': ['EconomicDataProcessor'],\n",
    "                    'Fecha de proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                    'Total indicadores': [len(self.stats)],\n",
    "                    'Periodo': [f\"{self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}\"],\n",
    "                    'Total días': [len(self.daily_index)]\n",
    "                }\n",
    "                pd.DataFrame(meta).to_excel(writer, sheet_name='Metadatos', index=False)\n",
    "            self.logger.info(f\"Archivo guardado exitosamente: {output_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al guardar resultados: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run(self, output_file='datos_economicos_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Ejecuta el proceso completo:\n",
    "          1. Lee la configuración.\n",
    "          2. Procesa cada archivo.\n",
    "          3. Determina el rango global de fechas.\n",
    "          4. Genera el índice diario.\n",
    "          5. Convierte cada serie a datos diarios y los combina.\n",
    "          6. Realiza un análisis de cobertura.\n",
    "          7. Guarda los resultados.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.read_config() is None:\n",
    "            return False\n",
    "\n",
    "        for _, config_row in self.config_data.iterrows():\n",
    "            var, df_processed = self.process_file(config_row)\n",
    "            self.processed_data[var] = df_processed\n",
    "\n",
    "        if len([df for df in self.processed_data.values() if df is not None]) == 0:\n",
    "            self.logger.error(\"No se procesó ningún archivo correctamente\")\n",
    "            return False\n",
    "\n",
    "        self.generate_daily_index()\n",
    "        self.combine_data()\n",
    "        self.analyze_coverage()\n",
    "        result = self.save_results(output_file)\n",
    "        end_time = time.time()\n",
    "        self.logger.info(\"\\nResumen de Ejecución:\")\n",
    "        self.logger.info(f\"Tiempo de ejecución: {end_time - start_time:.2f} segundos\")\n",
    "        self.logger.info(f\"Archivos procesados: {len(self.config_data)}\")\n",
    "        self.logger.info(f\"Archivo de salida: {output_file}\")\n",
    "        self.logger.info(f\"Estado: {'COMPLETADO' if result else 'ERROR'}\")\n",
    "        return result\n",
    "\n",
    "# Función principal para ejecutar el proceso\n",
    "def run_economic_data_processor(config_file='Data Engineering.xlsx',\n",
    "                                output_file='datos_economicos_procesados_cp.xlsx',\n",
    "                                data_root='data/Macro/raw',\n",
    "                                log_file='myinvestingreportcp.log'):\n",
    "    processor = EconomicDataProcessor(config_file, data_root, log_file)\n",
    "    return processor.run(output_file)\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    success = run_economic_data_processor()\n",
    "    print(f\"Proceso {'completado exitosamente' if success else 'finalizado con errores'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyInvesting Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 11:48:01,932 [INFO] ================================================================================\n",
      "2025-03-31 11:48:01,934 [INFO] INICIANDO PROCESO: MyinvestingreportNormal\n",
      "2025-03-31 11:48:01,935 [INFO] Archivo de configuración: Data Engineering.xlsx\n",
      "2025-03-31 11:48:01,937 [INFO] Directorio raíz de datos: data/Macro/raw\n",
      "2025-03-31 11:48:01,938 [INFO] Fecha y hora: 2025-03-31 11:48:01\n",
      "2025-03-31 11:48:01,940 [INFO] ================================================================================\n",
      "2025-03-31 11:48:01,940 [INFO] Iniciando proceso completo MyinvestingreportNormal...\n",
      "2025-03-31 11:48:01,942 [INFO] Leyendo archivo de configuración...\n",
      "2025-03-31 11:48:01,969 [INFO] Se encontraron 28 configuraciones para procesar\n",
      "2025-03-31 11:48:01,970 [INFO] \n",
      "Procesando: Australia_10Y_Bond (bond)\n",
      "2025-03-31 11:48:01,970 [INFO] - Archivo: Australia_10Y_Bond\n",
      "2025-03-31 11:48:01,972 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:01,972 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Australia_10Y_Bond.csv\n",
      "2025-03-31 11:48:01,982 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:01,992 [INFO] Formato de fecha detectado para Australia_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:03,378 [INFO] Ejemplos de fechas convertidas para Australia_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:03,381 [INFO] Formato numérico detectado para Australia_10Y_Bond: americano\n",
      "2025-03-31 11:48:03,386 [INFO] Para Australia_10Y_Bond (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:03,386 [INFO] - Australia_10Y_Bond: 3810 filas procesadas, periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:03,388 [INFO] \n",
      "Procesando: Italy_10Y_Bond (bond)\n",
      "2025-03-31 11:48:03,389 [INFO] - Archivo: Italy_10Y_Bond\n",
      "2025-03-31 11:48:03,391 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:03,391 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Italy_10Y_Bond.csv\n",
      "2025-03-31 11:48:03,398 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:03,411 [INFO] Formato de fecha detectado para Italy_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:04,548 [INFO] Ejemplos de fechas convertidas para Italy_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:04,551 [INFO] Formato numérico detectado para Italy_10Y_Bond: americano\n",
      "2025-03-31 11:48:04,557 [INFO] Para Italy_10Y_Bond (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:04,558 [INFO] - Italy_10Y_Bond: 3238 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:04,560 [INFO] \n",
      "Procesando: Japan_10Y_Bond (bond)\n",
      "2025-03-31 11:48:04,561 [INFO] - Archivo: Japan_10Y_Bond\n",
      "2025-03-31 11:48:04,561 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:04,563 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Japan_10Y_Bond.csv\n",
      "2025-03-31 11:48:04,571 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:04,583 [INFO] Formato de fecha detectado para Japan_10Y_Bond: dayfirst=False (confianza: 0.15)\n",
      "2025-03-31 11:48:05,523 [INFO] Ejemplos de fechas convertidas para Japan_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-19 00:00:00')]\n",
      "2025-03-31 11:48:05,526 [INFO] Formato numérico detectado para Japan_10Y_Bond: americano\n",
      "2025-03-31 11:48:05,530 [INFO] Para Japan_10Y_Bond (columna Date), la fecha mínima es 2014-01-06 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:05,532 [INFO] - Japan_10Y_Bond: 3203 filas procesadas, periodo: 2014-01-06 a 2025-03-26\n",
      "2025-03-31 11:48:05,533 [INFO] \n",
      "Procesando: UK_10Y_Bond (bond)\n",
      "2025-03-31 11:48:05,533 [INFO] - Archivo: UK_10Y_Bond\n",
      "2025-03-31 11:48:05,535 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:05,535 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\UK_10Y_Bond.csv\n",
      "2025-03-31 11:48:05,544 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:05,551 [INFO] Formato de fecha detectado para UK_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:06,649 [INFO] Ejemplos de fechas convertidas para UK_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:06,651 [INFO] Formato numérico detectado para UK_10Y_Bond: americano\n",
      "2025-03-31 11:48:06,658 [INFO] Para UK_10Y_Bond (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:06,660 [INFO] - UK_10Y_Bond: 3424 filas procesadas, periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:06,663 [INFO] \n",
      "Procesando: Germany_10Y_Bond (bond)\n",
      "2025-03-31 11:48:06,663 [INFO] - Archivo: Germany_10Y_Bond\n",
      "2025-03-31 11:48:06,664 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:06,664 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Germany_10Y_Bond.csv\n",
      "2025-03-31 11:48:06,672 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:06,681 [INFO] Formato de fecha detectado para Germany_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:07,630 [INFO] Ejemplos de fechas convertidas para Germany_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:07,635 [INFO] Formato numérico detectado para Germany_10Y_Bond: americano\n",
      "2025-03-31 11:48:07,643 [INFO] Para Germany_10Y_Bond (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:07,644 [INFO] - Germany_10Y_Bond: 3075 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:07,647 [INFO] \n",
      "Procesando: Canada_10Y_Bond (bond)\n",
      "2025-03-31 11:48:07,649 [INFO] - Archivo: Canada_10Y_Bond\n",
      "2025-03-31 11:48:07,649 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:07,651 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Canada_10Y_Bond.csv\n",
      "2025-03-31 11:48:07,660 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:07,670 [INFO] Formato de fecha detectado para Canada_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:08,693 [INFO] Ejemplos de fechas convertidas para Canada_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:08,698 [INFO] Formato numérico detectado para Canada_10Y_Bond: americano\n",
      "2025-03-31 11:48:08,703 [INFO] Para Canada_10Y_Bond (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:08,704 [INFO] - Canada_10Y_Bond: 2965 filas procesadas, periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:08,706 [INFO] \n",
      "Procesando: China_10Y_Bond (bond)\n",
      "2025-03-31 11:48:08,706 [INFO] - Archivo: China_10Y_Bond\n",
      "2025-03-31 11:48:08,707 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:08,707 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\China_10Y_Bond.csv\n",
      "2025-03-31 11:48:08,715 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:08,723 [INFO] Formato de fecha detectado para China_10Y_Bond: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:09,847 [INFO] Ejemplos de fechas convertidas para China_10Y_Bond: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:09,850 [INFO] Formato numérico detectado para China_10Y_Bond: americano\n",
      "2025-03-31 11:48:09,858 [INFO] Para China_10Y_Bond (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:09,860 [INFO] - China_10Y_Bond: 2914 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:09,864 [INFO] \n",
      "Procesando: CrudeOil_WTI (commodities)\n",
      "2025-03-31 11:48:09,866 [INFO] - Archivo: CrudeOil_WTI\n",
      "2025-03-31 11:48:09,866 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:09,868 [INFO] - Ruta encontrada: data/Macro/raw\\commodities\\CrudeOil_WTI.csv\n",
      "2025-03-31 11:48:09,882 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:09,896 [INFO] Formato de fecha detectado para CrudeOil_WTI: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:11,032 [INFO] Ejemplos de fechas convertidas para CrudeOil_WTI: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:11,034 [INFO] Formato numérico detectado para CrudeOil_WTI: americano\n",
      "2025-03-31 11:48:11,039 [INFO] Para CrudeOil_WTI (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:11,040 [INFO] - CrudeOil_WTI: 2953 filas procesadas, periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:11,042 [INFO] \n",
      "Procesando: Gold_Spot (commodities)\n",
      "2025-03-31 11:48:11,043 [INFO] - Archivo: Gold_Spot\n",
      "2025-03-31 11:48:11,043 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:11,043 [INFO] - Ruta encontrada: data/Macro/raw\\commodities\\Gold_Spot.csv\n",
      "2025-03-31 11:48:11,054 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:11,062 [INFO] Formato de fecha detectado para Gold_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:12,100 [INFO] Ejemplos de fechas convertidas para Gold_Spot: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:12,102 [INFO] Formato numérico detectado para Gold_Spot: americano\n",
      "2025-03-31 11:48:12,114 [INFO] Para Gold_Spot (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:12,114 [INFO] - Gold_Spot: 2874 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:12,117 [INFO] \n",
      "Procesando: Silver_Spot (commodities)\n",
      "2025-03-31 11:48:12,117 [INFO] - Archivo: Silver_Spot\n",
      "2025-03-31 11:48:12,119 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:12,119 [INFO] - Ruta encontrada: data/Macro/raw\\commodities\\Silver_Spot.csv\n",
      "2025-03-31 11:48:12,128 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:12,137 [INFO] Formato de fecha detectado para Silver_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:13,234 [INFO] Ejemplos de fechas convertidas para Silver_Spot: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:13,237 [INFO] Formato numérico detectado para Silver_Spot: americano\n",
      "2025-03-31 11:48:13,241 [INFO] Para Silver_Spot (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:13,243 [INFO] - Silver_Spot: 2911 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:13,245 [INFO] \n",
      "Procesando: Copper_Futures (commodities)\n",
      "2025-03-31 11:48:13,245 [INFO] - Archivo: Copper_Futures\n",
      "2025-03-31 11:48:13,246 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:13,246 [INFO] - Ruta encontrada: data/Macro/raw\\commodities\\Copper_Futures.csv\n",
      "2025-03-31 11:48:13,254 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:13,261 [INFO] Formato de fecha detectado para Copper_Futures: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:14,241 [INFO] Ejemplos de fechas convertidas para Copper_Futures: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:14,244 [INFO] Formato numérico detectado para Copper_Futures: americano\n",
      "2025-03-31 11:48:14,252 [INFO] Para Copper_Futures (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:14,252 [INFO] - Copper_Futures: 2908 filas procesadas, periodo: 2014-01-02 a 2025-03-26\n",
      "2025-03-31 11:48:14,255 [INFO] \n",
      "Procesando: Platinum_Spot (commodities)\n",
      "2025-03-31 11:48:14,256 [INFO] - Archivo: Platinum_Spot\n",
      "2025-03-31 11:48:14,258 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:14,259 [INFO] - Ruta encontrada: data/Macro/raw\\commodities\\Platinum_Spot.csv\n",
      "2025-03-31 11:48:14,273 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:14,284 [INFO] Formato de fecha detectado para Platinum_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:15,514 [INFO] Ejemplos de fechas convertidas para Platinum_Spot: [Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00'), Timestamp('2025-03-20 00:00:00')]\n",
      "2025-03-31 11:48:15,516 [INFO] Formato numérico detectado para Platinum_Spot: americano\n",
      "2025-03-31 11:48:15,530 [INFO] Para Platinum_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-26 00:00:00\n",
      "2025-03-31 11:48:15,532 [INFO] - Platinum_Spot: 3463 filas procesadas, periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:15,533 [INFO] \n",
      "Procesando: EUR_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:15,533 [INFO] - Archivo: EUR_USD_Spot\n",
      "2025-03-31 11:48:15,535 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:15,535 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\EUR_USD_Spot.csv\n",
      "2025-03-31 11:48:15,542 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:15,553 [INFO] Formato de fecha detectado para EUR_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:16,538 [INFO] Ejemplos de fechas convertidas para EUR_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:16,541 [INFO] Formato numérico detectado para EUR_USD_Spot: americano\n",
      "2025-03-31 11:48:16,546 [INFO] Para EUR_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:16,547 [INFO] - EUR_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:16,557 [INFO] \n",
      "Procesando: GBP_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:16,557 [INFO] - Archivo: GBP_USD_Spot\n",
      "2025-03-31 11:48:16,558 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:16,560 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\GBP_USD_Spot.csv\n",
      "2025-03-31 11:48:16,571 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:16,583 [INFO] Formato de fecha detectado para GBP_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:17,569 [INFO] Ejemplos de fechas convertidas para GBP_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:17,572 [INFO] Formato numérico detectado para GBP_USD_Spot: americano\n",
      "2025-03-31 11:48:17,577 [INFO] Para GBP_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:17,578 [INFO] - GBP_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:17,587 [INFO] \n",
      "Procesando: JPY_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:17,589 [INFO] - Archivo: JPY_USD_Spot\n",
      "2025-03-31 11:48:17,590 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:17,590 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\JPY_USD_Spot.csv\n",
      "2025-03-31 11:48:17,599 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:17,607 [INFO] Formato de fecha detectado para JPY_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:18,440 [INFO] Ejemplos de fechas convertidas para JPY_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:18,444 [INFO] Formato numérico detectado para JPY_USD_Spot: americano\n",
      "2025-03-31 11:48:18,452 [INFO] Para JPY_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:18,454 [INFO] - JPY_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:18,464 [INFO] \n",
      "Procesando: CNY_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:18,465 [INFO] - Archivo: CNY_USD_Spot\n",
      "2025-03-31 11:48:18,465 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:18,467 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\CNY_USD_Spot.csv\n",
      "2025-03-31 11:48:18,476 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:18,488 [INFO] Formato de fecha detectado para CNY_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:19,494 [INFO] Ejemplos de fechas convertidas para CNY_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:19,497 [INFO] Formato numérico detectado para CNY_USD_Spot: americano\n",
      "2025-03-31 11:48:19,500 [INFO] Para CNY_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:19,502 [INFO] - CNY_USD_Spot: 2933 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:19,511 [INFO] \n",
      "Procesando: AUD_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:19,511 [INFO] - Archivo: AUD_USD_Spot\n",
      "2025-03-31 11:48:19,513 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:19,513 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\AUD_USD_Spot.csv\n",
      "2025-03-31 11:48:19,519 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:19,525 [INFO] Formato de fecha detectado para AUD_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:20,374 [INFO] Ejemplos de fechas convertidas para AUD_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:20,377 [INFO] Formato numérico detectado para AUD_USD_Spot: americano\n",
      "2025-03-31 11:48:20,380 [INFO] Para AUD_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:20,381 [INFO] - AUD_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:20,384 [INFO] \n",
      "Procesando: CAD_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:20,384 [INFO] - Archivo: CAD_USD_Spot\n",
      "2025-03-31 11:48:20,384 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:20,386 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\CAD_USD_Spot.csv\n",
      "2025-03-31 11:48:20,393 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:20,406 [INFO] Formato de fecha detectado para CAD_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:21,286 [INFO] Ejemplos de fechas convertidas para CAD_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:21,289 [INFO] Formato numérico detectado para CAD_USD_Spot: americano\n",
      "2025-03-31 11:48:21,295 [INFO] Para CAD_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:21,297 [INFO] - CAD_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:21,297 [INFO] \n",
      "Procesando: MXN_USD_Spot (exchange_rate)\n",
      "2025-03-31 11:48:21,298 [INFO] - Archivo: MXN_USD_Spot\n",
      "2025-03-31 11:48:21,298 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:21,300 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\MXN_USD_Spot.csv\n",
      "2025-03-31 11:48:21,306 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:21,318 [INFO] Formato de fecha detectado para MXN_USD_Spot: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:22,139 [INFO] Ejemplos de fechas convertidas para MXN_USD_Spot: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:22,141 [INFO] Formato numérico detectado para MXN_USD_Spot: americano\n",
      "2025-03-31 11:48:22,146 [INFO] Para MXN_USD_Spot (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:22,146 [INFO] - MXN_USD_Spot: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:22,149 [INFO] \n",
      "Procesando: EUR_GBP_Cross (exchange_rate)\n",
      "2025-03-31 11:48:22,149 [INFO] - Archivo: EUR_GBP_Cross\n",
      "2025-03-31 11:48:22,149 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:22,150 [INFO] - Ruta encontrada: data/Macro/raw\\exchange_rate\\EUR_GBP_Cross.csv\n",
      "2025-03-31 11:48:22,156 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:22,166 [INFO] Formato de fecha detectado para EUR_GBP_Cross: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:23,060 [INFO] Ejemplos de fechas convertidas para EUR_GBP_Cross: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:23,063 [INFO] Formato numérico detectado para EUR_GBP_Cross: americano\n",
      "2025-03-31 11:48:23,066 [INFO] Para EUR_GBP_Cross (columna Date), la fecha mínima es 2014-01-01 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:23,068 [INFO] - EUR_GBP_Cross: 2932 filas procesadas, periodo: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:23,077 [INFO] \n",
      "Procesando: S&P500_Index (index_pricing)\n",
      "2025-03-31 11:48:23,077 [INFO] - Archivo: S&P500_Index\n",
      "2025-03-31 11:48:23,079 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:23,079 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\S&P500_Index.csv\n",
      "2025-03-31 11:48:23,087 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:23,094 [INFO] Formato de fecha detectado para S&P500_Index: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:23,325 [INFO] Ejemplos de fechas convertidas para S&P500_Index: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:23,327 [INFO] Formato numérico detectado para S&P500_Index: americano\n",
      "2025-03-31 11:48:23,334 [INFO] Para S&P500_Index (columna Fecha), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:23,336 [INFO] - S&P500_Index: 2826 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:23,347 [INFO] \n",
      "Procesando: NASDAQ_Composite (index_pricing)\n",
      "2025-03-31 11:48:23,348 [INFO] - Archivo: NASDAQ_Composite\n",
      "2025-03-31 11:48:23,348 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:23,350 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\NASDAQ_Composite.csv\n",
      "2025-03-31 11:48:23,361 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:23,368 [INFO] Formato de fecha detectado para NASDAQ_Composite: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:23,564 [INFO] Ejemplos de fechas convertidas para NASDAQ_Composite: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:23,566 [INFO] Formato numérico detectado para NASDAQ_Composite: americano\n",
      "2025-03-31 11:48:23,577 [INFO] Para NASDAQ_Composite (columna Fecha), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:23,578 [INFO] - NASDAQ_Composite: 2826 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:23,580 [INFO] \n",
      "Procesando: Russell_2000 (index_pricing)\n",
      "2025-03-31 11:48:23,581 [INFO] - Archivo: Russell_2000\n",
      "2025-03-31 11:48:23,581 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:23,583 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\Russell_2000.csv\n",
      "2025-03-31 11:48:23,593 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:23,602 [INFO] Formato de fecha detectado para Russell_2000: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:23,865 [INFO] Ejemplos de fechas convertidas para Russell_2000: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:23,866 [INFO] Formato numérico detectado para Russell_2000: americano\n",
      "2025-03-31 11:48:23,884 [INFO] Para Russell_2000 (columna Fecha), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:23,885 [INFO] - Russell_2000: 2840 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:23,887 [INFO] \n",
      "Procesando: FTSE_100 (index_pricing)\n",
      "2025-03-31 11:48:23,888 [INFO] - Archivo: FTSE_100\n",
      "2025-03-31 11:48:23,890 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:23,890 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\FTSE_100.csv\n",
      "2025-03-31 11:48:23,909 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:23,921 [INFO] Formato de fecha detectado para FTSE_100: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:24,998 [INFO] Ejemplos de fechas convertidas para FTSE_100: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:25,000 [INFO] Formato numérico detectado para FTSE_100: americano\n",
      "2025-03-31 11:48:25,010 [INFO] Para FTSE_100 (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:25,010 [INFO] - FTSE_100: 2840 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:25,013 [INFO] \n",
      "Procesando: Nikkei_225 (index_pricing)\n",
      "2025-03-31 11:48:25,013 [INFO] - Archivo: Nikkei_225\n",
      "2025-03-31 11:48:25,015 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:25,015 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\Nikkei_225.csv\n",
      "2025-03-31 11:48:25,029 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:25,040 [INFO] Formato de fecha detectado para Nikkei_225: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:25,305 [INFO] Ejemplos de fechas convertidas para Nikkei_225: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:25,306 [INFO] Formato numérico detectado para Nikkei_225: americano\n",
      "2025-03-31 11:48:25,317 [INFO] Para Nikkei_225 (columna Fecha), la fecha mínima es 2014-01-06 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:25,319 [INFO] - Nikkei_225: 2741 filas procesadas, periodo: 2014-01-06 a 2025-03-27\n",
      "2025-03-31 11:48:25,320 [INFO] \n",
      "Procesando: DAX_30 (index_pricing)\n",
      "2025-03-31 11:48:25,322 [INFO] - Archivo: DAX_30\n",
      "2025-03-31 11:48:25,322 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:25,322 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\DAX_30.csv\n",
      "2025-03-31 11:48:25,343 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:25,357 [INFO] Formato de fecha detectado para DAX_30: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:25,616 [INFO] Ejemplos de fechas convertidas para DAX_30: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:25,618 [INFO] Formato numérico detectado para DAX_30: americano\n",
      "2025-03-31 11:48:25,635 [INFO] Para DAX_30 (columna Fecha), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:25,638 [INFO] - DAX_30: 2851 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:25,649 [INFO] \n",
      "Procesando: Shanghai_Composite (index_pricing)\n",
      "2025-03-31 11:48:25,650 [INFO] - Archivo: Shanghai_Composite\n",
      "2025-03-31 11:48:25,650 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:25,652 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\Shanghai_Composite.csv\n",
      "2025-03-31 11:48:25,671 [INFO] Columna de fecha detectada: Date\n",
      "2025-03-31 11:48:25,680 [INFO] Formato de fecha detectado para Shanghai_Composite: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:26,491 [INFO] Ejemplos de fechas convertidas para Shanghai_Composite: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:26,493 [INFO] Formato numérico detectado para Shanghai_Composite: americano\n",
      "2025-03-31 11:48:26,504 [INFO] Para Shanghai_Composite (columna Date), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:26,506 [INFO] - Shanghai_Composite: 2731 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:26,507 [INFO] \n",
      "Procesando: VIX_VolatilityIndex (index_pricing)\n",
      "2025-03-31 11:48:26,509 [INFO] - Archivo: VIX_VolatilityIndex\n",
      "2025-03-31 11:48:26,509 [INFO] - Columna TARGET: ULTIMO\n",
      "2025-03-31 11:48:26,510 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\VIX_VolatilityIndex.csv\n",
      "2025-03-31 11:48:26,519 [INFO] Columna de fecha detectada: Fecha\n",
      "2025-03-31 11:48:26,529 [INFO] Formato de fecha detectado para VIX_VolatilityIndex: dayfirst=False (confianza: 0.00)\n",
      "2025-03-31 11:48:26,791 [INFO] Ejemplos de fechas convertidas para VIX_VolatilityIndex: [Timestamp('2025-03-27 00:00:00'), Timestamp('2025-03-26 00:00:00'), Timestamp('2025-03-25 00:00:00'), Timestamp('2025-03-24 00:00:00'), Timestamp('2025-03-21 00:00:00')]\n",
      "2025-03-31 11:48:26,794 [INFO] Formato numérico detectado para VIX_VolatilityIndex: americano\n",
      "2025-03-31 11:48:26,806 [INFO] Para VIX_VolatilityIndex (columna Fecha), la fecha mínima es 2014-01-02 00:00:00 y la fecha máxima es 2025-03-27 00:00:00\n",
      "2025-03-31 11:48:26,808 [INFO] - VIX_VolatilityIndex: 2860 filas procesadas, periodo: 2014-01-02 a 2025-03-27\n",
      "2025-03-31 11:48:26,811 [INFO] \n",
      "Generando índice temporal diario...\n",
      "2025-03-31 11:48:26,811 [INFO] Archivo con fecha mínima global: Australia_10Y_Bond (2014-01-01)\n",
      "2025-03-31 11:48:26,813 [INFO] Archivo con fecha máxima global: EUR_USD_Spot (2025-03-27)\n",
      "2025-03-31 11:48:26,816 [INFO] - Total de fechas diarias generadas: 4104\n",
      "2025-03-31 11:48:26,817 [INFO] \n",
      "Combinando datos con índice diario (usando join para reducir consumo de memoria)...\n",
      "2025-03-31 11:48:26,820 [INFO] - Combinando: PRICE_Australia_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,827 [INFO] - Combinando: PRICE_Italy_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,831 [INFO] - Combinando: PRICE_Japan_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,836 [INFO] - Combinando: PRICE_UK_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,841 [INFO] - Combinando: PRICE_Germany_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,845 [INFO] - Combinando: PRICE_Canada_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,851 [INFO] - Combinando: PRICE_China_10Y_Bond_bond\n",
      "2025-03-31 11:48:26,856 [INFO] - Combinando: PRICE_CrudeOil_WTI_commodities\n",
      "2025-03-31 11:48:26,861 [INFO] - Combinando: PRICE_Gold_Spot_commodities\n",
      "2025-03-31 11:48:26,865 [INFO] - Combinando: PRICE_Silver_Spot_commodities\n",
      "2025-03-31 11:48:26,868 [INFO] - Combinando: PRICE_Copper_Futures_commodities\n",
      "2025-03-31 11:48:26,873 [INFO] - Combinando: PRICE_Platinum_Spot_commodities\n",
      "2025-03-31 11:48:26,878 [INFO] - Combinando: PRICE_EUR_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,881 [INFO] - Combinando: PRICE_GBP_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,882 [INFO] - Combinando: PRICE_JPY_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,887 [INFO] - Combinando: PRICE_CNY_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,889 [INFO] - Combinando: PRICE_AUD_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,892 [INFO] - Combinando: PRICE_CAD_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,895 [INFO] - Combinando: PRICE_MXN_USD_Spot_exchange_rate\n",
      "2025-03-31 11:48:26,898 [INFO] - Combinando: PRICE_EUR_GBP_Cross_exchange_rate\n",
      "2025-03-31 11:48:26,899 [INFO] - Combinando: ULTIMO_S&P500_Index_index_pricing\n",
      "2025-03-31 11:48:26,902 [INFO] - Combinando: ULTIMO_NASDAQ_Composite_index_pricing\n",
      "2025-03-31 11:48:26,906 [INFO] - Combinando: ULTIMO_Russell_2000_index_pricing\n",
      "2025-03-31 11:48:26,909 [INFO] - Combinando: ULTIMO_FTSE_100_index_pricing\n",
      "2025-03-31 11:48:26,912 [INFO] - Combinando: ULTIMO_Nikkei_225_index_pricing\n",
      "2025-03-31 11:48:26,915 [INFO] - Combinando: ULTIMO_DAX_30_index_pricing\n",
      "2025-03-31 11:48:26,918 [INFO] - Combinando: PRICE_Shanghai_Composite_index_pricing\n",
      "2025-03-31 11:48:26,921 [INFO] - Combinando: ULTIMO_VIX_VolatilityIndex_index_pricing\n",
      "2025-03-31 11:48:26,924 [INFO] - DataFrame combinado: 4104 filas, 29 columnas\n",
      "2025-03-31 11:48:26,926 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:26,927 [INFO] RESUMEN DE COBERTURA FINAL\n",
      "2025-03-31 11:48:26,927 [INFO] ==================================================\n",
      "2025-03-31 11:48:26,929 [INFO] Total indicadores procesados: 28\n",
      "2025-03-31 11:48:26,930 [INFO] Rango de fechas: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:26,930 [INFO] Total días en la serie: 4104\n",
      "2025-03-31 11:48:26,932 [INFO] - Australia_10Y_Bond (PRICE_Australia_10Y_Bond_bond): Cobertura aproximada 92.84%\n",
      "2025-03-31 11:48:26,932 [INFO] - Italy_10Y_Bond (PRICE_Italy_10Y_Bond_bond): Cobertura aproximada 78.90%\n",
      "2025-03-31 11:48:26,933 [INFO] - Japan_10Y_Bond (PRICE_Japan_10Y_Bond_bond): Cobertura aproximada 78.05%\n",
      "2025-03-31 11:48:26,935 [INFO] - UK_10Y_Bond (PRICE_UK_10Y_Bond_bond): Cobertura aproximada 83.43%\n",
      "2025-03-31 11:48:26,935 [INFO] - Germany_10Y_Bond (PRICE_Germany_10Y_Bond_bond): Cobertura aproximada 74.93%\n",
      "2025-03-31 11:48:26,936 [INFO] - Canada_10Y_Bond (PRICE_Canada_10Y_Bond_bond): Cobertura aproximada 72.25%\n",
      "2025-03-31 11:48:26,936 [INFO] - China_10Y_Bond (PRICE_China_10Y_Bond_bond): Cobertura aproximada 71.00%\n",
      "2025-03-31 11:48:26,936 [INFO] - CrudeOil_WTI (PRICE_CrudeOil_WTI_commodities): Cobertura aproximada 71.95%\n",
      "2025-03-31 11:48:26,938 [INFO] - Gold_Spot (PRICE_Gold_Spot_commodities): Cobertura aproximada 70.03%\n",
      "2025-03-31 11:48:26,939 [INFO] - Silver_Spot (PRICE_Silver_Spot_commodities): Cobertura aproximada 70.93%\n",
      "2025-03-31 11:48:26,939 [INFO] - Copper_Futures (PRICE_Copper_Futures_commodities): Cobertura aproximada 70.86%\n",
      "2025-03-31 11:48:26,941 [INFO] - Platinum_Spot (PRICE_Platinum_Spot_commodities): Cobertura aproximada 84.38%\n",
      "2025-03-31 11:48:26,941 [INFO] - EUR_USD_Spot (PRICE_EUR_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,942 [INFO] - GBP_USD_Spot (PRICE_GBP_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,942 [INFO] - JPY_USD_Spot (PRICE_JPY_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,942 [INFO] - CNY_USD_Spot (PRICE_CNY_USD_Spot_exchange_rate): Cobertura aproximada 71.47%\n",
      "2025-03-31 11:48:26,944 [INFO] - AUD_USD_Spot (PRICE_AUD_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,944 [INFO] - CAD_USD_Spot (PRICE_CAD_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,944 [INFO] - MXN_USD_Spot (PRICE_MXN_USD_Spot_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,945 [INFO] - EUR_GBP_Cross (PRICE_EUR_GBP_Cross_exchange_rate): Cobertura aproximada 71.44%\n",
      "2025-03-31 11:48:26,945 [INFO] - S&P500_Index (ULTIMO_S&P500_Index_index_pricing): Cobertura aproximada 68.86%\n",
      "2025-03-31 11:48:26,945 [INFO] - NASDAQ_Composite (ULTIMO_NASDAQ_Composite_index_pricing): Cobertura aproximada 68.86%\n",
      "2025-03-31 11:48:26,947 [INFO] - Russell_2000 (ULTIMO_Russell_2000_index_pricing): Cobertura aproximada 69.20%\n",
      "2025-03-31 11:48:26,947 [INFO] - FTSE_100 (ULTIMO_FTSE_100_index_pricing): Cobertura aproximada 69.20%\n",
      "2025-03-31 11:48:26,948 [INFO] - Nikkei_225 (ULTIMO_Nikkei_225_index_pricing): Cobertura aproximada 66.79%\n",
      "2025-03-31 11:48:26,948 [INFO] - DAX_30 (ULTIMO_DAX_30_index_pricing): Cobertura aproximada 69.47%\n",
      "2025-03-31 11:48:26,948 [INFO] - Shanghai_Composite (PRICE_Shanghai_Composite_index_pricing): Cobertura aproximada 66.54%\n",
      "2025-03-31 11:48:26,950 [INFO] - VIX_VolatilityIndex (ULTIMO_VIX_VolatilityIndex_index_pricing): Cobertura aproximada 69.69%\n",
      "2025-03-31 11:48:26,950 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:26,950 [INFO] ESTADÍSTICAS DE VALORES\n",
      "2025-03-31 11:48:26,952 [INFO] ==================================================\n",
      "2025-03-31 11:48:26,953 [INFO] \n",
      "Estadísticas para PRICE_Australia_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,953 [INFO] - Min: 0.6010\n",
      "2025-03-31 11:48:26,955 [INFO] - Max: 4.9680\n",
      "2025-03-31 11:48:26,955 [INFO] - Media: 2.7097\n",
      "2025-03-31 11:48:26,955 [INFO] - Mediana: 2.7025\n",
      "2025-03-31 11:48:26,956 [INFO] - Desv. Estándar: 1.0845\n",
      "2025-03-31 11:48:26,958 [INFO] \n",
      "Estadísticas para PRICE_Italy_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,958 [INFO] - Min: 0.4610\n",
      "2025-03-31 11:48:26,958 [INFO] - Max: 4.9880\n",
      "2025-03-31 11:48:26,958 [INFO] - Media: 2.3621\n",
      "2025-03-31 11:48:26,959 [INFO] - Mediana: 2.1180\n",
      "2025-03-31 11:48:26,959 [INFO] - Desv. Estándar: 1.1532\n",
      "2025-03-31 11:48:26,962 [INFO] \n",
      "Estadísticas para PRICE_Japan_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,964 [INFO] - Min: -0.2910\n",
      "2025-03-31 11:48:26,964 [INFO] - Max: 1.5610\n",
      "2025-03-31 11:48:26,964 [INFO] - Media: 0.2615\n",
      "2025-03-31 11:48:26,965 [INFO] - Mediana: 0.1040\n",
      "2025-03-31 11:48:26,965 [INFO] - Desv. Estándar: 0.3457\n",
      "2025-03-31 11:48:26,967 [INFO] \n",
      "Estadísticas para PRICE_UK_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,967 [INFO] - Min: 0.0750\n",
      "2025-03-31 11:48:26,968 [INFO] - Max: 4.8850\n",
      "2025-03-31 11:48:26,970 [INFO] - Media: 1.9328\n",
      "2025-03-31 11:48:26,970 [INFO] - Mediana: 1.4620\n",
      "2025-03-31 11:48:26,971 [INFO] - Desv. Estándar: 1.3117\n",
      "2025-03-31 11:48:26,973 [INFO] \n",
      "Estadísticas para PRICE_Germany_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,973 [INFO] - Min: -0.8540\n",
      "2025-03-31 11:48:26,973 [INFO] - Max: 2.9680\n",
      "2025-03-31 11:48:26,973 [INFO] - Media: 0.7411\n",
      "2025-03-31 11:48:26,974 [INFO] - Mediana: 0.4270\n",
      "2025-03-31 11:48:26,976 [INFO] - Desv. Estándar: 1.0257\n",
      "2025-03-31 11:48:26,978 [INFO] \n",
      "Estadísticas para PRICE_Canada_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,978 [INFO] - Min: 0.4350\n",
      "2025-03-31 11:48:26,978 [INFO] - Max: 4.2750\n",
      "2025-03-31 11:48:26,979 [INFO] - Media: 2.0485\n",
      "2025-03-31 11:48:26,979 [INFO] - Mediana: 1.8740\n",
      "2025-03-31 11:48:26,981 [INFO] - Desv. Estándar: 0.8708\n",
      "2025-03-31 11:48:26,984 [INFO] \n",
      "Estadísticas para PRICE_China_10Y_Bond_bond:\n",
      "2025-03-31 11:48:26,984 [INFO] - Min: 1.6070\n",
      "2025-03-31 11:48:26,985 [INFO] - Max: 4.7100\n",
      "2025-03-31 11:48:26,987 [INFO] - Media: 3.1233\n",
      "2025-03-31 11:48:26,987 [INFO] - Mediana: 3.0940\n",
      "2025-03-31 11:48:26,988 [INFO] - Desv. Estándar: 0.5733\n",
      "2025-03-31 11:48:26,991 [INFO] \n",
      "Estadísticas para PRICE_CrudeOil_WTI_commodities:\n",
      "2025-03-31 11:48:26,994 [INFO] - Min: 11.5700\n",
      "2025-03-31 11:48:26,996 [INFO] - Max: 119.7800\n",
      "2025-03-31 11:48:26,997 [INFO] - Media: 64.8420\n",
      "2025-03-31 11:48:26,999 [INFO] - Mediana: 63.3800\n",
      "2025-03-31 11:48:26,999 [INFO] - Desv. Estándar: 19.1693\n",
      "2025-03-31 11:48:27,002 [INFO] \n",
      "Estadísticas para PRICE_Gold_Spot_commodities:\n",
      "2025-03-31 11:48:27,003 [INFO] - Min: 1049.6000\n",
      "2025-03-31 11:48:27,003 [INFO] - Max: 3071.3000\n",
      "2025-03-31 11:48:27,005 [INFO] - Media: 1601.5690\n",
      "2025-03-31 11:48:27,005 [INFO] - Mediana: 1462.9000\n",
      "2025-03-31 11:48:27,006 [INFO] - Desv. Estándar: 427.1199\n",
      "2025-03-31 11:48:27,008 [INFO] \n",
      "Estadísticas para PRICE_Silver_Spot_commodities:\n",
      "2025-03-31 11:48:27,008 [INFO] - Min: 11.7720\n",
      "2025-03-31 11:48:27,009 [INFO] - Max: 35.0410\n",
      "2025-03-31 11:48:27,009 [INFO] - Media: 20.3172\n",
      "2025-03-31 11:48:27,011 [INFO] - Mediana: 18.6510\n",
      "2025-03-31 11:48:27,011 [INFO] - Desv. Estándar: 4.8965\n",
      "2025-03-31 11:48:27,013 [INFO] \n",
      "Estadísticas para PRICE_Copper_Futures_commodities:\n",
      "2025-03-31 11:48:27,014 [INFO] - Min: 1.9435\n",
      "2025-03-31 11:48:27,014 [INFO] - Max: 5.2430\n",
      "2025-03-31 11:48:27,016 [INFO] - Media: 3.2489\n",
      "2025-03-31 11:48:27,016 [INFO] - Mediana: 3.0675\n",
      "2025-03-31 11:48:27,017 [INFO] - Desv. Estándar: 0.7632\n",
      "2025-03-31 11:48:27,019 [INFO] \n",
      "Estadísticas para PRICE_Platinum_Spot_commodities:\n",
      "2025-03-31 11:48:27,019 [INFO] - Min: 595.2000\n",
      "2025-03-31 11:48:27,019 [INFO] - Max: 1516.2000\n",
      "2025-03-31 11:48:27,020 [INFO] - Media: 1001.9474\n",
      "2025-03-31 11:48:27,020 [INFO] - Mediana: 966.6500\n",
      "2025-03-31 11:48:27,020 [INFO] - Desv. Estándar: 157.0175\n",
      "2025-03-31 11:48:27,022 [INFO] \n",
      "Estadísticas para PRICE_EUR_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,023 [INFO] - Min: 0.9592\n",
      "2025-03-31 11:48:27,025 [INFO] - Max: 1.3933\n",
      "2025-03-31 11:48:27,025 [INFO] - Media: 1.1360\n",
      "2025-03-31 11:48:27,026 [INFO] - Mediana: 1.1192\n",
      "2025-03-31 11:48:27,026 [INFO] - Desv. Estándar: 0.0800\n",
      "2025-03-31 11:48:27,028 [INFO] \n",
      "Estadísticas para PRICE_GBP_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,029 [INFO] - Min: 1.0684\n",
      "2025-03-31 11:48:27,029 [INFO] - Max: 1.7163\n",
      "2025-03-31 11:48:27,031 [INFO] - Media: 1.3479\n",
      "2025-03-31 11:48:27,031 [INFO] - Mediana: 1.3062\n",
      "2025-03-31 11:48:27,032 [INFO] - Desv. Estándar: 0.1300\n",
      "2025-03-31 11:48:27,032 [INFO] \n",
      "Estadísticas para PRICE_JPY_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,034 [INFO] - Min: 99.8700\n",
      "2025-03-31 11:48:27,034 [INFO] - Max: 161.6800\n",
      "2025-03-31 11:48:27,034 [INFO] - Media: 119.5522\n",
      "2025-03-31 11:48:27,034 [INFO] - Mediana: 112.5050\n",
      "2025-03-31 11:48:27,035 [INFO] - Desv. Estándar: 16.2139\n",
      "2025-03-31 11:48:27,035 [INFO] \n",
      "Estadísticas para PRICE_CNY_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,037 [INFO] - Min: 6.0402\n",
      "2025-03-31 11:48:27,037 [INFO] - Max: 7.3430\n",
      "2025-03-31 11:48:27,039 [INFO] - Media: 6.7138\n",
      "2025-03-31 11:48:27,039 [INFO] - Mediana: 6.7204\n",
      "2025-03-31 11:48:27,040 [INFO] - Desv. Estándar: 0.3520\n",
      "2025-03-31 11:48:27,040 [INFO] \n",
      "Estadísticas para PRICE_AUD_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,042 [INFO] - Min: 0.5741\n",
      "2025-03-31 11:48:27,042 [INFO] - Max: 0.9496\n",
      "2025-03-31 11:48:27,042 [INFO] - Media: 0.7311\n",
      "2025-03-31 11:48:27,042 [INFO] - Mediana: 0.7226\n",
      "2025-03-31 11:48:27,043 [INFO] - Desv. Estándar: 0.0706\n",
      "2025-03-31 11:48:27,045 [INFO] \n",
      "Estadísticas para PRICE_CAD_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,046 [INFO] - Min: 1.0631\n",
      "2025-03-31 11:48:27,048 [INFO] - Max: 1.4576\n",
      "2025-03-31 11:48:27,048 [INFO] - Media: 1.2978\n",
      "2025-03-31 11:48:27,049 [INFO] - Mediana: 1.3135\n",
      "2025-03-31 11:48:27,049 [INFO] - Desv. Estándar: 0.0775\n",
      "2025-03-31 11:48:27,052 [INFO] \n",
      "Estadísticas para PRICE_MXN_USD_Spot_exchange_rate:\n",
      "2025-03-31 11:48:27,052 [INFO] - Min: 12.8375\n",
      "2025-03-31 11:48:27,052 [INFO] - Max: 25.3380\n",
      "2025-03-31 11:48:27,054 [INFO] - Media: 18.5027\n",
      "2025-03-31 11:48:27,054 [INFO] - Mediana: 18.9318\n",
      "2025-03-31 11:48:27,055 [INFO] - Desv. Estándar: 2.3166\n",
      "2025-03-31 11:48:27,057 [INFO] \n",
      "Estadísticas para PRICE_EUR_GBP_Cross_exchange_rate:\n",
      "2025-03-31 11:48:27,057 [INFO] - Min: 0.6937\n",
      "2025-03-31 11:48:27,058 [INFO] - Max: 0.9396\n",
      "2025-03-31 11:48:27,058 [INFO] - Media: 0.8457\n",
      "2025-03-31 11:48:27,060 [INFO] - Mediana: 0.8568\n",
      "2025-03-31 11:48:27,060 [INFO] - Desv. Estándar: 0.0493\n",
      "2025-03-31 11:48:27,061 [INFO] \n",
      "Estadísticas para ULTIMO_S&P500_Index_index_pricing:\n",
      "2025-03-31 11:48:27,061 [INFO] - Min: 1741.9000\n",
      "2025-03-31 11:48:27,063 [INFO] - Max: 6144.1500\n",
      "2025-03-31 11:48:27,063 [INFO] - Media: 3283.5838\n",
      "2025-03-31 11:48:27,063 [INFO] - Mediana: 2914.0000\n",
      "2025-03-31 11:48:27,064 [INFO] - Desv. Estándar: 1166.1993\n",
      "2025-03-31 11:48:27,066 [INFO] \n",
      "Estadísticas para ULTIMO_NASDAQ_Composite_index_pricing:\n",
      "2025-03-31 11:48:27,068 [INFO] - Min: 3996.9600\n",
      "2025-03-31 11:48:27,068 [INFO] - Max: 20173.8900\n",
      "2025-03-31 11:48:27,068 [INFO] - Media: 9561.0893\n",
      "2025-03-31 11:48:27,069 [INFO] - Mediana: 8028.2300\n",
      "2025-03-31 11:48:27,071 [INFO] - Desv. Estándar: 4414.3752\n",
      "2025-03-31 11:48:27,071 [INFO] \n",
      "Estadísticas para ULTIMO_Russell_2000_index_pricing:\n",
      "2025-03-31 11:48:27,072 [INFO] - Min: 953.7200\n",
      "2025-03-31 11:48:27,072 [INFO] - Max: 2442.7400\n",
      "2025-03-31 11:48:27,074 [INFO] - Media: 1622.3269\n",
      "2025-03-31 11:48:27,074 [INFO] - Mediana: 1565.7500\n",
      "2025-03-31 11:48:27,075 [INFO] - Desv. Estándar: 379.5475\n",
      "2025-03-31 11:48:27,075 [INFO] \n",
      "Estadísticas para ULTIMO_FTSE_100_index_pricing:\n",
      "2025-03-31 11:48:27,077 [INFO] - Min: 4993.8900\n",
      "2025-03-31 11:48:27,078 [INFO] - Max: 8871.3100\n",
      "2025-03-31 11:48:27,079 [INFO] - Media: 7130.4668\n",
      "2025-03-31 11:48:27,081 [INFO] - Mediana: 7198.7000\n",
      "2025-03-31 11:48:27,082 [INFO] - Desv. Estándar: 634.3461\n",
      "2025-03-31 11:48:27,084 [INFO] \n",
      "Estadísticas para ULTIMO_Nikkei_225_index_pricing:\n",
      "2025-03-31 11:48:27,086 [INFO] - Min: 13910.1600\n",
      "2025-03-31 11:48:27,087 [INFO] - Max: 42224.0200\n",
      "2025-03-31 11:48:27,088 [INFO] - Media: 24275.9486\n",
      "2025-03-31 11:48:27,089 [INFO] - Mediana: 22508.6900\n",
      "2025-03-31 11:48:27,091 [INFO] - Desv. Estándar: 6837.3404\n",
      "2025-03-31 11:48:27,093 [INFO] \n",
      "Estadísticas para ULTIMO_DAX_30_index_pricing:\n",
      "2025-03-31 11:48:27,094 [INFO] - Min: 8441.7100\n",
      "2025-03-31 11:48:27,095 [INFO] - Max: 23419.4800\n",
      "2025-03-31 11:48:27,096 [INFO] - Media: 13188.9842\n",
      "2025-03-31 11:48:27,097 [INFO] - Mediana: 12630.2300\n",
      "2025-03-31 11:48:27,098 [INFO] - Desv. Estándar: 2873.9918\n",
      "2025-03-31 11:48:27,101 [INFO] \n",
      "Estadísticas para PRICE_Shanghai_Composite_index_pricing:\n",
      "2025-03-31 11:48:27,103 [INFO] - Min: 1991.2500\n",
      "2025-03-31 11:48:27,104 [INFO] - Max: 5166.3500\n",
      "2025-03-31 11:48:27,105 [INFO] - Media: 3109.6248\n",
      "2025-03-31 11:48:27,106 [INFO] - Mediana: 3135.8900\n",
      "2025-03-31 11:48:27,107 [INFO] - Desv. Estándar: 427.9591\n",
      "2025-03-31 11:48:27,110 [INFO] \n",
      "Estadísticas para ULTIMO_VIX_VolatilityIndex_index_pricing:\n",
      "2025-03-31 11:48:27,111 [INFO] - Min: 9.1400\n",
      "2025-03-31 11:48:27,112 [INFO] - Max: 82.6900\n",
      "2025-03-31 11:48:27,112 [INFO] - Media: 17.8182\n",
      "2025-03-31 11:48:27,114 [INFO] - Mediana: 15.9600\n",
      "2025-03-31 11:48:27,115 [INFO] - Desv. Estándar: 6.9428\n",
      "2025-03-31 11:48:27,115 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:27,116 [INFO] GUARDANDO RESULTADOS\n",
      "2025-03-31 11:48:27,117 [INFO] ==================================================\n",
      "2025-03-31 11:48:27,117 [INFO] Guardando resultados en: datos_economicos_normales_procesados.xlsx\n",
      "2025-03-31 11:48:29,184 [INFO] Archivo guardado exitosamente: datos_economicos_normales_procesados.xlsx\n",
      "2025-03-31 11:48:29,186 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:29,186 [INFO] RESUMEN DE EJECUCIÓN\n",
      "2025-03-31 11:48:29,187 [INFO] ==================================================\n",
      "2025-03-31 11:48:29,187 [INFO] Proceso: MyinvestingreportNormal\n",
      "2025-03-31 11:48:29,189 [INFO] Tiempo de ejecución: 27.25 segundos\n",
      "2025-03-31 11:48:29,189 [INFO] Archivos procesados: 28\n",
      "2025-03-31 11:48:29,191 [INFO] Archivos con error: 0\n",
      "2025-03-31 11:48:29,191 [INFO] Archivos procesados correctamente: 28\n",
      "2025-03-31 11:48:29,191 [INFO] Periodo de datos: 2014-01-01 a 2025-03-27\n",
      "2025-03-31 11:48:29,192 [INFO] Datos combinados: 4104 filas, 29 columnas\n",
      "2025-03-31 11:48:29,192 [INFO] Archivo de salida: datos_economicos_normales_procesados.xlsx\n",
      "2025-03-31 11:48:29,192 [INFO] Estado: COMPLETADO\n",
      "2025-03-31 11:48:29,194 [INFO] ==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Configuración de logging\n",
    "# ---------------------------------------------------\n",
    "def configurar_logging(log_file='myinvestingreportnormal.log'):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('MyinvestingreportNormal')\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Función para convertir valores numéricos\n",
    "# ---------------------------------------------------\n",
    "def convertir_valor(valor, variable=None, formatos_conocidos=None):\n",
    "    \"\"\"\n",
    "    Convierte cualquier representación de valor numérico a float.\n",
    "    \"\"\"\n",
    "    if isinstance(valor, (int, float)):\n",
    "        return float(valor)\n",
    "    \n",
    "    if not isinstance(valor, str) or valor is None:\n",
    "        return None\n",
    "\n",
    "    valor_limpio = valor.strip()\n",
    "    if not valor_limpio:\n",
    "        return None\n",
    "\n",
    "    # Aplicar formato conocido si existe\n",
    "    if variable and formatos_conocidos and variable in formatos_conocidos:\n",
    "        formato = formatos_conocidos[variable]\n",
    "        if formato == 'europeo':\n",
    "            valor_limpio = valor_limpio.replace('.', '')\n",
    "            valor_limpio = valor_limpio.replace(',', '.')\n",
    "    \n",
    "    # Multiplicadores para sufijos (K, M, etc.)\n",
    "    multiplicadores = {'%': 1, 'K': 1e3, 'M': 1e6, 'B': 1e9, 'T': 1e12}\n",
    "    multiplicador = 1\n",
    "    for sufijo, mult in multiplicadores.items():\n",
    "        if valor_limpio.endswith(sufijo):\n",
    "            valor_limpio = valor_limpio.replace(sufijo, '')\n",
    "            multiplicador = mult\n",
    "            break\n",
    "\n",
    "    # Ajuste de separadores: si se detectan ambos, se decide según la posición\n",
    "    if ',' in valor_limpio and '.' in valor_limpio:\n",
    "        if valor_limpio.rfind(',') > valor_limpio.rfind('.'):\n",
    "            valor_limpio = valor_limpio.replace('.', '')\n",
    "            valor_limpio = valor_limpio.replace(',', '.')\n",
    "        else:\n",
    "            valor_limpio = valor_limpio.replace(',', '')\n",
    "    elif ',' in valor_limpio:\n",
    "        partes = valor_limpio.split(',')\n",
    "        if len(partes) == 2 and len(partes[1]) <= 2:\n",
    "            valor_limpio = valor_limpio.replace(',', '.')\n",
    "        else:\n",
    "            valor_limpio = valor_limpio.replace(',', '')\n",
    "    \n",
    "    try:\n",
    "        return float(valor_limpio) * multiplicador\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Detección dinámica del formato de fechas\n",
    "# ---------------------------------------------------\n",
    "def detectar_formato_fecha_inteligente(df, col_fecha, muestra_registros=10):\n",
    "    \"\"\"\n",
    "    Analiza una muestra de la columna de fecha para determinar si se debe forzar dayfirst=True.\n",
    "    Retorna un diccionario con {'dayfirst': bool, 'confianza': float}\n",
    "    \"\"\"\n",
    "    fecha_actual = pd.Timestamp(datetime.now().date())\n",
    "    muestras = df[col_fecha].dropna().astype(str).head(muestra_registros).tolist()\n",
    "    \n",
    "    resultados = {\n",
    "        'dayfirst': {'validas': 0, 'invalidas': 0, 'futuras': 0},\n",
    "        'no_dayfirst': {'validas': 0, 'invalidas': 0, 'futuras': 0}\n",
    "    }\n",
    "    \n",
    "    for fecha_str in muestras:\n",
    "        fecha_str = fecha_str.strip()\n",
    "        ambigua = False\n",
    "        # Identifica fechas con formato numérico separado por /, - o .\n",
    "        if re.match(r'^\\d{1,2}[\\/\\-\\.]\\d{1,2}[\\/\\-\\.]\\d{4}$', fecha_str):\n",
    "            separador = re.findall(r'[\\/\\-\\.]', fecha_str)[0]\n",
    "            partes = fecha_str.split(separador)\n",
    "            if len(partes) == 3:\n",
    "                try:\n",
    "                    p1, p2 = int(partes[0]), int(partes[1])\n",
    "                    # Si ambos números son menores o iguales a 12, es ambigua\n",
    "                    if p1 <= 12 and p2 <= 12:\n",
    "                        ambigua = True\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        for modo, dayfirst in [('dayfirst', True), ('no_dayfirst', False)]:\n",
    "            try:\n",
    "                fecha = pd.to_datetime(fecha_str, dayfirst=dayfirst)\n",
    "                if fecha > fecha_actual + pd.Timedelta(days=30):\n",
    "                    resultados[modo]['futuras'] += 1\n",
    "                else:\n",
    "                    resultados[modo]['validas'] += 1\n",
    "            except:\n",
    "                resultados[modo]['invalidas'] += 1\n",
    "\n",
    "    score_dayfirst = resultados['dayfirst']['validas'] - (resultados['dayfirst']['invalidas'] * 0.5) - (resultados['dayfirst']['futuras'] * 2)\n",
    "    score_no_dayfirst = resultados['no_dayfirst']['validas'] - (resultados['no_dayfirst']['invalidas'] * 0.5) - (resultados['no_dayfirst']['futuras'] * 2)\n",
    "    usar_dayfirst = score_dayfirst > score_no_dayfirst\n",
    "    confianza = abs(score_dayfirst - score_no_dayfirst) / (muestra_registros * 2)\n",
    "    return {'dayfirst': usar_dayfirst, 'confianza': confianza}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Conversión adaptativa de fechas\n",
    "# ---------------------------------------------------\n",
    "def convertir_fecha_adaptativo(fecha_str, configuracion_archivo=None):\n",
    "    \"\"\"\n",
    "    Convierte una fecha a pd.Timestamp usando la configuración detectada.\n",
    "    Si la cadena contiene puntos (.) y coincide con el patrón DD.MM.YYYY, se fuerza el formato.\n",
    "    \"\"\"\n",
    "    # Si ya es Timestamp o datetime, retornarla\n",
    "    if isinstance(fecha_str, (pd.Timestamp, datetime)):\n",
    "        return pd.Timestamp(fecha_str)\n",
    "    if pd.isna(fecha_str):\n",
    "        return None\n",
    "    fecha_str = str(fecha_str).strip()\n",
    "    # Si se detecta un punto como separador y el patrón es DD.MM.YYYY, forzamos el formato\n",
    "    if re.match(r'^\\d{1,2}\\.\\d{1,2}\\.\\d{4}$', fecha_str):\n",
    "        try:\n",
    "            fecha = pd.to_datetime(fecha_str, format='%d.%m.%Y', dayfirst=True)\n",
    "            return fecha\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Si hay configuración detectada, usarla\n",
    "    if configuracion_archivo is not None:\n",
    "        try:\n",
    "            fecha = pd.to_datetime(fecha_str, dayfirst=configuracion_archivo['dayfirst'])\n",
    "            return fecha\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Intento estándar sin forzar dayfirst\n",
    "    try:\n",
    "        fecha = pd.to_datetime(fecha_str)\n",
    "        return fecha\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Clase para gestionar y cachear formatos de fechas\n",
    "# ---------------------------------------------------\n",
    "class FormatosFechas:\n",
    "    \"\"\"Gestiona y cachea la configuración de conversión de fechas por archivo.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.formatos_cache = {}  # {variable: configuracion}\n",
    "    \n",
    "    def detectar_formato(self, df, col_fecha, variable=None):\n",
    "        configuracion = detectar_formato_fecha_inteligente(df, col_fecha)\n",
    "        if variable:\n",
    "            self.formatos_cache[variable] = configuracion\n",
    "        return configuracion\n",
    "        \n",
    "    def obtener_formato(self, variable):\n",
    "        return self.formatos_cache.get(variable)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Clase principal para procesar los datos económicos\n",
    "# ---------------------------------------------------\n",
    "class MyinvestingreportNormal:\n",
    "    \"\"\"\n",
    "    Procesa datos económicos con detección dinámica de formatos de fechas y \n",
    "    validación para evitar interpretaciones erróneas (como fechas en abril cuando\n",
    "    los datos crudos solo llegan hasta marzo).\n",
    "    \"\"\"\n",
    "    def __init__(self, config_file, data_root='data/Macro/raw', log_file='myinvestingreportnormal.log'):\n",
    "        self.config_file = config_file\n",
    "        self.data_root = data_root\n",
    "        self.logger = configurar_logging(log_file)\n",
    "        self.config_data = None\n",
    "        self.fecha_min_global = None\n",
    "        self.fecha_max_global = None\n",
    "        self.archivo_fecha_min = None\n",
    "        self.archivo_fecha_max = None\n",
    "        self.indice_diario = None\n",
    "        self.datos_procesados = {}\n",
    "        self.df_combinado = None\n",
    "        self.estadisticas = {}\n",
    "\n",
    "        self.formatos_numericos = {}  # Se usará un diccionario para formatos numéricos simples (si se desea ampliar)\n",
    "        self.formatos_fechas = FormatosFechas()\n",
    "        self.formatos_conocidos = {}  # Configuraciones exitosas de CSV\n",
    "\n",
    "        self.inicializar_formatos_conocidos()\n",
    "\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"INICIANDO PROCESO: MyinvestingreportNormal\")\n",
    "        self.logger.info(f\"Archivo de configuración: {config_file}\")\n",
    "        self.logger.info(f\"Directorio raíz de datos: {data_root}\")\n",
    "        self.logger.info(f\"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "\n",
    "    def inicializar_formatos_conocidos(self):\n",
    "        # Para algunos índices se conoce el formato numérico; aquí se puede ampliar si es necesario.\n",
    "        indices_europeos = ['Russell_2000', 'NASDAQ_Composite', 'S&P500_Index', 'Nikkei_225', 'DAX_30', 'VIX_VolatilityIndex']\n",
    "        for indice in indices_europeos:\n",
    "            self.formatos_conocidos[indice] = 'europeo'\n",
    "\n",
    "    def leer_configuracion(self):\n",
    "        self.logger.info(\"Leyendo archivo de configuración...\")\n",
    "        try:\n",
    "            df_config = pd.read_excel(self.config_file)\n",
    "            self.config_data = df_config[\n",
    "                (df_config['Fuente'] == 'Investing Data') &\n",
    "                (df_config['Tipo de Preprocesamiento Según la Fuente'] == 'Normal')\n",
    "            ].copy()\n",
    "            num_configs = len(self.config_data)\n",
    "            self.logger.info(f\"Se encontraron {num_configs} configuraciones para procesar\")\n",
    "            if num_configs == 0:\n",
    "                self.logger.warning(\"No se encontraron configuraciones que cumplan los criterios\")\n",
    "                return None\n",
    "            return self.config_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer configuración: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def encontrar_ruta_archivo(self, variable, tipo_macro):\n",
    "        ruta_base = os.path.join(self.data_root, tipo_macro)\n",
    "        nombre_archivo = f\"{variable}.csv\"\n",
    "        ruta_completa = os.path.join(ruta_base, nombre_archivo)\n",
    "        if os.path.exists(ruta_completa):\n",
    "            return ruta_completa\n",
    "        nombre_archivo_alt = f\"{variable}.xlsx\"\n",
    "        ruta_completa_alt = os.path.join(ruta_base, nombre_archivo_alt)\n",
    "        if os.path.exists(ruta_completa_alt):\n",
    "            return ruta_completa_alt\n",
    "        for root, dirs, files in os.walk(self.data_root):\n",
    "            if nombre_archivo in files:\n",
    "                return os.path.join(root, nombre_archivo)\n",
    "            if nombre_archivo_alt in files:\n",
    "                return os.path.join(root, nombre_archivo_alt)\n",
    "        return None\n",
    "\n",
    "    def leer_csv_adaptativo(self, ruta_archivo, variable):\n",
    "        configuraciones = [\n",
    "            {'sep': ',', 'decimal': '.', 'encoding': 'utf-8'},\n",
    "            {'sep': ',', 'decimal': '.', 'encoding': 'latin1'},\n",
    "            {'sep': ';', 'decimal': ',', 'thousands': '.', 'encoding': 'utf-8'},\n",
    "            {'sep': ';', 'decimal': ',', 'thousands': '.', 'encoding': 'latin1'},\n",
    "            {'sep': ',', 'decimal': ',', 'thousands': '.', 'encoding': 'utf-8'},\n",
    "            {'sep': '\\t', 'encoding': 'utf-8'},\n",
    "            {'sep': ' ', 'encoding': 'utf-8'}\n",
    "        ]\n",
    "        if variable in self.formatos_conocidos:\n",
    "            config_conocida = self.formatos_conocidos[variable]\n",
    "            try:\n",
    "                df = pd.read_csv(ruta_archivo, **config_conocida)\n",
    "                if len(df) > 0:\n",
    "                    return df\n",
    "            except Exception:\n",
    "                pass\n",
    "        errores = []\n",
    "        for idx, config in enumerate(configuraciones):\n",
    "            try:\n",
    "                df = pd.read_csv(ruta_archivo, **config)\n",
    "                if len(df) > 0:\n",
    "                    self.formatos_conocidos[variable] = config\n",
    "                    return df\n",
    "            except Exception as e:\n",
    "                errores.append(f\"Config {idx}: {str(e)}\")\n",
    "        self.logger.error(f\"No se pudo leer {ruta_archivo} con ninguna configuración\")\n",
    "        for error in errores:\n",
    "            self.logger.debug(f\"- {error}\")\n",
    "        return None\n",
    "\n",
    "    def detectar_columnas(self, df):\n",
    "        candidatos_fecha = [col for col in df.columns if any(\n",
    "            palabra in col.lower() for palabra in ['date', 'fecha', 'time', 'día', 'day', 'periodo']\n",
    "        )]\n",
    "        candidatos_valor = [col for col in df.columns if any(\n",
    "            palabra in col.lower() for palabra in ['price', 'precio', 'close', 'cierre', 'último', 'ultimo', 'valor', 'value']\n",
    "        )]\n",
    "        columnas_datetime = [col for col in df.columns if pd.api.types.is_datetime64_any_dtype(df[col])]\n",
    "        columna_fecha = None\n",
    "        if columnas_datetime:\n",
    "            columna_fecha = columnas_datetime[0]\n",
    "        elif candidatos_fecha:\n",
    "            for col in candidatos_fecha:\n",
    "                try:\n",
    "                    pd.to_datetime(df[col].iloc[:5])\n",
    "                    columna_fecha = col\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "        if columna_fecha is None and len(df.columns) > 0:\n",
    "            try:\n",
    "                pd.to_datetime(df[df.columns[0]].iloc[:5])\n",
    "                columna_fecha = df.columns[0]\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        columna_valor = None\n",
    "        if candidatos_valor:\n",
    "            columna_valor = candidatos_valor[0]\n",
    "        elif len(df.columns) > 1:\n",
    "            columna_valor = df.columns[1]\n",
    "        return columna_fecha, columna_valor\n",
    "\n",
    "    def convertir_fecha(self, fecha_str, configuracion_archivo=None):\n",
    "        return convertir_fecha_adaptativo(fecha_str, configuracion_archivo)\n",
    "\n",
    "    def limpiar_valor_porcentaje(self, valor, variable=None):\n",
    "        return convertir_valor(valor, variable, self.formatos_conocidos)\n",
    "\n",
    "    def procesar_archivo(self, config_row):\n",
    "        variable = config_row['Variable']\n",
    "        tipo_macro = config_row['Tipo Macro']\n",
    "        target_col = config_row['TARGET']\n",
    "\n",
    "        ruta_archivo = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "        self.logger.info(f\"\\nProcesando: {variable} ({tipo_macro})\")\n",
    "        self.logger.info(f\"- Archivo: {variable}\")\n",
    "        self.logger.info(f\"- Columna TARGET: {target_col}\")\n",
    "        if ruta_archivo is None:\n",
    "            self.logger.error(f\"- ERROR: Archivo no encontrado: {variable}\")\n",
    "            return variable, None\n",
    "        self.logger.info(f\"- Ruta encontrada: {ruta_archivo}\")\n",
    "\n",
    "        try:\n",
    "            _, extension = os.path.splitext(ruta_archivo)\n",
    "            extension = extension.lower()\n",
    "            if extension == '.csv':\n",
    "                df = self.leer_csv_adaptativo(ruta_archivo, variable)\n",
    "            elif extension in ['.xlsx', '.xls']:\n",
    "                df = pd.read_excel(ruta_archivo, engine='openpyxl')\n",
    "            else:\n",
    "                self.logger.error(f\"- ERROR: Formato de archivo no soportado: {extension}\")\n",
    "                return variable, None\n",
    "            if df is None or len(df) == 0:\n",
    "                self.logger.error(\"- ERROR: El archivo está vacío o no se pudo leer\")\n",
    "                return variable, None\n",
    "\n",
    "            col_fecha, col_valor = self.detectar_columnas(df)\n",
    "            if col_fecha is None or col_valor is None:\n",
    "                self.logger.error(\"- ERROR: No se pudieron detectar columnas necesarias (fecha o valor)\")\n",
    "                return variable, None\n",
    "\n",
    "            self.logger.info(f\"Columna de fecha detectada: {col_fecha}\")\n",
    "            # Detección dinámica del formato de fechas\n",
    "            config_fecha = self.formatos_fechas.detectar_formato(df, col_fecha, variable)\n",
    "            self.logger.info(f\"Formato de fecha detectado para {variable}: dayfirst={config_fecha['dayfirst']} (confianza: {config_fecha['confianza']:.2f})\")\n",
    "            \n",
    "            # Convertir fechas usando la configuración detectada\n",
    "            df['fecha'] = df[col_fecha].apply(lambda x: self.convertir_fecha(x, configuracion_archivo=config_fecha))\n",
    "            df = df.dropna(subset=['fecha'])\n",
    "            ejemplos = df['fecha'].head(5).tolist()\n",
    "            self.logger.info(f\"Ejemplos de fechas convertidas para {variable}: {ejemplos}\")\n",
    "            \n",
    "            # Validar fechas excesivamente futuras (más de 30 días a partir de hoy)\n",
    "            fecha_actual = pd.Timestamp(datetime.now().date())\n",
    "            fechas_futuras = df[df['fecha'] > fecha_actual + pd.Timedelta(days=30)]\n",
    "            if len(fechas_futuras) > 0:\n",
    "                self.logger.warning(f\"Se detectaron {len(fechas_futuras)} fechas futuras anómalas en {variable}\")\n",
    "                self.logger.warning(f\"Ejemplos de fechas futuras: {fechas_futuras['fecha'].head(3).tolist()}\")\n",
    "                df = df[df['fecha'] <= fecha_actual + pd.Timedelta(days=30)].copy()\n",
    "            \n",
    "            muestra_valores = df[col_valor].astype(str).head(20).tolist()\n",
    "            formato_detectado = \"americano\"  # Se usa como etiqueta básica\n",
    "            self.logger.info(f\"Formato numérico detectado para {variable}: {formato_detectado}\")\n",
    "\n",
    "            df['valor'] = df[col_valor].apply(lambda x: self.limpiar_valor_porcentaje(x, variable))\n",
    "            df = df.dropna(subset=['valor'])\n",
    "\n",
    "            total_filas = len(df)\n",
    "            if total_filas == 0:\n",
    "                self.logger.error(f\"- ERROR: No se encontraron valores válidos en {variable}\")\n",
    "                return variable, None\n",
    "\n",
    "            nuevo_nombre = f\"{target_col}_{variable}_{tipo_macro}\"\n",
    "            df.rename(columns={'valor': nuevo_nombre}, inplace=True)\n",
    "            df_procesado = df[['fecha', nuevo_nombre]].copy()\n",
    "            df_procesado = df_procesado.sort_values('fecha')\n",
    "\n",
    "            fecha_min = df_procesado['fecha'].min()\n",
    "            fecha_max = df_procesado['fecha'].max()\n",
    "            self.logger.info(f\"Para {variable} (columna {col_fecha}), la fecha mínima es {fecha_min} y la fecha máxima es {fecha_max}\")\n",
    "            \n",
    "            if self.fecha_min_global is None or fecha_min < self.fecha_min_global:\n",
    "                self.fecha_min_global = fecha_min\n",
    "                self.archivo_fecha_min = variable\n",
    "            if self.fecha_max_global is None or fecha_max > self.fecha_max_global:\n",
    "                self.fecha_max_global = fecha_max\n",
    "                self.archivo_fecha_max = variable\n",
    "\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col,\n",
    "                'total_filas': total_filas,\n",
    "                'valores_validos': df_procesado[nuevo_nombre].count(),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max,\n",
    "                'nuevo_nombre': nuevo_nombre,\n",
    "                'formato_fecha': f\"dayfirst={config_fecha['dayfirst']}\",\n",
    "                'confianza_formato': config_fecha['confianza']\n",
    "            }\n",
    "            self.logger.info(f\"- {variable}: {total_filas} filas procesadas, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            return variable, df_procesado\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"- ERROR al procesar {ruta_archivo}: {str(e)}\")\n",
    "            return variable, None\n",
    "\n",
    "    def generar_indice_diario(self):\n",
    "        if self.fecha_min_global is None or self.fecha_max_global is None:\n",
    "            self.logger.error(\"No se pudieron determinar fechas mínima y máxima globales\")\n",
    "            return None\n",
    "        self.logger.info(\"\\nGenerando índice temporal diario...\")\n",
    "        self.logger.info(f\"Archivo con fecha mínima global: {self.archivo_fecha_min} ({self.fecha_min_global.strftime('%Y-%m-%d')})\")\n",
    "        self.logger.info(f\"Archivo con fecha máxima global: {self.archivo_fecha_max} ({self.fecha_max_global.strftime('%Y-%m-%d')})\")\n",
    "        todas_fechas = pd.date_range(start=self.fecha_min_global, end=self.fecha_max_global, freq='D')\n",
    "        self.indice_diario = pd.DataFrame({'fecha': todas_fechas})\n",
    "        self.logger.info(f\"- Total de fechas diarias generadas: {len(self.indice_diario)}\")\n",
    "        return self.indice_diario\n",
    "\n",
    "    def combinar_datos(self):\n",
    "        if not self.datos_procesados:\n",
    "            self.logger.error(\"No hay datos procesados para combinar\")\n",
    "            return None\n",
    "        if self.indice_diario is None:\n",
    "            self.logger.error(\"No se ha generado el índice diario\")\n",
    "            return None\n",
    "        self.logger.info(\"\\nCombinando datos con índice diario (usando join para reducir consumo de memoria)...\")\n",
    "        df_combinado = self.indice_diario.copy().set_index('fecha')\n",
    "        for variable, df in self.datos_procesados.items():\n",
    "            if df is None:\n",
    "                self.logger.warning(f\"Omitiendo {variable} por errores de procesamiento\")\n",
    "                continue\n",
    "            nombre_col = df.columns[1]\n",
    "            self.logger.info(f\"- Combinando: {nombre_col}\")\n",
    "            df_temp = df.set_index('fecha')[[nombre_col]]\n",
    "            df_temp = df_temp[~df_temp.index.duplicated(keep='first')]\n",
    "            df_temp = df_temp.reindex(df_combinado.index)\n",
    "            df_temp = df_temp.ffill()\n",
    "            df_combinado = df_combinado.join(df_temp)\n",
    "        self.df_combinado = df_combinado.reset_index()\n",
    "        self.logger.info(f\"- DataFrame combinado: {len(self.df_combinado)} filas, {len(self.df_combinado.columns)} columnas\")\n",
    "        return self.df_combinado\n",
    "\n",
    "    def analizar_cobertura_final(self):\n",
    "        if self.df_combinado is None or not self.estadisticas:\n",
    "            self.logger.error(\"No hay datos combinados o estadísticas para analizar\")\n",
    "            return\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE COBERTURA FINAL\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        total_indicadores = len(self.estadisticas)\n",
    "        total_dias = len(self.indice_diario)\n",
    "        self.logger.info(f\"Total indicadores procesados: {total_indicadores}\")\n",
    "        self.logger.info(f\"Rango de fechas: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Total días en la serie: {total_dias}\")\n",
    "        for variable, stats in self.estadisticas.items():\n",
    "            cobertura = (stats['valores_validos'] / total_dias) * 100\n",
    "            self.logger.info(f\"- {variable} ({stats['nuevo_nombre']}): Cobertura aproximada {cobertura:.2f}%\")\n",
    "\n",
    "    def generar_estadisticas_valores(self):\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos combinados para analizar\")\n",
    "            return\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"ESTADÍSTICAS DE VALORES\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        for col in self.df_combinado.columns:\n",
    "            if col == 'fecha':\n",
    "                continue\n",
    "            serie = self.df_combinado[col].dropna()\n",
    "            if len(serie) == 0:\n",
    "                self.logger.warning(f\"La columna {col} no tiene valores\")\n",
    "                continue\n",
    "            stats = {\n",
    "                'min': serie.min(),\n",
    "                'max': serie.max(),\n",
    "                'mean': serie.mean(),\n",
    "                'median': serie.median(),\n",
    "                'std': serie.std()\n",
    "            }\n",
    "            self.logger.info(f\"\\nEstadísticas para {col}:\")\n",
    "            self.logger.info(f\"- Min: {stats['min']:.4f}\")\n",
    "            self.logger.info(f\"- Max: {stats['max']:.4f}\")\n",
    "            self.logger.info(f\"- Media: {stats['mean']:.4f}\")\n",
    "            self.logger.info(f\"- Mediana: {stats['median']:.4f}\")\n",
    "            self.logger.info(f\"- Desv. Estándar: {stats['std']:.4f}\")\n",
    "            if col in self.estadisticas:\n",
    "                self.estadisticas[col] = {**self.estadisticas.get(col, {}), **stats}\n",
    "\n",
    "    def guardar_resultados(self, output_file='datos_economicos_normales_procesados.xlsx'):\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos para guardar\")\n",
    "            return False\n",
    "        try:\n",
    "            self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "            self.logger.info(\"GUARDANDO RESULTADOS\")\n",
    "            self.logger.info(\"=\" * 50)\n",
    "            self.logger.info(f\"Guardando resultados en: {output_file}\")\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                self.df_combinado.to_excel(writer, sheet_name='Datos Diarios', index=False)\n",
    "                df_stats = pd.DataFrame()\n",
    "                for var, stats in self.estadisticas.items():\n",
    "                    serie = pd.Series(stats, name=var)\n",
    "                    df_temp = pd.DataFrame(serie).transpose()\n",
    "                    df_stats = pd.concat([df_stats, df_temp])\n",
    "                df_stats.to_excel(writer, sheet_name='Estadisticas')\n",
    "                metadata = {\n",
    "                    'Proceso': ['MyinvestingreportNormal'],\n",
    "                    'Fecha de proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                    'Total indicadores': [len(self.estadisticas)],\n",
    "                    'Periodo': [f\"{self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\"],\n",
    "                    'Total días': [len(self.indice_diario)]\n",
    "                }\n",
    "                pd.DataFrame(metadata).to_excel(writer, sheet_name='Metadatos')\n",
    "            self.logger.info(f\"Archivo guardado exitosamente: {output_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al guardar resultados: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def ejecutar_proceso_completo(self, output_file='datos_economicos_normales_procesados.xlsx'):\n",
    "        inicio = time.time()\n",
    "        self.logger.info(\"Iniciando proceso completo MyinvestingreportNormal...\")\n",
    "        self.leer_configuracion()\n",
    "        if self.config_data is None or len(self.config_data) == 0:\n",
    "            return False\n",
    "        for _, config_row in self.config_data.iterrows():\n",
    "            variable, df_procesado = self.procesar_archivo(config_row)\n",
    "            self.datos_procesados[variable] = df_procesado\n",
    "        archivos_correctos = sum(1 for df in self.datos_procesados.values() if df is not None)\n",
    "        if archivos_correctos == 0:\n",
    "            self.logger.error(\"No se pudo procesar correctamente ningún archivo\")\n",
    "            return False\n",
    "        self.generar_indice_diario()\n",
    "        if self.indice_diario is None:\n",
    "            return False\n",
    "        self.combinar_datos()\n",
    "        if self.df_combinado is None:\n",
    "            return False\n",
    "        self.analizar_cobertura_final()\n",
    "        self.generar_estadisticas_valores()\n",
    "        resultado = self.guardar_resultados(output_file)\n",
    "        fin = time.time()\n",
    "        tiempo_ejecucion = fin - inicio\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE EJECUCIÓN\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        self.logger.info(f\"Proceso: MyinvestingreportNormal\")\n",
    "        self.logger.info(f\"Tiempo de ejecución: {tiempo_ejecucion:.2f} segundos\")\n",
    "        self.logger.info(f\"Archivos procesados: {len(self.datos_procesados)}\")\n",
    "        self.logger.info(f\"Archivos con error: {sum(1 for df in self.datos_procesados.values() if df is None)}\")\n",
    "        self.logger.info(f\"Archivos procesados correctamente: {archivos_correctos}\")\n",
    "        self.logger.info(f\"Periodo de datos: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Datos combinados: {len(self.df_combinado)} filas, {len(self.df_combinado.columns)} columnas\")\n",
    "        self.logger.info(f\"Archivo de salida: {output_file}\")\n",
    "        self.logger.info(f\"Estado: {'COMPLETADO' if resultado else 'ERROR'}\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        return resultado\n",
    "\n",
    "def ejecutar_myinvestingreportnormal(config_file='Data Engineering.xlsx',\n",
    "                                     output_file='datos_economicos_normales_procesados.xlsx',\n",
    "                                     data_root='data/Macro/raw',\n",
    "                                     log_file='myinvestingreportnormal.log'):\n",
    "    procesador = MyinvestingreportNormal(config_file, data_root, log_file)\n",
    "    return procesador.ejecutar_proceso_completo(output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resultado = ejecutar_myinvestingreportnormal()\n",
    "    print(f\"Proceso {'completado exitosamente' if resultado else 'finalizado con errores'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRED-NORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 11:48:29,239 [INFO] ================================================================================\n",
      "2025-03-31 11:48:29,241 [INFO] INICIANDO PROCESO: FredDataProcessor\n",
      "2025-03-31 11:48:29,241 [INFO] Archivo de configuración: Data Engineering.xlsx\n",
      "2025-03-31 11:48:29,242 [INFO] Directorio raíz de datos: data/Macro/raw\n",
      "2025-03-31 11:48:29,244 [INFO] Fecha y hora: 2025-03-31 11:48:29\n",
      "2025-03-31 11:48:29,244 [INFO] ================================================================================\n",
      "2025-03-31 11:48:29,245 [INFO] Leyendo archivo de configuración...\n",
      "2025-03-31 11:48:29,268 [INFO] Se encontraron 35 configuraciones para procesar\n",
      "2025-03-31 11:48:29,270 [INFO] \n",
      "Procesando: US_10Y_Treasury (bond)\n",
      "2025-03-31 11:48:29,270 [INFO] - Archivo: US_10Y_Treasury.csv\n",
      "2025-03-31 11:48:29,270 [INFO] - Columna TARGET: DGS10\n",
      "2025-03-31 11:48:29,271 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\US_10Y_Treasury.csv\n",
      "2025-03-31 11:48:29,276 [INFO] - Filas encontradas: 2929\n",
      "2025-03-31 11:48:29,277 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:29,279 [INFO] Formato detectado para data/Macro/raw\\bond\\US_10Y_Treasury.csv: ISO\n",
      "2025-03-31 11:48:29,455 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-02 00:00:00'), Timestamp('2014-01-03 00:00:00'), Timestamp('2014-01-06 00:00:00'), Timestamp('2014-01-07 00:00:00'), Timestamp('2014-01-08 00:00:00')]\n",
      "2025-03-31 11:48:29,458 [INFO] - Valores no nulos en TARGET: 2808\n",
      "2025-03-31 11:48:29,458 [INFO] - Periodo: 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:29,460 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:29,461 [INFO] \n",
      "Procesando: US_2Y_Treasury (bond)\n",
      "2025-03-31 11:48:29,461 [INFO] - Archivo: US_2Y_Treasury.csv\n",
      "2025-03-31 11:48:29,463 [INFO] - Columna TARGET: DGS2\n",
      "2025-03-31 11:48:29,463 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\US_2Y_Treasury.csv\n",
      "2025-03-31 11:48:29,466 [INFO] - Filas encontradas: 2929\n",
      "2025-03-31 11:48:29,468 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:29,469 [INFO] Formato detectado para data/Macro/raw\\bond\\US_2Y_Treasury.csv: ISO\n",
      "2025-03-31 11:48:29,686 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-02 00:00:00'), Timestamp('2014-01-03 00:00:00'), Timestamp('2014-01-06 00:00:00'), Timestamp('2014-01-07 00:00:00'), Timestamp('2014-01-08 00:00:00')]\n",
      "2025-03-31 11:48:29,689 [INFO] - Valores no nulos en TARGET: 2808\n",
      "2025-03-31 11:48:29,691 [INFO] - Periodo: 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:29,691 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:29,694 [INFO] \n",
      "Procesando: Corporate_Bond_AAA_Spread (bond)\n",
      "2025-03-31 11:48:29,696 [INFO] - Archivo: Corporate_Bond_AAA_Spread.csv\n",
      "2025-03-31 11:48:29,697 [INFO] - Columna TARGET: AAA\n",
      "2025-03-31 11:48:29,697 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Corporate_Bond_AAA_Spread.csv\n",
      "2025-03-31 11:48:29,700 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:29,700 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:29,701 [INFO] Formato detectado para data/Macro/raw\\bond\\Corporate_Bond_AAA_Spread.csv: ISO\n",
      "2025-03-31 11:48:29,713 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:29,717 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:29,718 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:29,718 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:29,720 [INFO] \n",
      "Procesando: Corporate_Bond_BBB_Spread (bond)\n",
      "2025-03-31 11:48:29,721 [INFO] - Archivo: Corporate_Bond_BBB_Spread.csv\n",
      "2025-03-31 11:48:29,721 [INFO] - Columna TARGET: BAA10YM\n",
      "2025-03-31 11:48:29,723 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\Corporate_Bond_BBB_Spread.csv\n",
      "2025-03-31 11:48:29,726 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:29,727 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:29,727 [INFO] Formato detectado para data/Macro/raw\\bond\\Corporate_Bond_BBB_Spread.csv: ISO\n",
      "2025-03-31 11:48:29,738 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:29,741 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:29,743 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:29,743 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:29,746 [INFO] \n",
      "Procesando: High_Yield_Bond_Spread (bond)\n",
      "2025-03-31 11:48:29,746 [INFO] - Archivo: High_Yield_Bond_Spread.csv\n",
      "2025-03-31 11:48:29,746 [INFO] - Columna TARGET: BAMLH0A0HYM2\n",
      "2025-03-31 11:48:29,747 [INFO] - Ruta encontrada: data/Macro/raw\\bond\\High_Yield_Bond_Spread.csv\n",
      "2025-03-31 11:48:29,751 [INFO] - Filas encontradas: 2967\n",
      "2025-03-31 11:48:29,751 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:29,752 [INFO] Formato detectado para data/Macro/raw\\bond\\High_Yield_Bond_Spread.csv: ISO\n",
      "2025-03-31 11:48:29,983 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-02 00:00:00'), Timestamp('2014-01-03 00:00:00'), Timestamp('2014-01-06 00:00:00'), Timestamp('2014-01-07 00:00:00'), Timestamp('2014-01-08 00:00:00')]\n",
      "2025-03-31 11:48:29,988 [INFO] - Valores no nulos en TARGET: 2932\n",
      "2025-03-31 11:48:29,989 [INFO] - Periodo: 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:29,989 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,002 [INFO] \n",
      "Procesando: Denmark_Car_Registrations_MoM (car_registrations)\n",
      "2025-03-31 11:48:30,002 [INFO] - Archivo: Denmark_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,003 [INFO] - Columna TARGET: DNKSLRTCR03GPSAM\n",
      "2025-03-31 11:48:30,003 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\Denmark_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,008 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:30,009 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,009 [INFO] Formato detectado para data/Macro/raw\\car_registrations\\Denmark_Car_Registrations_MoM.csv: ISO\n",
      "2025-03-31 11:48:30,029 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,034 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:30,035 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,037 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,040 [INFO] \n",
      "Procesando: US_Car_Registrations_MoM (car_registrations)\n",
      "2025-03-31 11:48:30,042 [INFO] - Archivo: US_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,043 [INFO] - Columna TARGET: USASLRTCR03GPSAM\n",
      "2025-03-31 11:48:30,043 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\US_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,050 [INFO] - Filas encontradas: 132\n",
      "2025-03-31 11:48:30,051 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,053 [INFO] Formato detectado para data/Macro/raw\\car_registrations\\US_Car_Registrations_MoM.csv: ISO\n",
      "2025-03-31 11:48:30,070 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,074 [INFO] - Valores no nulos en TARGET: 132\n",
      "2025-03-31 11:48:30,076 [INFO] - Periodo: 2014-01-01 a 2024-12-01\n",
      "2025-03-31 11:48:30,078 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,081 [INFO] \n",
      "Procesando: SouthAfrica_Car_Registrations_MoM (car_registrations)\n",
      "2025-03-31 11:48:30,082 [INFO] - Archivo: SouthAfrica_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,082 [INFO] - Columna TARGET: ZAFSLRTCR03GPSAM\n",
      "2025-03-31 11:48:30,084 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\SouthAfrica_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,087 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,088 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,088 [INFO] Formato detectado para data/Macro/raw\\car_registrations\\SouthAfrica_Car_Registrations_MoM.csv: ISO\n",
      "2025-03-31 11:48:30,100 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,103 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,105 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,106 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,108 [INFO] \n",
      "Procesando: United_Kingdom_Car_Registrations_MoM (car_registrations)\n",
      "2025-03-31 11:48:30,108 [INFO] - Archivo: United_Kingdom_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,108 [INFO] - Columna TARGET: GBRSLRTCR03GPSAM\n",
      "2025-03-31 11:48:30,109 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\United_Kingdom_Car_Registrations_MoM.csv\n",
      "2025-03-31 11:48:30,111 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,112 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,112 [INFO] Formato detectado para data/Macro/raw\\car_registrations\\United_Kingdom_Car_Registrations_MoM.csv: ISO\n",
      "2025-03-31 11:48:30,126 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,129 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,131 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,132 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,136 [INFO] \n",
      "Procesando: Spain_Car_Registrations_MoM (car_registrations)\n",
      "2025-03-31 11:48:30,137 [INFO] - Archivo: Spain_Car_Registrations_MoM.xlsx\n",
      "2025-03-31 11:48:30,140 [INFO] - Columna TARGET: ESPSLRTCR03GPSAM\n",
      "2025-03-31 11:48:30,142 [INFO] - Ruta encontrada: data/Macro/raw\\car_registrations\\Spain_Car_Registrations_MoM.xlsx\n",
      "2025-03-31 11:48:30,154 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,155 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:30,157 [INFO] Formato detectado para data/Macro/raw\\car_registrations\\Spain_Car_Registrations_MoM.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:30,163 [INFO] Preferencia de dayfirst para data/Macro/raw\\car_registrations\\Spain_Car_Registrations_MoM.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:30,166 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,169 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,171 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,172 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,174 [INFO] \n",
      "Procesando: US_Commercial_Loans (comm_loans)\n",
      "2025-03-31 11:48:30,174 [INFO] - Archivo: US_Commercial_Loans.csv\n",
      "2025-03-31 11:48:30,175 [INFO] - Columna TARGET: BUSLOANS\n",
      "2025-03-31 11:48:30,175 [INFO] - Ruta encontrada: data/Macro/raw\\comm_loans\\US_Commercial_Loans.csv\n",
      "2025-03-31 11:48:30,178 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,178 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,180 [INFO] Formato detectado para data/Macro/raw\\comm_loans\\US_Commercial_Loans.csv: ISO\n",
      "2025-03-31 11:48:30,192 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,195 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,197 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,197 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,198 [INFO] \n",
      "Procesando: US_RealEstate_Commercial_Loans (comm_loans)\n",
      "2025-03-31 11:48:30,200 [INFO] - Archivo: US_RealEstate_Commercial_Loans.csv\n",
      "2025-03-31 11:48:30,200 [INFO] - Columna TARGET: CREACBM027NBOG\n",
      "2025-03-31 11:48:30,201 [INFO] - Ruta encontrada: data/Macro/raw\\comm_loans\\US_RealEstate_Commercial_Loans.csv\n",
      "2025-03-31 11:48:30,204 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,205 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,206 [INFO] Formato detectado para data/Macro/raw\\comm_loans\\US_RealEstate_Commercial_Loans.csv: ISO\n",
      "2025-03-31 11:48:30,219 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,222 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,224 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,225 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,227 [INFO] \n",
      "Procesando: US_Consumer_Credit (comm_loans)\n",
      "2025-03-31 11:48:30,228 [INFO] - Archivo: US_Consumer_Credit.csv\n",
      "2025-03-31 11:48:30,228 [INFO] - Columna TARGET: TOTALSL\n",
      "2025-03-31 11:48:30,229 [INFO] - Ruta encontrada: data/Macro/raw\\comm_loans\\US_Consumer_Credit.csv\n",
      "2025-03-31 11:48:30,232 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:30,234 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,235 [INFO] Formato detectado para data/Macro/raw\\comm_loans\\US_Consumer_Credit.csv: ISO\n",
      "2025-03-31 11:48:30,248 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,250 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:30,250 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,252 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,253 [INFO] \n",
      "Procesando: EuroZone_Consumer_Confidence (consumer_confidence)\n",
      "2025-03-31 11:48:30,253 [INFO] - Archivo: EuroZone_Consumer_Confidence.csv\n",
      "2025-03-31 11:48:30,255 [INFO] - Columna TARGET: CSCICP02EZM460S\n",
      "2025-03-31 11:48:30,255 [INFO] - Ruta encontrada: data/Macro/raw\\consumer_confidence\\EuroZone_Consumer_Confidence.csv\n",
      "2025-03-31 11:48:30,258 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,259 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,261 [INFO] Formato detectado para data/Macro/raw\\consumer_confidence\\EuroZone_Consumer_Confidence.csv: ISO\n",
      "2025-03-31 11:48:30,275 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,278 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,278 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,279 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,281 [INFO] \n",
      "Procesando: Switzerland_Consumer_Confidence (consumer_confidence)\n",
      "2025-03-31 11:48:30,281 [INFO] - Archivo: Switzerland_Consumer_Confidence.csv\n",
      "2025-03-31 11:48:30,281 [INFO] - Columna TARGET: CSCICP02CHQ460S\n",
      "2025-03-31 11:48:30,282 [INFO] - Ruta encontrada: data/Macro/raw\\consumer_confidence\\Switzerland_Consumer_Confidence.csv\n",
      "2025-03-31 11:48:30,285 [INFO] - Filas encontradas: 45\n",
      "2025-03-31 11:48:30,287 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,288 [INFO] Formato detectado para data/Macro/raw\\consumer_confidence\\Switzerland_Consumer_Confidence.csv: ISO\n",
      "2025-03-31 11:48:30,294 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-07-01 00:00:00'), Timestamp('2014-10-01 00:00:00'), Timestamp('2015-01-01 00:00:00')]\n",
      "2025-03-31 11:48:30,298 [INFO] - Valores no nulos en TARGET: 45\n",
      "2025-03-31 11:48:30,299 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,299 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,300 [INFO] \n",
      "Procesando: Michigan_Consumer_Sentiment (consumer_confidence)\n",
      "2025-03-31 11:48:30,300 [INFO] - Archivo: Michigan_Consumer_Sentiment.csv\n",
      "2025-03-31 11:48:30,300 [INFO] - Columna TARGET: UMCSENT\n",
      "2025-03-31 11:48:30,302 [INFO] - Ruta encontrada: data/Macro/raw\\consumer_confidence\\Michigan_Consumer_Sentiment.csv\n",
      "2025-03-31 11:48:30,306 [INFO] - Filas encontradas: 121\n",
      "2025-03-31 11:48:30,306 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,307 [INFO] Formato detectado para data/Macro/raw\\consumer_confidence\\Michigan_Consumer_Sentiment.csv: ISO\n",
      "2025-03-31 11:48:30,316 [INFO] Primeras fechas convertidas: [Timestamp('2015-01-01 00:00:00'), Timestamp('2015-02-01 00:00:00'), Timestamp('2015-03-01 00:00:00'), Timestamp('2015-04-01 00:00:00'), Timestamp('2015-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,320 [INFO] - Valores no nulos en TARGET: 121\n",
      "2025-03-31 11:48:30,321 [INFO] - Periodo: 2015-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,323 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,324 [INFO] \n",
      "Procesando: US_CPI (economics)\n",
      "2025-03-31 11:48:30,326 [INFO] - Archivo: US_CPI.csv\n",
      "2025-03-31 11:48:30,326 [INFO] - Columna TARGET: CPIAUCSL\n",
      "2025-03-31 11:48:30,326 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_CPI.csv\n",
      "2025-03-31 11:48:30,329 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,330 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,332 [INFO] Formato detectado para data/Macro/raw\\economics\\US_CPI.csv: ISO\n",
      "2025-03-31 11:48:30,344 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,349 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,349 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,350 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,355 [INFO] \n",
      "Procesando: US_Core_CPI (economics)\n",
      "2025-03-31 11:48:30,356 [INFO] - Archivo: US_Core_CPI.csv\n",
      "2025-03-31 11:48:30,356 [INFO] - Columna TARGET: CPILFESL\n",
      "2025-03-31 11:48:30,360 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Core_CPI.csv\n",
      "2025-03-31 11:48:30,365 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,368 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,370 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Core_CPI.csv: ISO\n",
      "2025-03-31 11:48:30,395 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,399 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,402 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,404 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,405 [INFO] \n",
      "Procesando: US_PCE (economics)\n",
      "2025-03-31 11:48:30,405 [INFO] - Archivo: US_PCE.csv\n",
      "2025-03-31 11:48:30,408 [INFO] - Columna TARGET: PCE\n",
      "2025-03-31 11:48:30,410 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_PCE.csv\n",
      "2025-03-31 11:48:30,413 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:30,415 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,415 [INFO] Formato detectado para data/Macro/raw\\economics\\US_PCE.csv: ISO\n",
      "2025-03-31 11:48:30,427 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,431 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:30,431 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,433 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,435 [INFO] \n",
      "Procesando: US_Core_PCE (economics)\n",
      "2025-03-31 11:48:30,436 [INFO] - Archivo: US_Core_PCE.csv\n",
      "2025-03-31 11:48:30,436 [INFO] - Columna TARGET: PCEPILFE\n",
      "2025-03-31 11:48:30,438 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Core_PCE.csv\n",
      "2025-03-31 11:48:30,441 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:30,442 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,442 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Core_PCE.csv: ISO\n",
      "2025-03-31 11:48:30,453 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,456 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:30,456 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,457 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,459 [INFO] \n",
      "Procesando: US_PPI (economics)\n",
      "2025-03-31 11:48:30,459 [INFO] - Archivo: US_PPI.csv\n",
      "2025-03-31 11:48:30,461 [INFO] - Columna TARGET: PPIACO\n",
      "2025-03-31 11:48:30,462 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_PPI.csv\n",
      "2025-03-31 11:48:30,464 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,465 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,465 [INFO] Formato detectado para data/Macro/raw\\economics\\US_PPI.csv: ISO\n",
      "2025-03-31 11:48:30,480 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,484 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,484 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,485 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,487 [INFO] \n",
      "Procesando: US_Industrial_Production_MoM (economics)\n",
      "2025-03-31 11:48:30,488 [INFO] - Archivo: US_Industrial_Production_MoM.csv\n",
      "2025-03-31 11:48:30,488 [INFO] - Columna TARGET: INDPRO\n",
      "2025-03-31 11:48:30,490 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Industrial_Production_MoM.csv\n",
      "2025-03-31 11:48:30,491 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,493 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,494 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Industrial_Production_MoM.csv: ISO\n",
      "2025-03-31 11:48:30,508 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,509 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,511 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,511 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,512 [INFO] \n",
      "Procesando: US_CaseShiller_HomePrice (economics)\n",
      "2025-03-31 11:48:30,514 [INFO] - Archivo: US_CaseShiller_HomePrice.csv\n",
      "2025-03-31 11:48:30,514 [INFO] - Columna TARGET: CSUSHPINSA\n",
      "2025-03-31 11:48:30,516 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_CaseShiller_HomePrice.csv\n",
      "2025-03-31 11:48:30,517 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:30,519 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,519 [INFO] Formato detectado para data/Macro/raw\\economics\\US_CaseShiller_HomePrice.csv: ISO\n",
      "2025-03-31 11:48:30,532 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,536 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:30,537 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:30,537 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,539 [INFO] \n",
      "Procesando: US_GDP_Growth (economics)\n",
      "2025-03-31 11:48:30,539 [INFO] - Archivo: US_GDP_Growth.csv\n",
      "2025-03-31 11:48:30,540 [INFO] - Columna TARGET: GDP\n",
      "2025-03-31 11:48:30,540 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_GDP_Growth.csv\n",
      "2025-03-31 11:48:30,543 [INFO] - Filas encontradas: 44\n",
      "2025-03-31 11:48:30,543 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,545 [INFO] Formato detectado para data/Macro/raw\\economics\\US_GDP_Growth.csv: ISO\n",
      "2025-03-31 11:48:30,554 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-07-01 00:00:00'), Timestamp('2014-10-01 00:00:00'), Timestamp('2015-01-01 00:00:00')]\n",
      "2025-03-31 11:48:30,557 [INFO] - Valores no nulos en TARGET: 44\n",
      "2025-03-31 11:48:30,558 [INFO] - Periodo: 2014-01-01 a 2024-10-01\n",
      "2025-03-31 11:48:30,560 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,562 [INFO] \n",
      "Procesando: US_Capacity_Utilization (economics)\n",
      "2025-03-31 11:48:30,563 [INFO] - Archivo: US_Capacity_Utilization.csv\n",
      "2025-03-31 11:48:30,563 [INFO] - Columna TARGET: TCU\n",
      "2025-03-31 11:48:30,565 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Capacity_Utilization.csv\n",
      "2025-03-31 11:48:30,569 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,571 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,571 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Capacity_Utilization.csv: ISO\n",
      "2025-03-31 11:48:30,583 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,586 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,586 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,588 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,589 [INFO] \n",
      "Procesando: US_Building_Permits (economics)\n",
      "2025-03-31 11:48:30,589 [INFO] - Archivo: US_Building_Permits.csv\n",
      "2025-03-31 11:48:30,589 [INFO] - Columna TARGET: PERMIT\n",
      "2025-03-31 11:48:30,591 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Building_Permits.csv\n",
      "2025-03-31 11:48:30,592 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,594 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,594 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Building_Permits.csv: ISO\n",
      "2025-03-31 11:48:30,609 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,613 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,613 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,615 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,616 [INFO] \n",
      "Procesando: US_Housing_Starts (economics)\n",
      "2025-03-31 11:48:30,616 [INFO] - Archivo: US_Housing_Starts.csv\n",
      "2025-03-31 11:48:30,618 [INFO] - Columna TARGET: HOUST\n",
      "2025-03-31 11:48:30,618 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_Housing_Starts.csv\n",
      "2025-03-31 11:48:30,621 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,621 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,621 [INFO] Formato detectado para data/Macro/raw\\economics\\US_Housing_Starts.csv: ISO\n",
      "2025-03-31 11:48:30,633 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,636 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,638 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,639 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,641 [INFO] \n",
      "Procesando: US_FedFunds_Rate (economics)\n",
      "2025-03-31 11:48:30,642 [INFO] - Archivo: US_FedFunds_Rate.csv\n",
      "2025-03-31 11:48:30,642 [INFO] - Columna TARGET: FEDFUNDS\n",
      "2025-03-31 11:48:30,644 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\US_FedFunds_Rate.csv\n",
      "2025-03-31 11:48:30,647 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:30,649 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,650 [INFO] Formato detectado para data/Macro/raw\\economics\\US_FedFunds_Rate.csv: ISO\n",
      "2025-03-31 11:48:30,664 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:30,665 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:30,667 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:30,667 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:30,668 [INFO] \n",
      "Procesando: ECB_Deposit_Rate (economics)\n",
      "2025-03-31 11:48:30,670 [INFO] - Archivo: ECB_Deposit_Rate.csv\n",
      "2025-03-31 11:48:30,670 [INFO] - Columna TARGET: ECBDFR\n",
      "2025-03-31 11:48:30,670 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\ECB_Deposit_Rate.csv\n",
      "2025-03-31 11:48:30,675 [INFO] - Filas encontradas: 4103\n",
      "2025-03-31 11:48:30,676 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:30,678 [INFO] Formato detectado para data/Macro/raw\\economics\\ECB_Deposit_Rate.csv: ISO\n",
      "2025-03-31 11:48:31,009 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-01-02 00:00:00'), Timestamp('2014-01-03 00:00:00'), Timestamp('2014-01-04 00:00:00'), Timestamp('2014-01-05 00:00:00')]\n",
      "2025-03-31 11:48:31,012 [INFO] - Valores no nulos en TARGET: 4103\n",
      "2025-03-31 11:48:31,013 [INFO] - Periodo: 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:31,015 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,026 [INFO] \n",
      "Procesando: Fed_Balance_Sheet (economics)\n",
      "2025-03-31 11:48:31,027 [INFO] - Archivo: Fed_Balance_Sheet.csv\n",
      "2025-03-31 11:48:31,029 [INFO] - Columna TARGET: WALCL\n",
      "2025-03-31 11:48:31,029 [INFO] - Ruta encontrada: data/Macro/raw\\economics\\Fed_Balance_Sheet.csv\n",
      "2025-03-31 11:48:31,035 [INFO] - Filas encontradas: 586\n",
      "2025-03-31 11:48:31,035 [INFO] Detección formato: 20/20 registros ISO (ratio 1.00)\n",
      "2025-03-31 11:48:31,037 [INFO] Formato detectado para data/Macro/raw\\economics\\Fed_Balance_Sheet.csv: ISO\n",
      "2025-03-31 11:48:31,138 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-01-08 00:00:00'), Timestamp('2014-01-15 00:00:00'), Timestamp('2014-01-22 00:00:00'), Timestamp('2014-01-29 00:00:00')]\n",
      "2025-03-31 11:48:31,141 [INFO] - Valores no nulos en TARGET: 586\n",
      "2025-03-31 11:48:31,143 [INFO] - Periodo: 2014-01-01 a 2025-03-19\n",
      "2025-03-31 11:48:31,146 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,150 [INFO] \n",
      "Procesando: Dollar_Index_DXY (index_pricing)\n",
      "2025-03-31 11:48:31,152 [INFO] - Archivo: Dollar_Index_DXY.xlsx\n",
      "2025-03-31 11:48:31,152 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:31,154 [INFO] - Ruta encontrada: data/Macro/raw\\index_pricing\\Dollar_Index_DXY.xlsx\n",
      "2025-03-31 11:48:31,250 [INFO] - Filas encontradas: 2927\n",
      "2025-03-31 11:48:31,252 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:31,255 [INFO] Formato detectado para data/Macro/raw\\index_pricing\\Dollar_Index_DXY.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:31,258 [INFO] Preferencia de dayfirst para data/Macro/raw\\index_pricing\\Dollar_Index_DXY.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:31,269 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-02 00:00:00'), Timestamp('2014-01-03 00:00:00'), Timestamp('2014-01-06 00:00:00'), Timestamp('2014-01-07 00:00:00'), Timestamp('2014-01-08 00:00:00')]\n",
      "2025-03-31 11:48:31,271 [WARNING] No se encontró 'PRICE', se usará 'Price'\n",
      "2025-03-31 11:48:31,275 [INFO] - Valores no nulos en TARGET: 2792\n",
      "2025-03-31 11:48:31,278 [INFO] - Periodo: 2014-01-02 a 2025-03-21\n",
      "2025-03-31 11:48:31,278 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,282 [INFO] \n",
      "Procesando: US_Unemployment_Rate (unemployment_rate)\n",
      "2025-03-31 11:48:31,282 [INFO] - Archivo: US_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:31,283 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:31,285 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\US_Unemployment_Rate.xlsx\n",
      "2025-03-31 11:48:31,295 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:31,297 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:31,297 [INFO] Formato detectado para data/Macro/raw\\unemployment_rate\\US_Unemployment_Rate.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:31,300 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\US_Unemployment_Rate.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:31,302 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:31,305 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:31,305 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,305 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,306 [INFO] \n",
      "Procesando: US_Nonfarm_Payrolls (unemployment_rate)\n",
      "2025-03-31 11:48:31,306 [INFO] - Archivo: US_Nonfarm_Payrolls.xlsx\n",
      "2025-03-31 11:48:31,308 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:31,308 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\US_Nonfarm_Payrolls.xlsx\n",
      "2025-03-31 11:48:31,320 [INFO] - Filas encontradas: 134\n",
      "2025-03-31 11:48:31,321 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:31,321 [INFO] Formato detectado para data/Macro/raw\\unemployment_rate\\US_Nonfarm_Payrolls.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:31,323 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\US_Nonfarm_Payrolls.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:31,326 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:31,329 [INFO] - Valores no nulos en TARGET: 134\n",
      "2025-03-31 11:48:31,331 [INFO] - Periodo: 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,331 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,334 [INFO] \n",
      "Procesando: US_Initial_Jobless_Claims (unemployment_rate)\n",
      "2025-03-31 11:48:31,334 [INFO] - Archivo: US_Initial_Jobless_Claims.xlsx\n",
      "2025-03-31 11:48:31,335 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:31,337 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\US_Initial_Jobless_Claims.xlsx\n",
      "2025-03-31 11:48:31,355 [INFO] - Filas encontradas: 586\n",
      "2025-03-31 11:48:31,355 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:31,357 [INFO] Formato detectado para data/Macro/raw\\unemployment_rate\\US_Initial_Jobless_Claims.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:31,358 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\US_Initial_Jobless_Claims.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:31,361 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-04 00:00:00'), Timestamp('2014-01-11 00:00:00'), Timestamp('2014-01-18 00:00:00'), Timestamp('2014-01-25 00:00:00'), Timestamp('2014-02-01 00:00:00')]\n",
      "2025-03-31 11:48:31,364 [INFO] - Valores no nulos en TARGET: 586\n",
      "2025-03-31 11:48:31,364 [INFO] - Periodo: 2014-01-04 a 2025-03-22\n",
      "2025-03-31 11:48:31,364 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,366 [INFO] \n",
      "Procesando: US_JOLTS (unemployment_rate)\n",
      "2025-03-31 11:48:31,367 [INFO] - Archivo: US_JOLTS.xlsx\n",
      "2025-03-31 11:48:31,369 [INFO] - Columna TARGET: PRICE\n",
      "2025-03-31 11:48:31,369 [INFO] - Ruta encontrada: data/Macro/raw\\unemployment_rate\\US_JOLTS.xlsx\n",
      "2025-03-31 11:48:31,378 [INFO] - Filas encontradas: 133\n",
      "2025-03-31 11:48:31,380 [INFO] Detección formato: 0/20 registros ISO (ratio 0.00)\n",
      "2025-03-31 11:48:31,380 [INFO] Formato detectado para data/Macro/raw\\unemployment_rate\\US_JOLTS.xlsx: AMBIGUOUS\n",
      "2025-03-31 11:48:31,383 [INFO] Preferencia de dayfirst para data/Macro/raw\\unemployment_rate\\US_JOLTS.xlsx: True (score True: 1.00, False: 1.00)\n",
      "2025-03-31 11:48:31,386 [INFO] Primeras fechas convertidas: [Timestamp('2014-01-01 00:00:00'), Timestamp('2014-02-01 00:00:00'), Timestamp('2014-03-01 00:00:00'), Timestamp('2014-04-01 00:00:00'), Timestamp('2014-05-01 00:00:00')]\n",
      "2025-03-31 11:48:31,389 [INFO] - Valores no nulos en TARGET: 133\n",
      "2025-03-31 11:48:31,389 [INFO] - Periodo: 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,390 [INFO] - Cobertura: 100.00%\n",
      "2025-03-31 11:48:31,392 [INFO] Índice diario generado: 4103 días desde 2014-01-01 hasta 2025-03-26\n",
      "2025-03-31 11:48:31,527 [INFO] DataFrame final combinado: 4103 filas, 36 columnas\n",
      "2025-03-31 11:48:31,530 [INFO] \n",
      "Resumen de Cobertura:\n",
      "2025-03-31 11:48:31,530 [INFO] - US_10Y_Treasury: 100.00% desde 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:31,531 [INFO] - US_2Y_Treasury: 100.00% desde 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:31,533 [INFO] - Corporate_Bond_AAA_Spread: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,533 [INFO] - Corporate_Bond_BBB_Spread: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,535 [INFO] - High_Yield_Bond_Spread: 100.00% desde 2014-01-02 a 2025-03-25\n",
      "2025-03-31 11:48:31,538 [INFO] - Denmark_Car_Registrations_MoM: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,539 [INFO] - US_Car_Registrations_MoM: 100.00% desde 2014-01-01 a 2024-12-01\n",
      "2025-03-31 11:48:31,539 [INFO] - SouthAfrica_Car_Registrations_MoM: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,542 [INFO] - United_Kingdom_Car_Registrations_MoM: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,544 [INFO] - Spain_Car_Registrations_MoM: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,545 [INFO] - US_Commercial_Loans: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,547 [INFO] - US_RealEstate_Commercial_Loans: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,547 [INFO] - US_Consumer_Credit: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,548 [INFO] - EuroZone_Consumer_Confidence: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,550 [INFO] - Switzerland_Consumer_Confidence: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,552 [INFO] - Michigan_Consumer_Sentiment: 100.00% desde 2015-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,553 [INFO] - US_CPI: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,553 [INFO] - US_Core_CPI: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,555 [INFO] - US_PCE: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,556 [INFO] - US_Core_PCE: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,556 [INFO] - US_PPI: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,558 [INFO] - US_Industrial_Production_MoM: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,558 [INFO] - US_CaseShiller_HomePrice: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:31,559 [INFO] - US_GDP_Growth: 100.00% desde 2014-01-01 a 2024-10-01\n",
      "2025-03-31 11:48:31,561 [INFO] - US_Capacity_Utilization: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,562 [INFO] - US_Building_Permits: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,562 [INFO] - US_Housing_Starts: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,564 [INFO] - US_FedFunds_Rate: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,564 [INFO] - ECB_Deposit_Rate: 100.00% desde 2014-01-01 a 2025-03-26\n",
      "2025-03-31 11:48:31,565 [INFO] - Fed_Balance_Sheet: 100.00% desde 2014-01-01 a 2025-03-19\n",
      "2025-03-31 11:48:31,567 [INFO] - Dollar_Index_DXY: 100.00% desde 2014-01-02 a 2025-03-21\n",
      "2025-03-31 11:48:31,568 [INFO] - US_Unemployment_Rate: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,570 [INFO] - US_Nonfarm_Payrolls: 100.00% desde 2014-01-01 a 2025-02-01\n",
      "2025-03-31 11:48:31,571 [INFO] - US_Initial_Jobless_Claims: 100.00% desde 2014-01-04 a 2025-03-22\n",
      "2025-03-31 11:48:31,571 [INFO] - US_JOLTS: 100.00% desde 2014-01-01 a 2025-01-01\n",
      "2025-03-31 11:48:33,977 [INFO] Archivo guardado exitosamente: datos_economicos_procesados_Fred.xlsx\n",
      "2025-03-31 11:48:33,978 [INFO] \n",
      "Resumen de Ejecución:\n",
      "2025-03-31 11:48:33,978 [INFO] Tiempo de ejecución: 4.73 segundos\n",
      "2025-03-31 11:48:33,980 [INFO] Archivos procesados: 35\n",
      "2025-03-31 11:48:33,980 [INFO] Archivo de salida: datos_economicos_procesados_Fred.xlsx\n",
      "2025-03-31 11:48:33,981 [INFO] Estado: COMPLETADO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de logging\n",
    "def configurar_logging(log_file='freddataprocessor.log'):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('FredDataProcessor')\n",
    "\n",
    "class FredDataProcessor:\n",
    "    \"\"\"\n",
    "    Clase para procesar datos de FRED (Fuente \"FRED\" y Preprocesamiento \"Normal\").\n",
    "\n",
    "    Se espera que cada archivo tenga:\n",
    "      - Una columna de fecha llamada \"observation_date\" (o \"DATE\" si no existe).\n",
    "      - Una columna de datos cuyo nombre se especifica en el archivo de configuración (TARGET).\n",
    "\n",
    "    Esta versión:\n",
    "      - Busca el archivo usando varias extensiones: .csv, .xlsx, .xls.\n",
    "      - Detecta dinámicamente el formato de fecha analizando hasta 20 registros.\n",
    "         Si la mayoría siguen el formato ISO (YYYY-MM-DD), se fuerza ese formato;\n",
    "         de lo contrario se evalúan ambas opciones (dayfirst True/False) usando la monotonicidad.\n",
    "      - La función improved_robust_parse_date ahora maneja objetos Timestamp y valores no-string.\n",
    "      - Convierte la columna de fecha y la columna target a numérico.\n",
    "      - Renombra la columna de datos con el patrón: {TARGET}_{variable}_{Tipo_Macro}.\n",
    "      - Genera un índice diario global y usa merge_asof para imputar los datos (forward fill).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_file, data_root='data/Macro/raw', log_file='freddataprocessor.log'):\n",
    "        self.config_file = config_file\n",
    "        self.data_root = data_root\n",
    "        self.logger = configurar_logging(log_file)\n",
    "        self.config_data = None\n",
    "        self.global_min_date = None\n",
    "        self.global_max_date = None\n",
    "        self.daily_index = None\n",
    "        self.processed_data = {}   # {variable: DataFrame procesado}\n",
    "        self.final_df = None\n",
    "        self.stats = {}\n",
    "        self.date_cache = {}  # Guarda la preferencia de formato para cada archivo\n",
    "\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"INICIANDO PROCESO: FredDataProcessor\")\n",
    "        self.logger.info(f\"Archivo de configuración: {config_file}\")\n",
    "        self.logger.info(f\"Directorio raíz de datos: {data_root}\")\n",
    "        self.logger.info(f\"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "\n",
    "    def read_config(self):\n",
    "        try:\n",
    "            self.logger.info(\"Leyendo archivo de configuración...\")\n",
    "            df_config = pd.read_excel(self.config_file)\n",
    "            self.config_data = df_config[\n",
    "                (df_config['Fuente'] == 'FRED') &\n",
    "                (df_config['Tipo de Preprocesamiento Según la Fuente'] == 'Normal')\n",
    "            ].copy()\n",
    "            self.logger.info(f\"Se encontraron {len(self.config_data)} configuraciones para procesar\")\n",
    "            return self.config_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer configuración: {e}\")\n",
    "            return None\n",
    "\n",
    "    def detect_date_format(self, series, n=20, iso_threshold=0.6):\n",
    "        \"\"\"\n",
    "        Analiza hasta n registros de la serie de fechas para determinar si la mayoría \n",
    "        siguen el formato ISO (YYYY-MM-DD).\n",
    "\n",
    "        Returns:\n",
    "            \"ISO\" si al menos iso_threshold de los registros coinciden con el patrón ISO,\n",
    "            de lo contrario \"AMBIGUOUS\".\n",
    "        \"\"\"\n",
    "        sample = series.dropna().head(n)\n",
    "        if len(sample) == 0:\n",
    "            return \"AMBIGUOUS\"\n",
    "        iso_count = 0\n",
    "        for val in sample:\n",
    "            if isinstance(val, str) and re.match(r'^\\d{4}-\\d{2}-\\d{2}$', val.strip()):\n",
    "                iso_count += 1\n",
    "        ratio = iso_count / len(sample)\n",
    "        self.logger.info(f\"Detección formato: {iso_count}/{len(sample)} registros ISO (ratio {ratio:.2f})\")\n",
    "        return \"ISO\" if ratio >= iso_threshold else \"AMBIGUOUS\"\n",
    "\n",
    "    def monotonic_score(self, parsed_series):\n",
    "        \"\"\"\n",
    "        Calcula la puntuación de monotonicidad de una serie de fechas.\n",
    "        Es la proporción de diferencias no negativas respecto al total.\n",
    "        \"\"\"\n",
    "        parsed = parsed_series.dropna()\n",
    "        if len(parsed) < 2:\n",
    "            return 0\n",
    "        diffs = parsed.diff().dropna()\n",
    "        score = (diffs >= timedelta(0)).sum() / len(diffs)\n",
    "        return score\n",
    "\n",
    "    def improved_robust_parse_date(self, date_str, preferred_dayfirst=None, use_iso=False):\n",
    "        \"\"\"\n",
    "        Convierte una cadena o timestamp de fecha.\n",
    "\n",
    "        Args:\n",
    "            date_str: Cadena de fecha o timestamp.\n",
    "            preferred_dayfirst (bool, opcional): Preferencia para la conversión.\n",
    "            use_iso (bool): Si se debe forzar el formato ISO (YYYY-MM-DD).\n",
    "\n",
    "        Returns:\n",
    "            pd.Timestamp o None.\n",
    "        \"\"\"\n",
    "        # Si ya es un timestamp, devolverlo directamente\n",
    "        if isinstance(date_str, pd.Timestamp):\n",
    "            return date_str\n",
    "\n",
    "        if not isinstance(date_str, str):\n",
    "            self.logger.debug(f\"Valor no string en fecha: {date_str} (tipo: {type(date_str)})\")\n",
    "            return None\n",
    "\n",
    "        date_str = date_str.strip()\n",
    "        if not date_str:\n",
    "            return None\n",
    "\n",
    "        if use_iso:\n",
    "            try:\n",
    "                return pd.to_datetime(date_str, format='%Y-%m-%d', errors='coerce')\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error al convertir formato ISO en '{date_str}': {e}\")\n",
    "                return None\n",
    "\n",
    "        # Intentar patrón \"Apr 01, 2025 (Mar)\"\n",
    "        m = re.search(r'([A-Za-z]+\\s+\\d{1,2},\\s+\\d{4})', date_str)\n",
    "        if m:\n",
    "            candidate = m.group(1)\n",
    "            try:\n",
    "                parsed = pd.to_datetime(candidate, errors='coerce')\n",
    "                if pd.notnull(parsed):\n",
    "                    return parsed\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error al parsear patrón en '{date_str}': {e}\")\n",
    "\n",
    "        if preferred_dayfirst is not None:\n",
    "            try:\n",
    "                return pd.to_datetime(date_str, dayfirst=preferred_dayfirst, errors='coerce')\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error con dayfirst={preferred_dayfirst} en '{date_str}': {e}\")\n",
    "\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, dayfirst=True, errors='coerce')\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error en improved_robust_parse_date para '{date_str}': {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_file(self, config_row):\n",
    "        \"\"\"\n",
    "        Procesa un archivo individual de FRED.\n",
    "        \n",
    "        - Busca el archivo usando extensiones: .csv, .xlsx, .xls.\n",
    "        - Usa la columna de fecha \"observation_date\" (o \"DATE\").\n",
    "        - Analiza hasta 20 registros para determinar el formato de fecha.\n",
    "        - Convierte la columna de fecha y la columna target a numérico.\n",
    "        - Renombra la columna de datos con el patrón: {TARGET}_{variable}_{Tipo_Macro}.\n",
    "        - Devuelve un DataFrame con columnas ['fecha', nuevo_nombre].\n",
    "        \"\"\"\n",
    "        variable = config_row['Variable']\n",
    "        macro_type = config_row['Tipo Macro']\n",
    "        target_col = config_row['TARGET']\n",
    "\n",
    "        # Lista de extensiones a buscar\n",
    "        extensions = ['.csv', '.xlsx', '.xls']\n",
    "        ruta = None\n",
    "        for ext in extensions:\n",
    "            ruta_candidate = os.path.join(self.data_root, macro_type, f\"{variable}{ext}\")\n",
    "            if os.path.exists(ruta_candidate):\n",
    "                ruta = ruta_candidate\n",
    "                break\n",
    "        if ruta is None:\n",
    "            for ext in extensions:\n",
    "                for root, dirs, files in os.walk(self.data_root):\n",
    "                    if f\"{variable}{ext}\" in files:\n",
    "                        ruta = os.path.join(root, f\"{variable}{ext}\")\n",
    "                        break\n",
    "                if ruta is not None:\n",
    "                    break\n",
    "        if ruta is None:\n",
    "            self.logger.error(f\"Archivo no encontrado: {variable}* (se probaron extensiones: {', '.join(extensions)})\")\n",
    "            return variable, None\n",
    "\n",
    "        self.logger.info(f\"\\nProcesando: {variable} ({macro_type})\")\n",
    "        self.logger.info(f\"- Archivo: {os.path.basename(ruta)}\")\n",
    "        self.logger.info(f\"- Columna TARGET: {target_col}\")\n",
    "        self.logger.info(f\"- Ruta encontrada: {ruta}\")\n",
    "\n",
    "        try:\n",
    "            if ruta.endswith('.csv'):\n",
    "                df = pd.read_csv(ruta)\n",
    "            elif ruta.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(ruta)\n",
    "            else:\n",
    "                self.logger.error(f\"Extensión no soportada para {ruta}\")\n",
    "                return variable, None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer {ruta}: {e}\")\n",
    "            return variable, None\n",
    "\n",
    "        self.logger.info(f\"- Filas encontradas: {len(df)}\")\n",
    "        # Determinar la columna de fecha: preferir \"observation_date\", sino \"DATE\"\n",
    "        if 'observation_date' in df.columns:\n",
    "            date_col = 'observation_date'\n",
    "        elif 'DATE' in df.columns:\n",
    "            date_col = 'DATE'\n",
    "        else:\n",
    "            self.logger.error(f\"No se encontró columna de fecha ('observation_date' o 'DATE') en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        # Detectar el formato de fecha a partir de 20 registros\n",
    "        fmt = self.detect_date_format(df[date_col], n=20, iso_threshold=0.6)\n",
    "        use_iso = (fmt == \"ISO\")\n",
    "        self.logger.info(f\"Formato detectado para {ruta}: {fmt}\")\n",
    "\n",
    "        # Si el formato no es ISO, determinar la preferencia de dayfirst usando la monotonicidad\n",
    "        if not use_iso:\n",
    "            sample = df[date_col].dropna().head(20)\n",
    "            parsed_true = pd.to_datetime(sample, dayfirst=True, errors='coerce')\n",
    "            parsed_false = pd.to_datetime(sample, dayfirst=False, errors='coerce')\n",
    "            score_true = self.monotonic_score(parsed_true)\n",
    "            score_false = self.monotonic_score(parsed_false)\n",
    "            preferred = score_true >= score_false\n",
    "            self.date_cache[ruta] = preferred\n",
    "            self.logger.info(f\"Preferencia de dayfirst para {ruta}: {preferred} (score True: {score_true:.2f}, False: {score_false:.2f})\")\n",
    "        else:\n",
    "            preferred = None\n",
    "\n",
    "        # Convertir la columna de fecha usando improved_robust_parse_date\n",
    "        df['fecha'] = df[date_col].apply(lambda x: self.improved_robust_parse_date(x, preferred_dayfirst=preferred, use_iso=use_iso))\n",
    "        df = df.dropna(subset=['fecha'])\n",
    "        df = df.sort_values('fecha')\n",
    "        self.logger.info(f\"Primeras fechas convertidas: {df['fecha'].head(5).tolist()}\")\n",
    "\n",
    "        # Verificar la columna target usando búsqueda insensible a mayúsculas\n",
    "        if target_col not in df.columns:\n",
    "            for col in df.columns:\n",
    "                if col.strip().lower() == target_col.strip().lower():\n",
    "                    target_col = col\n",
    "                    self.logger.warning(f\"No se encontró '{config_row['TARGET']}', se usará '{target_col}'\")\n",
    "                    break\n",
    "        if target_col not in df.columns:\n",
    "            self.logger.error(f\"No se encontró columna TARGET ni alternativa en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        df['valor'] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "        df = df.dropna(subset=['valor'])\n",
    "        if df.empty:\n",
    "            self.logger.error(f\"No se encontraron valores válidos para '{target_col}' en {ruta}\")\n",
    "            return variable, None\n",
    "\n",
    "        current_min = df['fecha'].min()\n",
    "        current_max = df['fecha'].max()\n",
    "        if self.global_min_date is None or current_min < self.global_min_date:\n",
    "            self.global_min_date = current_min\n",
    "        if self.global_max_date is None or current_max > self.global_max_date:\n",
    "            self.global_max_date = current_max\n",
    "\n",
    "        # Renombrar la columna de datos usando el patrón\n",
    "        nuevo_nombre = f\"{target_col}_{variable}_{macro_type}\"\n",
    "        df.rename(columns={'valor': nuevo_nombre}, inplace=True)\n",
    "        self.stats[variable] = {\n",
    "            'macro_type': macro_type,\n",
    "            'target_column': target_col,\n",
    "            'total_rows': len(df),\n",
    "            'valid_values': len(df),\n",
    "            'coverage': 100.0,\n",
    "            'date_min': current_min,\n",
    "            'date_max': current_max,\n",
    "            'nuevo_nombre': nuevo_nombre\n",
    "        }\n",
    "        self.logger.info(f\"- Valores no nulos en TARGET: {len(df)}\")\n",
    "        self.logger.info(f\"- Periodo: {current_min.strftime('%Y-%m-%d')} a {current_max.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"- Cobertura: 100.00%\")\n",
    "        return variable, df[['fecha', nuevo_nombre]].copy()\n",
    "\n",
    "    def generate_daily_index(self):\n",
    "        \"\"\"\n",
    "        Genera un índice diario desde la fecha global mínima hasta la máxima.\n",
    "        \"\"\"\n",
    "        if self.global_min_date is None or self.global_max_date is None:\n",
    "            self.logger.error(\"No se pudieron determinar las fechas globales\")\n",
    "            return None\n",
    "        self.daily_index = pd.DataFrame({\n",
    "            'fecha': pd.date_range(start=self.global_min_date, end=self.global_max_date, freq='D')\n",
    "        })\n",
    "        self.logger.info(f\"Índice diario generado: {len(self.daily_index)} días desde {self.global_min_date.strftime('%Y-%m-%d')} hasta {self.global_max_date.strftime('%Y-%m-%d')}\")\n",
    "        return self.daily_index\n",
    "\n",
    "    def combine_data(self):\n",
    "        \"\"\"\n",
    "        Convierte cada serie (diaria o de menor frecuencia) a datos diarios usando merge_asof.\n",
    "        Cada día se asocia al valor reportado más reciente (forward fill).\n",
    "        \"\"\"\n",
    "        if self.daily_index is None:\n",
    "            self.logger.error(\"El índice diario no ha sido generado\")\n",
    "            return None\n",
    "\n",
    "        combined = self.daily_index.copy()\n",
    "        for variable, df in self.processed_data.items():\n",
    "            if df is None or df.empty:\n",
    "                self.logger.warning(f\"Omitiendo {variable} por falta de datos\")\n",
    "                continue\n",
    "            df = df.sort_values('fecha')\n",
    "            df_daily = pd.merge_asof(combined, df, on='fecha', direction='backward')\n",
    "            col_name = self.stats[variable]['nuevo_nombre']\n",
    "            df_daily[col_name] = df_daily[col_name].ffill()\n",
    "            combined = combined.merge(df_daily[['fecha', col_name]], on='fecha', how='left')\n",
    "        self.final_df = combined\n",
    "        self.logger.info(f\"DataFrame final combinado: {len(self.final_df)} filas, {len(self.final_df.columns)} columnas\")\n",
    "        return self.final_df\n",
    "\n",
    "    def analyze_coverage(self):\n",
    "        \"\"\"\n",
    "        Genera un resumen de cobertura y estadísticas para cada variable.\n",
    "        \"\"\"\n",
    "        total_days = len(self.daily_index)\n",
    "        self.logger.info(\"\\nResumen de Cobertura:\")\n",
    "        for variable, stats in self.stats.items():\n",
    "            self.logger.info(f\"- {variable}: {stats['coverage']:.2f}% desde {stats['date_min'].strftime('%Y-%m-%d')} a {stats['date_max'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    def save_results(self, output_file='datos_economicos_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Guarda el DataFrame final combinado y las estadísticas en un archivo Excel.\n",
    "        \"\"\"\n",
    "        if self.final_df is None:\n",
    "            self.logger.error(\"No hay datos combinados para guardar\")\n",
    "            return False\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                self.final_df.to_excel(writer, sheet_name='Datos Diarios', index=False)\n",
    "                df_stats = pd.DataFrame(self.stats).T\n",
    "                df_stats.to_excel(writer, sheet_name='Estadisticas')\n",
    "                meta = {\n",
    "                    'Proceso': ['FredDataProcessor'],\n",
    "                    'Fecha de proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                    'Total indicadores': [len(self.stats)],\n",
    "                    'Periodo': [f\"{self.global_min_date.strftime('%Y-%m-%d')} a {self.global_max_date.strftime('%Y-%m-%d')}\"],\n",
    "                    'Total días': [len(self.daily_index)]\n",
    "                }\n",
    "                pd.DataFrame(meta).to_excel(writer, sheet_name='Metadatos', index=False)\n",
    "            self.logger.info(f\"Archivo guardado exitosamente: {output_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al guardar resultados: {e}\")\n",
    "            return False\n",
    "\n",
    "    def run(self, output_file='datos_economicos_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Ejecuta el proceso completo:\n",
    "          1. Lee la configuración.\n",
    "          2. Procesa cada archivo de FRED (buscando las extensiones adecuadas).\n",
    "          3. Genera el índice diario global.\n",
    "          4. Convierte cada serie a datos diarios y las combina.\n",
    "          5. Analiza la cobertura.\n",
    "          6. Guarda los resultados.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        if self.read_config() is None:\n",
    "            return False\n",
    "\n",
    "        for _, config_row in self.config_data.iterrows():\n",
    "            var, df_processed = self.process_file(config_row)\n",
    "            self.processed_data[var] = df_processed\n",
    "\n",
    "        if len([df for df in self.processed_data.values() if df is not None]) == 0:\n",
    "            self.logger.error(\"No se procesó ningún archivo correctamente\")\n",
    "            return False\n",
    "\n",
    "        self.generate_daily_index()\n",
    "        self.combine_data()\n",
    "        self.analyze_coverage()\n",
    "        result = self.save_results(output_file)\n",
    "        end_time = time.time()\n",
    "        self.logger.info(\"\\nResumen de Ejecución:\")\n",
    "        self.logger.info(f\"Tiempo de ejecución: {end_time - start_time:.2f} segundos\")\n",
    "        self.logger.info(f\"Archivos procesados: {len(self.config_data)}\")\n",
    "        self.logger.info(f\"Archivo de salida: {output_file}\")\n",
    "        self.logger.info(f\"Estado: {'COMPLETADO' if result else 'ERROR'}\")\n",
    "        return result\n",
    "\n",
    "# Función principal para ejecutar el proceso\n",
    "def run_fred_data_processor(config_file='Data Engineering.xlsx',\n",
    "                            output_file='datos_economicos_procesados_Fred.xlsx',\n",
    "                            data_root='data/Macro/raw',\n",
    "                            log_file='freddataprocessor.log'):\n",
    "    processor = FredDataProcessor(config_file, data_root, log_file)\n",
    "    return processor.run(output_file)\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    success = run_fred_data_processor()\n",
    "    print(f\"Proceso {'completado exitosamente' if success else 'finalizado con errores'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 11:48:34,132 [INFO] ================================================================================\n",
      "2025-03-31 11:48:34,134 [INFO] INICIANDO PROCESO: OtherDataProcessor\n",
      "2025-03-31 11:48:34,134 [INFO] Archivo de configuración: Data Engineering.xlsx\n",
      "2025-03-31 11:48:34,135 [INFO] Directorio raíz de datos: data/Macro/raw\n",
      "2025-03-31 11:48:34,137 [INFO] Fecha y hora: 2025-03-31 11:48:34\n",
      "2025-03-31 11:48:34,137 [INFO] ================================================================================\n",
      "2025-03-31 11:48:34,138 [INFO] Iniciando proceso completo OtherDataProcessor...\n",
      "2025-03-31 11:48:34,140 [INFO] Leyendo archivo de configuración...\n",
      "2025-03-31 11:48:34,186 [INFO] Se encontraron 4 configuraciones para procesar con fuente 'Other'\n",
      "2025-03-31 11:48:34,188 [INFO] \n",
      "Resumen de configuraciones a procesar:\n",
      "2025-03-31 11:48:34,189 [INFO] - US_Empire_State_Index (Tipo Macro: business_confidence, TARGET: nan)\n",
      "2025-03-31 11:48:34,191 [INFO] - AAII_Investor_Sentiment (Tipo Macro: consumer_confidence, TARGET: nan)\n",
      "2025-03-31 11:48:34,192 [INFO] - Put_Call_Ratio_SPY (Tipo Macro: consumer_confidence, TARGET: nan)\n",
      "2025-03-31 11:48:34,194 [INFO] - Chicago_Fed_NFCI (Tipo Macro: leading_economic_index, TARGET: ANFCI)\n",
      "2025-03-31 11:48:34,196 [INFO] \n",
      "Procesando: US_Empire_State_Index (business_confidence)\n",
      "2025-03-31 11:48:34,197 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:48:34,197 [INFO] - Tipo de preprocesamiento: Normal\n",
      "2025-03-31 11:48:34,199 [INFO] Procesando US_Empire_State_Index manualmente\n",
      "2025-03-31 11:48:34,199 [INFO] - Archivo de entrada: data/Macro/raw\\business_confidence\\US_Empire_State_Index.csv\n",
      "2025-03-31 11:48:34,202 [INFO] - Archivo cargado: 285 filas, 24 columnas\n",
      "2025-03-31 11:48:34,206 [INFO] - Rango de fechas: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:48:34,207 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:48:34,209 [INFO] - Columnas encontradas y procesadas: GACDISA, AWCDISA\n",
      "2025-03-31 11:48:34,212 [INFO] - Columna ESI_GACDISA_US_Empire_State_Index_business_confidence: 285 valores no nulos (100.00%), rango: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:48:34,214 [INFO] - Columna ESI_AWCDISA_US_Empire_State_Index_business_confidence: 285 valores no nulos (100.00%), rango: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:48:34,215 [INFO] - US_Empire_State_Index: 285 filas procesadas manualmente, periodo: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:48:34,217 [INFO] \n",
      "Procesando: AAII_Investor_Sentiment (consumer_confidence)\n",
      "2025-03-31 11:48:34,217 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:48:34,218 [INFO] - Tipo de preprocesamiento: nan\n",
      "2025-03-31 11:48:34,220 [INFO] Procesando AAII_Investor_Sentiment manualmente\n",
      "2025-03-31 11:48:34,220 [INFO] - Archivo de entrada: data/Macro/raw\\consumer_confidence\\AAII_Investor_Sentiment.xlsx\n",
      "2025-03-31 11:48:34,221 [INFO] Intentando leer Excel con engine: openpyxl\n",
      "2025-03-31 11:48:34,430 [INFO] - Archivo cargado: 1966 filas, 13 columnas\n",
      "2025-03-31 11:48:34,430 [INFO] - Columnas disponibles: Reported Date, Bullish, Neutral, Bearish, Total, Bullish 8-week Mov Avg, Bull-Bear Spread, Bullish Average, Bullish Average +St. Dev.,  Bullish Average - St. Dev., S&P 500 Weekly High, S&P 500 Weekly Low, S&P 500 Weekly Close\n",
      "2025-03-31 11:48:34,432 [INFO] - Usando columna 'Reported Date' como fecha\n",
      "2025-03-31 11:48:34,432 [INFO] - Columnas encontradas: Bearish, Bull-Bear Spread, Bullish\n",
      "2025-03-31 11:48:34,436 [INFO] - Rango de fechas total: 1987-06-26 a 2025-03-20\n",
      "2025-03-31 11:48:34,438 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:48:34,439 [INFO] - Columna AAII_Bearish_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:48:34,441 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:48:34,443 [INFO] - Columna AAII_Bull-Bear Spread_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:48:34,443 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:48:34,444 [INFO] - Columna AAII_Bullish_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:48:34,446 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:48:34,447 [INFO] - AAII_Investor_Sentiment: 1966 filas procesadas manualmente, periodo: 1987-06-26 a 2025-03-20\n",
      "2025-03-31 11:48:34,447 [INFO] \n",
      "Procesando: Put_Call_Ratio_SPY (consumer_confidence)\n",
      "2025-03-31 11:48:34,449 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:48:34,449 [INFO] - Tipo de preprocesamiento: nan\n",
      "2025-03-31 11:48:34,449 [INFO] Procesando Put_Call_Ratio_SPY manualmente\n",
      "2025-03-31 11:48:34,450 [INFO] - Archivo de entrada: data/Macro/raw\\consumer_confidence\\Put_Call_Ratio_SPY.csv\n",
      "2025-03-31 11:48:34,613 [INFO] - Archivo cargado: 114936 filas, 13 columnas\n",
      "2025-03-31 11:48:34,615 [INFO] - Columnas disponibles: date, act_symbol, expiration, strike, call_put, bid, ask, vol, delta, gamma, theta, vega, rho\n",
      "2025-03-31 11:48:34,617 [INFO] - Usando columna 'date' como fecha\n",
      "2025-03-31 11:48:34,631 [INFO] - Rango de fechas total: 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,633 [INFO] - ¿Tiene datos desde 2014 o antes?: No\n",
      "2025-03-31 11:48:34,634 [WARNING] - ATENCIÓN: Los datos comienzan en 2019-02-09, después de 2014-01-01\n",
      "2025-03-31 11:48:34,634 [WARNING] - No se encontraron columnas call_volume y put_volume. Buscando otras columnas numéricas.\n",
      "2025-03-31 11:48:34,637 [INFO] - Columna PutCall_strike_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,637 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,642 [INFO] - Columna PutCall_bid_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,644 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,650 [INFO] - Columna PutCall_ask_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,650 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,657 [INFO] - Columna PutCall_vol_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,659 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,668 [INFO] - Columna PutCall_delta_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,670 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,682 [INFO] - Columna PutCall_gamma_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,684 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,696 [INFO] - Columna PutCall_theta_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,697 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,711 [INFO] - Columna PutCall_vega_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,713 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,727 [INFO] - Columna PutCall_rho_Put_Call_Ratio_SPY_consumer_confidence: Rango de fechas 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,728 [INFO] - Valores disponibles: 114936/114936 (100.00%)\n",
      "2025-03-31 11:48:34,728 [INFO] - Se utilizaron columnas numéricas alternativas: strike, bid, ask, vol, delta, gamma, theta, vega, rho\n",
      "2025-03-31 11:48:34,742 [INFO] - Put_Call_Ratio_SPY: 114936 filas procesadas manualmente, periodo: 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:48:34,748 [INFO] \n",
      "Procesando: Chicago_Fed_NFCI (leading_economic_index)\n",
      "2025-03-31 11:48:34,750 [INFO] - Columna TARGET: ANFCI\n",
      "2025-03-31 11:48:34,752 [INFO] - Tipo de preprocesamiento: Normal\n",
      "2025-03-31 11:48:34,753 [INFO] Procesando Chicago_Fed_NFCI manualmente\n",
      "2025-03-31 11:48:34,754 [INFO] - Archivo de entrada: data/Macro/raw\\leading_economic_index\\Chicago_Fed_NFCI.csv\n",
      "2025-03-31 11:48:34,764 [INFO] - Archivo cargado: 2829 filas, 7 columnas\n",
      "2025-03-31 11:48:34,766 [INFO] - Columnas disponibles: Friday_of_Week, NFCI, ANFCI, Risk, Credit, Leverage, Nonfinancial_Leverage\n",
      "2025-03-31 11:48:34,776 [INFO] - Usando columna 'Friday_of_Week' como fecha\n",
      "2025-03-31 11:48:34,778 [INFO] - Rango de fechas total: 1971-01-08 a 2025-03-21\n",
      "2025-03-31 11:48:34,780 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:48:34,782 [INFO] - Filtrando desde 2014-01-01: 586/2829 filas (20.71%)\n",
      "2025-03-31 11:48:34,785 [INFO] - Columna NFCI: Rango de fechas 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:48:34,787 [INFO] - Columna ANFCI: Rango de fechas 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:48:34,788 [INFO] - Columnas procesadas: NFCI, ANFCI\n",
      "2025-03-31 11:48:34,791 [INFO] - Chicago_Fed_NFCI: 586 filas procesadas manualmente, periodo: 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:48:34,791 [INFO] \n",
      "Archivos procesados correctamente: 4/4\n",
      "2025-03-31 11:48:34,794 [INFO] \n",
      "Generando índice temporal diario...\n",
      "2025-03-31 11:48:34,795 [INFO] - Rango de fechas global: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:48:34,796 [INFO] - Total de fechas diarias generadas: 13794\n",
      "2025-03-31 11:48:34,797 [INFO] \n",
      "Combinando datos con índice diario...\n",
      "2025-03-31 11:48:34,797 [INFO] - Combinando: US_Empire_State_Index\n",
      "2025-03-31 11:48:34,802 [INFO] - Combinando: AAII_Investor_Sentiment\n",
      "2025-03-31 11:48:34,806 [INFO] - Combinando: Put_Call_Ratio_SPY\n",
      "2025-03-31 11:48:34,854 [INFO] - Combinando: Chicago_Fed_NFCI\n",
      "2025-03-31 11:48:34,885 [INFO] - DataFrame combinado: 127835 filas, 17 columnas\n",
      "2025-03-31 11:48:34,885 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:34,887 [INFO] RESUMEN DE COBERTURA FINAL\n",
      "2025-03-31 11:48:34,887 [INFO] ==================================================\n",
      "2025-03-31 11:48:34,887 [INFO] Total indicadores procesados: 4\n",
      "2025-03-31 11:48:34,888 [INFO] Rango de fechas: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:48:34,890 [INFO] Total días en la serie: 13794\n",
      "2025-03-31 11:48:34,891 [INFO] \n",
      "Cobertura por indicador:\n",
      "2025-03-31 11:48:34,893 [INFO] - US_Empire_State_Index: 889.42%\n",
      "2025-03-31 11:48:34,895 [INFO] - AAII_Investor_Sentiment: 926.54%\n",
      "2025-03-31 11:48:34,899 [INFO] - Put_Call_Ratio_SPY: 843.00%\n",
      "2025-03-31 11:48:34,901 [INFO] - Chicago_Fed_NFCI: 856.51%\n",
      "2025-03-31 11:48:34,902 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:34,902 [INFO] ESTADÍSTICAS DE VALORES\n",
      "2025-03-31 11:48:34,904 [INFO] ==================================================\n",
      "2025-03-31 11:48:34,910 [INFO] \n",
      "Estadísticas para ESI_GACDISA_US_Empire_State_Index_business_confidence:\n",
      "2025-03-31 11:48:34,912 [INFO] - Min: -79.9\n",
      "2025-03-31 11:48:34,913 [INFO] - Max: 39.0\n",
      "2025-03-31 11:48:34,913 [INFO] - Media: 0.2369773242260741\n",
      "2025-03-31 11:48:34,915 [INFO] - Mediana: 0.5\n",
      "2025-03-31 11:48:34,915 [INFO] - Desv. Estándar: 19.098291845758684\n",
      "2025-03-31 11:48:34,919 [INFO] \n",
      "Estadísticas para ESI_AWCDISA_US_Empire_State_Index_business_confidence:\n",
      "2025-03-31 11:48:34,921 [INFO] - Min: -63.2\n",
      "2025-03-31 11:48:34,921 [INFO] - Max: 33.5\n",
      "2025-03-31 11:48:34,922 [INFO] - Media: 0.030172961870139937\n",
      "2025-03-31 11:48:34,923 [INFO] - Mediana: -0.9\n",
      "2025-03-31 11:48:34,923 [INFO] - Desv. Estándar: 12.340907430807254\n",
      "2025-03-31 11:48:34,929 [INFO] \n",
      "Estadísticas para AAII_Bearish_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:48:34,931 [INFO] - Min: 0.06\n",
      "2025-03-31 11:48:34,931 [INFO] - Max: 0.7027\n",
      "2025-03-31 11:48:34,933 [INFO] - Media: 0.3508379509807757\n",
      "2025-03-31 11:48:34,933 [INFO] - Mediana: 0.34\n",
      "2025-03-31 11:48:34,934 [INFO] - Desv. Estándar: 0.10334892901173451\n",
      "2025-03-31 11:48:34,939 [INFO] \n",
      "Estadísticas para AAII_Bull-Bear Spread_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:48:34,940 [INFO] - Min: -0.54\n",
      "2025-03-31 11:48:34,940 [INFO] - Max: 0.6286\n",
      "2025-03-31 11:48:34,942 [INFO] - Media: 0.000662551393898613\n",
      "2025-03-31 11:48:34,943 [INFO] - Mediana: 0.0\n",
      "2025-03-31 11:48:34,945 [INFO] - Desv. Estándar: 0.1933573104733393\n",
      "2025-03-31 11:48:34,948 [INFO] \n",
      "Estadísticas para AAII_Bullish_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:48:34,950 [INFO] - Min: 0.12\n",
      "2025-03-31 11:48:34,950 [INFO] - Max: 0.75\n",
      "2025-03-31 11:48:34,950 [INFO] - Media: 0.35150050237467434\n",
      "2025-03-31 11:48:34,951 [INFO] - Mediana: 0.347418\n",
      "2025-03-31 11:48:34,951 [INFO] - Desv. Estándar: 0.09729409084289153\n",
      "2025-03-31 11:48:34,956 [INFO] \n",
      "Estadísticas para PutCall_strike_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,956 [INFO] - Min: 5.0\n",
      "2025-03-31 11:48:34,957 [INFO] - Max: 735.0\n",
      "2025-03-31 11:48:34,957 [INFO] - Media: 422.74466822606723\n",
      "2025-03-31 11:48:34,959 [INFO] - Mediana: 415.0\n",
      "2025-03-31 11:48:34,960 [INFO] - Desv. Estándar: 106.67710620068586\n",
      "2025-03-31 11:48:34,963 [INFO] \n",
      "Estadísticas para PutCall_bid_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,965 [INFO] - Min: 0.0\n",
      "2025-03-31 11:48:34,965 [INFO] - Max: 336.56\n",
      "2025-03-31 11:48:34,965 [INFO] - Media: 28.79837982869526\n",
      "2025-03-31 11:48:34,966 [INFO] - Mediana: 8.74\n",
      "2025-03-31 11:48:34,966 [INFO] - Desv. Estándar: 37.47521121114415\n",
      "2025-03-31 11:48:34,971 [INFO] \n",
      "Estadísticas para PutCall_ask_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,973 [INFO] - Min: 0.0\n",
      "2025-03-31 11:48:34,973 [INFO] - Max: 337.21\n",
      "2025-03-31 11:48:34,973 [INFO] - Media: 29.129587561487387\n",
      "2025-03-31 11:48:34,974 [INFO] - Mediana: 8.84\n",
      "2025-03-31 11:48:34,974 [INFO] - Desv. Estándar: 37.78666680625484\n",
      "2025-03-31 11:48:34,977 [INFO] \n",
      "Estadísticas para PutCall_vol_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,979 [INFO] - Min: -1.0\n",
      "2025-03-31 11:48:34,979 [INFO] - Max: 4.296\n",
      "2025-03-31 11:48:34,980 [INFO] - Media: 0.27433479498469265\n",
      "2025-03-31 11:48:34,980 [INFO] - Mediana: 0.2322\n",
      "2025-03-31 11:48:34,982 [INFO] - Desv. Estándar: 0.17021965635656194\n",
      "2025-03-31 11:48:34,985 [INFO] \n",
      "Estadísticas para PutCall_delta_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,986 [INFO] - Min: -1.0\n",
      "2025-03-31 11:48:34,986 [INFO] - Max: 1.0\n",
      "2025-03-31 11:48:34,988 [INFO] - Media: 0.020562159024457363\n",
      "2025-03-31 11:48:34,989 [INFO] - Mediana: 0.0\n",
      "2025-03-31 11:48:34,989 [INFO] - Desv. Estándar: 0.649676453332417\n",
      "2025-03-31 11:48:34,994 [INFO] \n",
      "Estadísticas para PutCall_gamma_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:34,994 [INFO] - Min: 0.0\n",
      "2025-03-31 11:48:34,996 [INFO] - Max: 0.1225\n",
      "2025-03-31 11:48:34,996 [INFO] - Media: 0.006018882219393897\n",
      "2025-03-31 11:48:34,997 [INFO] - Mediana: 0.0025\n",
      "2025-03-31 11:48:34,997 [INFO] - Desv. Estándar: 0.007873098906305801\n",
      "2025-03-31 11:48:35,002 [INFO] \n",
      "Estadísticas para PutCall_theta_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:35,003 [INFO] - Min: -3.8015\n",
      "2025-03-31 11:48:35,003 [INFO] - Max: 0.0\n",
      "2025-03-31 11:48:35,005 [INFO] - Media: -0.07004985552612569\n",
      "2025-03-31 11:48:35,005 [INFO] - Mediana: -0.0531\n",
      "2025-03-31 11:48:35,005 [INFO] - Desv. Estándar: 0.0997670179386413\n",
      "2025-03-31 11:48:35,009 [INFO] \n",
      "Estadísticas para PutCall_vega_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:35,009 [INFO] - Min: 0.0\n",
      "2025-03-31 11:48:35,011 [INFO] - Max: 1.0124\n",
      "2025-03-31 11:48:35,011 [INFO] - Media: 0.17602285525093733\n",
      "2025-03-31 11:48:35,011 [INFO] - Mediana: 0.1075\n",
      "2025-03-31 11:48:35,012 [INFO] - Desv. Estándar: 0.18784802159085914\n",
      "2025-03-31 11:48:35,015 [INFO] \n",
      "Estadísticas para PutCall_rho_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:48:35,017 [INFO] - Min: -1.0682\n",
      "2025-03-31 11:48:35,017 [INFO] - Max: 0.8462\n",
      "2025-03-31 11:48:35,017 [INFO] - Media: 0.03510286281861649\n",
      "2025-03-31 11:48:35,018 [INFO] - Mediana: 0.0\n",
      "2025-03-31 11:48:35,018 [INFO] - Desv. Estándar: 0.21527614162821226\n",
      "2025-03-31 11:48:35,022 [INFO] \n",
      "Estadísticas para NFCI_Chicago_Fed_NFCI_leading_economic_index:\n",
      "2025-03-31 11:48:35,023 [INFO] - Min: -0.799666616089861\n",
      "2025-03-31 11:48:35,023 [INFO] - Max: 0.494555538914267\n",
      "2025-03-31 11:48:35,023 [INFO] - Media: -0.4262457009893776\n",
      "2025-03-31 11:48:35,025 [INFO] - Mediana: -0.508249851268891\n",
      "2025-03-31 11:48:35,025 [INFO] - Desv. Estándar: 0.22244254168275923\n",
      "2025-03-31 11:48:35,028 [INFO] \n",
      "Estadísticas para ANFCI_Chicago_Fed_NFCI_leading_economic_index:\n",
      "2025-03-31 11:48:35,028 [INFO] - Min: -0.908607104751089\n",
      "2025-03-31 11:48:35,028 [INFO] - Max: 0.633457159158181\n",
      "2025-03-31 11:48:35,029 [INFO] - Media: -0.4770622444086897\n",
      "2025-03-31 11:48:35,029 [INFO] - Mediana: -0.549216080706813\n",
      "2025-03-31 11:48:35,031 [INFO] - Desv. Estándar: 0.24438137382544337\n",
      "2025-03-31 11:48:35,032 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:48:35,035 [INFO] GUARDANDO RESULTADOS\n",
      "2025-03-31 11:48:35,035 [INFO] ==================================================\n",
      "2025-03-31 11:48:35,037 [INFO] Guardando resultados en: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:48:35,038 [INFO] Guardando DataFrame combinado en 'datos_economicos_other_procesados.xlsx'...\n",
      "2025-03-31 11:48:49,669 [INFO] Guardando estadísticas de los indicadores...\n",
      "2025-03-31 11:49:12,589 [INFO] Archivo guardado exitosamente: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:49:12,590 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:49:12,590 [INFO] RESUMEN DE EJECUCIÓN\n",
      "2025-03-31 11:49:12,592 [INFO] ==================================================\n",
      "2025-03-31 11:49:12,592 [INFO] Proceso: OtherDataProcessor\n",
      "2025-03-31 11:49:12,593 [INFO] Tiempo de ejecución: 38.45 segundos\n",
      "2025-03-31 11:49:12,593 [INFO] Archivos procesados: 4\n",
      "2025-03-31 11:49:12,593 [INFO] Archivos con error: 0\n",
      "2025-03-31 11:49:12,595 [INFO] Archivos procesados correctamente: 4\n",
      "2025-03-31 11:49:12,595 [INFO] Periodo de datos: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:49:12,596 [INFO] Datos combinados: 127835 filas, 17 columnas\n",
      "2025-03-31 11:49:12,596 [INFO] Archivo de salida: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:49:12,596 [INFO] Estado: COMPLETADO\n",
      "2025-03-31 11:49:12,598 [INFO] ==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de logging\n",
    "def configurar_logging(log_file='otherdataprocessor.log'):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('OtherDataProcessor')\n",
    "\n",
    "# Función para convertir fechas en diversos formatos\n",
    "def convertir_fecha(fecha_str):\n",
    "    \"\"\"\n",
    "    Convierte diversos formatos de fecha a pd.Timestamp.\n",
    "    \"\"\"\n",
    "    if isinstance(fecha_str, (pd.Timestamp, datetime)):\n",
    "        return pd.Timestamp(fecha_str)\n",
    "    \n",
    "    if pd.isna(fecha_str):\n",
    "        return None\n",
    "\n",
    "    if isinstance(fecha_str, str):\n",
    "        fecha_str = fecha_str.strip()\n",
    "        formatos = [\n",
    "            '%d.%m.%Y', '%d/%m/%Y', '%d-%m-%Y',\n",
    "            '%m/%d/%Y', '%Y-%m-%d',\n",
    "            '%Y%m%d', '%d%m%Y'\n",
    "        ]\n",
    "        for fmt in formatos:\n",
    "            try:\n",
    "                return pd.to_datetime(fecha_str, format=fmt)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        # Intentar detectar patrones como \"Apr 01, 2025\" o meses en español\n",
    "        try:\n",
    "            if re.search(r'([A-Za-z]+\\s+\\d+,\\s+\\d{4})', fecha_str):\n",
    "                match = re.search(r'([A-Za-z]+\\s+\\d+,\\s+\\d{4})', fecha_str)\n",
    "                return pd.to_datetime(match.group(1))\n",
    "            # Reemplazar meses en español por inglés\n",
    "            meses_es = {\n",
    "                'ene': 'Jan', 'feb': 'Feb', 'mar': 'Mar', 'abr': 'Apr',\n",
    "                'may': 'May', 'jun': 'Jun', 'jul': 'Jul', 'ago': 'Aug',\n",
    "                'sep': 'Sep', 'oct': 'Oct', 'nov': 'Nov', 'dic': 'Dec'\n",
    "            }\n",
    "            texto_procesado = fecha_str.lower()\n",
    "            for mes_es, mes_en in meses_es.items():\n",
    "                if mes_es in texto_procesado:\n",
    "                    texto_procesado = texto_procesado.replace(mes_es, mes_en)\n",
    "            return pd.to_datetime(texto_procesado)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Intento final con pandas por defecto\n",
    "    try:\n",
    "        return pd.to_datetime(fecha_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class OtherDataProcessor:\n",
    "    \"\"\"\n",
    "    Clase para procesar datos de la fuente \"Other\" usando scripts personalizados.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_file, data_root='data/Macro/raw', log_file='otherdataprocessor.log'):\n",
    "        self.config_file = config_file\n",
    "        self.data_root = data_root\n",
    "        self.logger = configurar_logging(log_file)\n",
    "        self.config_data = None\n",
    "        self.fecha_min_global = None\n",
    "        self.fecha_max_global = None\n",
    "        self.indice_diario = None\n",
    "        self.datos_procesados = {}\n",
    "        self.df_combinado = None\n",
    "        self.estadisticas = {}\n",
    "        \n",
    "        # Mapeo de variables a rutas de archivos conocidas\n",
    "        self.data_paths = {\n",
    "            'US_Empire_State_Index': os.path.join('business_confidence', 'US_Empire_State_Index.csv'),\n",
    "            'AAII_Investor_Sentiment': os.path.join('consumer_confidence', 'AAII_Investor_Sentiment.xls'),\n",
    "            'Put_Call_Ratio_SPY': os.path.join('consumer_confidence', 'Put_Call_Ratio_SPY.csv'),\n",
    "            'Chicago_Fed_NFCI': os.path.join('leading_economic_index', 'Chicago_Fed_NFCI.csv')\n",
    "        }\n",
    "\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"INICIANDO PROCESO: OtherDataProcessor\")\n",
    "        self.logger.info(f\"Archivo de configuración: {config_file}\")\n",
    "        self.logger.info(f\"Directorio raíz de datos: {data_root}\")\n",
    "        self.logger.info(f\"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "    \n",
    "    def leer_configuracion(self):\n",
    "        \"\"\"\n",
    "        Lee y filtra la configuración del archivo Excel para la fuente \"Other\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Leyendo archivo de configuración...\")\n",
    "            df_config = pd.read_excel(self.config_file)\n",
    "            \n",
    "            # Filtrar solo por fuente \"Other\" (sin restricción de tipo de preprocesamiento)\n",
    "            self.config_data = df_config[df_config['Fuente'] == 'Other'].copy()\n",
    "            \n",
    "            num_configs = len(self.config_data)\n",
    "            self.logger.info(f\"Se encontraron {num_configs} configuraciones para procesar con fuente 'Other'\")\n",
    "            \n",
    "            if num_configs == 0:\n",
    "                self.logger.warning(\"No se encontraron configuraciones que cumplan los criterios\")\n",
    "                return None\n",
    "                \n",
    "            return self.config_data\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer configuración: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def encontrar_ruta_archivo(self, variable, tipo_macro):\n",
    "        \"\"\"\n",
    "        Encuentra la ruta completa del archivo basado en la variable y tipo_macro\n",
    "        \"\"\"\n",
    "        # Primero verificar si tenemos una ruta conocida para esta variable\n",
    "        if variable in self.data_paths:\n",
    "            ruta_conocida = os.path.join(self.data_root, self.data_paths[variable])\n",
    "            if os.path.exists(ruta_conocida):\n",
    "                return ruta_conocida\n",
    "                \n",
    "        # Construir ruta basada en la estructura de directorios\n",
    "        ruta_base = os.path.join(self.data_root, tipo_macro)\n",
    "        \n",
    "        # Verificar diferentes extensiones\n",
    "        for ext in ['.csv', '.xlsx', '.xls']:\n",
    "            nombre_archivo = f\"{variable}{ext}\"\n",
    "            ruta_completa = os.path.join(ruta_base, nombre_archivo)\n",
    "            \n",
    "            if os.path.exists(ruta_completa):\n",
    "                return ruta_completa\n",
    "        \n",
    "        # Si no se encuentra, intentar buscar en todos los subdirectorios\n",
    "        for root, dirs, files in os.walk(self.data_root):\n",
    "            for file in files:\n",
    "                if file.startswith(variable) and file.endswith(('.csv', '.xlsx', '.xls')):\n",
    "                    return os.path.join(root, file)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def ejecutar_script(self, script_path, variable, output_path=None):\n",
    "        \"\"\"\n",
    "        Ejecuta un script de Python a través del sistema\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Ejecutando script: {script_path}\")\n",
    "            \n",
    "            # Cargar el script como un módulo\n",
    "            spec = importlib.util.spec_from_file_location(\"custom_script\", script_path)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            \n",
    "            # Guardar el directorio actual\n",
    "            current_dir = os.getcwd()\n",
    "            script_dir = os.path.dirname(os.path.abspath(script_path))\n",
    "            \n",
    "            # Cambiar al directorio del script para evitar problemas de rutas relativas\n",
    "            os.chdir(script_dir)\n",
    "            \n",
    "            # Ejecutar el script\n",
    "            sys.argv = ['']  # Reset argv para evitar conflictos\n",
    "            try:\n",
    "                spec.loader.exec_module(module)\n",
    "                resultado = True\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error al ejecutar el script {script_path}: {str(e)}\")\n",
    "                resultado = False\n",
    "            \n",
    "            # Restaurar el directorio original\n",
    "            os.chdir(current_dir)\n",
    "            \n",
    "            return resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al cargar el script {script_path}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def procesar_empire_state_manualmente(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Empire State Index manualmente cuando el script no está disponible\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        \n",
    "        # Construir ruta directa basada en la estructura conocida\n",
    "        ruta_directa = os.path.join(self.data_root, 'business_confidence', f\"{variable}.csv\")\n",
    "        \n",
    "        # Si no existe, intentar con la búsqueda normal\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "            \n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar el archivo CSV\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            \n",
    "            # Crear un nuevo DataFrame con las columnas necesarias\n",
    "            nuevo_df = pd.DataFrame()\n",
    "            \n",
    "            # Buscar columna de fecha\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'survey', 'week']):\n",
    "                    fecha_col = col\n",
    "                    break\n",
    "            \n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                fecha_col = df.columns[0]  # Usar primera columna como fallback\n",
    "            \n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "            \n",
    "            # Verificar si existe la columna surveyDate\n",
    "            if 'surveyDate' in df.columns:\n",
    "                fecha_col = 'surveyDate'\n",
    "                \n",
    "            nuevo_df['fecha'] = pd.to_datetime(df[fecha_col])\n",
    "            \n",
    "            # Verificar rango de fechas\n",
    "            fecha_min = nuevo_df['fecha'].min()\n",
    "            fecha_max = nuevo_df['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Verificar si tiene datos desde 2014\n",
    "            fecha_2014 = pd.Timestamp('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            \n",
    "            if fecha_min > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            \n",
    "            # Manejo específico para la configuración business_confidence + macro_business_confidence_fe_1month\n",
    "            if tipo_macro == 'business_confidence' and target_col == 'macro_business_confidence_fe_1month':\n",
    "                # Usar las columnas específicas como están en el archivo original\n",
    "                if 'GACDISA' in df.columns:\n",
    "                    nuevo_df[f'{target_col}_GACDISA_{variable}_{tipo_macro}'] = df['GACDISA']\n",
    "                    self.logger.info(f\"- Columna GACDISA encontrada y procesada\")\n",
    "                if 'AWCDISA' in df.columns:\n",
    "                    nuevo_df[f'{target_col}_AWCDISA_{variable}_{tipo_macro}'] = df['AWCDISA']\n",
    "                    self.logger.info(f\"- Columna AWCDISA encontrada y procesada\")\n",
    "                \n",
    "                self.logger.info(f\"Procesamiento específico para {variable} con TARGET={target_col}\")\n",
    "            else:\n",
    "                # Procesamiento estándar como teníamos antes\n",
    "                # Determinar qué columnas usar basado en las disponibles\n",
    "                columnas_indicador = ['GACDISA', 'AWCDISA', 'headline', 'main_index']\n",
    "                \n",
    "                prefix = target_col if pd.notna(target_col) else 'ESI'\n",
    "                \n",
    "                columnas_encontradas = []\n",
    "                for col in columnas_indicador:\n",
    "                    if col in df.columns:\n",
    "                        nuevo_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "                        columnas_encontradas.append(col)\n",
    "                \n",
    "                if columnas_encontradas:\n",
    "                    self.logger.info(f\"- Columnas encontradas y procesadas: {', '.join(columnas_encontradas)}\")\n",
    "                \n",
    "                # Si no se encontraron columnas específicas, usar todas las numéricas excepto la fecha\n",
    "                if len(nuevo_df.columns) == 1:  # Solo tiene la columna fecha\n",
    "                    columnas_usadas = []\n",
    "                    for col in df.columns:\n",
    "                        if col != fecha_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                            nuevo_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "                            columnas_usadas.append(col)\n",
    "                    \n",
    "                    if columnas_usadas:\n",
    "                        self.logger.info(f\"- Se usaron columnas numéricas genéricas: {', '.join(columnas_usadas)}\")\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            nuevo_df = nuevo_df.sort_values('fecha')\n",
    "            \n",
    "            # Mostrar información de cada columna de datos\n",
    "            for col in nuevo_df.columns:\n",
    "                if col != 'fecha':\n",
    "                    col_no_nulos = nuevo_df[col].notna().sum()\n",
    "                    pct_no_nulos = (col_no_nulos / len(nuevo_df)) * 100\n",
    "                    \n",
    "                    fecha_min_col = nuevo_df.loc[nuevo_df[col].notna(), 'fecha'].min()\n",
    "                    fecha_max_col = nuevo_df.loc[nuevo_df[col].notna(), 'fecha'].max()\n",
    "                    \n",
    "                    self.logger.info(f\"- Columna {col}: {col_no_nulos} valores no nulos ({pct_no_nulos:.2f}%), \"\n",
    "                                    f\"rango: {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Calcular fechas mínima y máxima\n",
    "            fecha_min = nuevo_df['fecha'].min()\n",
    "            fecha_max = nuevo_df['fecha'].max()\n",
    "            \n",
    "            # Registrar estadísticas\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or 'ESI',\n",
    "                'total_filas': len(nuevo_df),\n",
    "                'valores_validos': len(nuevo_df),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {len(nuevo_df)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return variable, nuevo_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "    \n",
    "    def procesar_aaii_investor_sentiment(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de AAII Investor Sentiment manualmente\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        \n",
    "        # Construir ruta directa basada en la estructura conocida\n",
    "        ruta_directa = os.path.join(self.data_root, 'consumer_confidence', f\"{variable}.xls\")\n",
    "        \n",
    "        # Si no existe, intentar con la búsqueda normal\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "            \n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Intentar leer el archivo Excel con diferentes engines\n",
    "            df = None\n",
    "            engines = ['openpyxl', 'xlrd']\n",
    "            \n",
    "            for engine in engines:\n",
    "                try:\n",
    "                    self.logger.info(f\"Intentando leer Excel con engine: {engine}\")\n",
    "                    df = pd.read_excel(input_file, engine=engine)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error al leer con engine {engine}: {str(e)}\")\n",
    "            \n",
    "            # Si todos los engines fallan, intentar leer como CSV\n",
    "            if df is None:\n",
    "                try:\n",
    "                    self.logger.info(\"Intentando leer como CSV...\")\n",
    "                    df = pd.read_csv(input_file)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error al leer como CSV: {str(e)}\")\n",
    "                    return variable, None\n",
    "            \n",
    "            if df is None or len(df) == 0:\n",
    "                self.logger.error(f\"No se pudo leer el archivo {input_file} con ningún método\")\n",
    "                return variable, None\n",
    "                \n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            \n",
    "            # Identificar la columna de fecha\n",
    "            columna_fecha = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'week', 'period']):\n",
    "                    columna_fecha = col\n",
    "                    self.logger.info(f\"- Usando columna '{col}' como fecha\")\n",
    "                    break\n",
    "            \n",
    "            if columna_fecha is None:\n",
    "                # Si no se encuentra una columna específica, usar la primera columna como fecha\n",
    "                columna_fecha = df.columns[0]\n",
    "                self.logger.info(f\"- Usando primera columna '{columna_fecha}' como fecha (fallback)\")\n",
    "            \n",
    "            # Verificar que las columnas necesarias existen\n",
    "            columnas_objetivo = ['Bearish', 'Bull-Bear Spread', 'Bullish']\n",
    "            columnas_encontradas = [col for col in columnas_objetivo if col in df.columns]\n",
    "            columnas_faltantes = [col for col in columnas_objetivo if col not in df.columns]\n",
    "            \n",
    "            if columnas_encontradas:\n",
    "                self.logger.info(f\"- Columnas encontradas: {', '.join(columnas_encontradas)}\")\n",
    "            if columnas_faltantes:\n",
    "                self.logger.warning(f\"- Columnas no encontradas: {', '.join(columnas_faltantes)}\")\n",
    "            \n",
    "            # Crear el resultado\n",
    "            resultado = pd.DataFrame()\n",
    "            resultado['fecha'] = pd.to_datetime(df[columna_fecha])\n",
    "            \n",
    "            # Verificar rango de fechas\n",
    "            fecha_min_total = resultado['fecha'].min()\n",
    "            fecha_max_total = resultado['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas total: {fecha_min_total.strftime('%Y-%m-%d')} a {fecha_max_total.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Verificar si tiene datos desde 2014\n",
    "            fecha_2014 = pd.to_datetime('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min_total <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            \n",
    "            if fecha_min_total > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min_total.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            \n",
    "            # Prefijo para renombrar las columnas\n",
    "            prefix = target_col if pd.notna(target_col) else 'AAII'\n",
    "            \n",
    "            # Añadir columnas de datos con los prefijos adecuados\n",
    "            for columna in columnas_encontradas:\n",
    "                nombre_col = f'{prefix}_{columna}_{variable}_{tipo_macro}'\n",
    "                resultado[nombre_col] = df[columna]\n",
    "                \n",
    "                # Verificar rango de fechas para esta columna\n",
    "                fechas_validas = resultado[resultado[nombre_col].notna()]\n",
    "                if len(fechas_validas) > 0:\n",
    "                    fecha_min_col = fechas_validas['fecha'].min()\n",
    "                    fecha_max_col = fechas_validas['fecha'].max()\n",
    "                    self.logger.info(f\"- Columna {nombre_col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                    self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(resultado)} ({len(fechas_validas)/len(resultado)*100:.2f}%)\")\n",
    "            \n",
    "            # Si no se encontraron columnas específicas, usar todas las numéricas\n",
    "            if not columnas_encontradas:\n",
    "                self.logger.warning(f\"- No se encontraron columnas específicas. Usando columnas numéricas disponibles.\")\n",
    "                columnas_usadas = []\n",
    "                for col in df.columns:\n",
    "                    if col != columna_fecha and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                        nombre_col = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                        resultado[nombre_col] = df[col]\n",
    "                        columnas_usadas.append(col)\n",
    "                        \n",
    "                        # Verificar rango de fechas para esta columna\n",
    "                        fechas_validas = resultado[resultado[nombre_col].notna()]\n",
    "                        if len(fechas_validas) > 0:\n",
    "                            fecha_min_col = fechas_validas['fecha'].min()\n",
    "                            fecha_max_col = fechas_validas['fecha'].max()\n",
    "                            self.logger.info(f\"- Columna {nombre_col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                            self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(resultado)} ({len(fechas_validas)/len(resultado)*100:.2f}%)\")\n",
    "                \n",
    "                if columnas_usadas:\n",
    "                    self.logger.info(f\"- Se utilizaron columnas numéricas alternativas: {', '.join(columnas_usadas)}\")\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            resultado = resultado.sort_values('fecha')\n",
    "            \n",
    "            # Calcular fechas mínima y máxima\n",
    "            fecha_min = resultado['fecha'].min()\n",
    "            fecha_max = resultado['fecha'].max()\n",
    "            \n",
    "            # Registrar estadísticas\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or prefix,\n",
    "                'total_filas': len(resultado),\n",
    "                'valores_validos': len(resultado),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {len(resultado)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return variable, resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "            \n",
    "    def procesar_put_call_ratio(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Put Call Ratio manualmente\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        \n",
    "        # Construir ruta directa basada en la estructura conocida\n",
    "        ruta_directa = os.path.join(self.data_root, 'consumer_confidence', f\"{variable}.csv\")\n",
    "        \n",
    "        # Si no existe, intentar con la búsqueda normal\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "            \n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar el archivo CSV\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            \n",
    "            # Crear un nuevo DataFrame para el resultado\n",
    "            result_df = pd.DataFrame()\n",
    "            \n",
    "            # Buscar columna de fecha\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'time', 'periodo']):\n",
    "                    fecha_col = col\n",
    "                    self.logger.info(f\"- Usando columna '{col}' como fecha\")\n",
    "                    break\n",
    "            \n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                fecha_col = df.columns[0]  # Usar primera columna como fallback\n",
    "                self.logger.info(f\"- Usando primera columna '{fecha_col}' como fecha (fallback)\")\n",
    "            \n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "                \n",
    "            # Convertir fechas y añadir al resultado\n",
    "            result_df['fecha'] = pd.to_datetime(df[fecha_col])\n",
    "            \n",
    "            # Verificar rango de fechas\n",
    "            fecha_min_total = result_df['fecha'].min()\n",
    "            fecha_max_total = result_df['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas total: {fecha_min_total.strftime('%Y-%m-%d')} a {fecha_max_total.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Verificar si tiene datos desde 2014\n",
    "            fecha_2014 = pd.to_datetime('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min_total <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            \n",
    "            if fecha_min_total > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min_total.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            \n",
    "            # Generar un indicador de confianza simplificado basado en el ratio put/call\n",
    "            col_name = f'consumer_confidence_PutCall_{variable}_{tipo_macro}'\n",
    "            \n",
    "            # Si ya existe la columna de confianza (archivo pre-procesado), usarla directamente\n",
    "            if 'consumer_confidence_PutCall' in df.columns:\n",
    "                self.logger.info(f\"- Usando columna 'consumer_confidence_PutCall' existente\")\n",
    "                result_df[col_name] = df['consumer_confidence_PutCall']\n",
    "                \n",
    "                # Verificar rango de fechas para esta columna\n",
    "                fechas_validas = result_df[result_df[col_name].notna()]\n",
    "                if len(fechas_validas) > 0:\n",
    "                    fecha_min_col = fechas_validas['fecha'].min()\n",
    "                    fecha_max_col = fechas_validas['fecha'].max()\n",
    "                    self.logger.info(f\"- Columna {col_name}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                    self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(result_df)} ({len(fechas_validas)/len(result_df)*100:.2f}%)\")\n",
    "            else:\n",
    "                # Intentar calcular un indicador basado en volúmenes si están disponibles\n",
    "                if 'call_volume' in df.columns and 'put_volume' in df.columns:\n",
    "                    self.logger.info(f\"- Calculando ratio basado en put_volume y call_volume\")\n",
    "                    # Calcular ratio put/call\n",
    "                    df['put_call_ratio'] = df['put_volume'] / df['call_volume'].replace(0, np.nan)\n",
    "                    \n",
    "                    # Usar una transformación simple para convertir el ratio en un indicador de confianza\n",
    "                    # (valores más bajos del ratio indican más confianza)\n",
    "                    if df['put_call_ratio'].notna().sum() > 0:\n",
    "                        max_ratio = df['put_call_ratio'].max()\n",
    "                        result_df[col_name] = 100 * (1 - df['put_call_ratio'] / max_ratio)\n",
    "                        \n",
    "                        # Verificar rango de fechas para esta columna\n",
    "                        fechas_validas = result_df[result_df[col_name].notna()]\n",
    "                        if len(fechas_validas) > 0:\n",
    "                            fecha_min_col = fechas_validas['fecha'].min()\n",
    "                            fecha_max_col = fechas_validas['fecha'].max()\n",
    "                            self.logger.info(f\"- Columna {col_name}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                            self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(result_df)} ({len(fechas_validas)/len(result_df)*100:.2f}%)\")\n",
    "                    else:\n",
    "                        self.logger.warning(f\"- No se pudieron calcular ratios put/call (posibles divisiones por cero)\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"- No se encontraron columnas call_volume y put_volume. Buscando otras columnas numéricas.\")\n",
    "                    # Si no hay columnas específicas, usar todas las numéricas excepto la fecha\n",
    "                    columnas_usadas = []\n",
    "                    for col in df.columns:\n",
    "                        if col != fecha_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                            prefix = target_col if pd.notna(target_col) else 'PutCall'\n",
    "                            nuevo_nombre = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                            result_df[nuevo_nombre] = df[col]\n",
    "                            columnas_usadas.append(col)\n",
    "                            \n",
    "                            # Verificar rango de fechas para esta columna\n",
    "                            fechas_validas = result_df[result_df[nuevo_nombre].notna()]\n",
    "                            if len(fechas_validas) > 0:\n",
    "                                fecha_min_col = fechas_validas['fecha'].min()\n",
    "                                fecha_max_col = fechas_validas['fecha'].max()\n",
    "                                self.logger.info(f\"- Columna {nuevo_nombre}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                                self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(result_df)} ({len(fechas_validas)/len(result_df)*100:.2f}%)\")\n",
    "                    \n",
    "                    if columnas_usadas:\n",
    "                        self.logger.info(f\"- Se utilizaron columnas numéricas alternativas: {', '.join(columnas_usadas)}\")\n",
    "                    else:\n",
    "                        self.logger.warning(f\"- No se encontraron columnas numéricas alternativas\")\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            result_df = result_df.sort_values('fecha')\n",
    "            \n",
    "            # Calcular fechas mínima y máxima\n",
    "            fecha_min = result_df['fecha'].min()\n",
    "            fecha_max = result_df['fecha'].max()\n",
    "            \n",
    "            # Registrar estadísticas\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or 'PutCall',\n",
    "                'total_filas': len(result_df),\n",
    "                'valores_validos': len(result_df),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {len(result_df)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return variable, result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "            \n",
    "    def procesar_archivo_generico(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesamiento genérico para cualquier archivo de datos\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} de forma genérica\")\n",
    "        \n",
    "        # Buscar el archivo en su ubicación esperada o en el árbol de directorios\n",
    "        input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "        if input_file is None:\n",
    "            self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "            return variable, None\n",
    "            \n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar el archivo según su extensión\n",
    "            if input_file.endswith('.csv'):\n",
    "                df = pd.read_csv(input_file)\n",
    "            elif input_file.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(input_file)\n",
    "            else:\n",
    "                self.logger.error(f\"Formato de archivo no soportado: {input_file}\")\n",
    "                return variable, None\n",
    "            \n",
    "            # Buscar columna de fecha\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'time', 'fecha', 'week', 'period']):\n",
    "                    fecha_col = col\n",
    "                    break\n",
    "            \n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                # Intentar con la primera columna como fecha\n",
    "                try:\n",
    "                    pd.to_datetime(df.iloc[:, 0])\n",
    "                    fecha_col = df.columns[0]\n",
    "                except:\n",
    "                    self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                    return variable, None\n",
    "            \n",
    "            # Crear un nuevo DataFrame con las columnas estandarizadas\n",
    "            result_df = pd.DataFrame()\n",
    "            result_df['fecha'] = pd.to_datetime(df[fecha_col])\n",
    "            result_df = result_df.dropna(subset=['fecha'])\n",
    "            \n",
    "            # Determinar el prefijo para las columnas de datos\n",
    "            prefix = target_col if pd.notna(target_col) else variable.split('_')[0]\n",
    "            \n",
    "            # Añadir todas las columnas numéricas con prefijo estandarizado\n",
    "            for col in df.columns:\n",
    "                if col != fecha_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    result_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            result_df = result_df.sort_values('fecha')\n",
    "            \n",
    "            # Calcular fechas mínima y máxima\n",
    "            fecha_min = result_df['fecha'].min()\n",
    "            fecha_max = result_df['fecha'].max()\n",
    "            \n",
    "            # Registrar estadísticas\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or prefix,\n",
    "                'total_filas': len(result_df),\n",
    "                'valores_validos': len(result_df),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {len(result_df)} filas procesadas genéricamente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return variable, result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} genéricamente: {str(e)}\")\n",
    "            return variable, None\n",
    "    \n",
    "    def procesar_chicago_fed_manualmente(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Chicago Fed NFCI manualmente cuando el script no está disponible\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        \n",
    "        # Construir ruta directa basada en la estructura conocida\n",
    "        ruta_directa = os.path.join(self.data_root, 'leading_economic_index', f\"{variable}.csv\")\n",
    "        \n",
    "        # Si no existe, intentar con la búsqueda normal\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "            \n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            # Cargar el dataset\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            \n",
    "            # Convertir la columna de fecha al formato correcto\n",
    "            if 'Friday_of_Week' in df.columns:\n",
    "                df['fecha'] = pd.to_datetime(df['Friday_of_Week'], format='%m/%d/%Y')\n",
    "                self.logger.info(f\"- Usando columna 'Friday_of_Week' como fecha\")\n",
    "            else:\n",
    "                # Buscar columna de fecha\n",
    "                for col in df.columns:\n",
    "                    if any(term in col.lower() for term in ['date', 'week', 'period']):\n",
    "                        df['fecha'] = pd.to_datetime(df[col])\n",
    "                        self.logger.info(f\"- Usando columna '{col}' como fecha\")\n",
    "                        break\n",
    "            \n",
    "            # Verificar que tenemos una columna de fecha\n",
    "            if 'fecha' not in df.columns:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "            \n",
    "            # Verificar rango de fechas total\n",
    "            fecha_min_total = df['fecha'].min()\n",
    "            fecha_max_total = df['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas total: {fecha_min_total.strftime('%Y-%m-%d')} a {fecha_max_total.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Verificar si tiene datos desde 2014\n",
    "            fecha_2014 = pd.to_datetime('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min_total <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            \n",
    "            if fecha_min_total > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min_total.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            \n",
    "            # Filtrar desde 2014 si corresponde\n",
    "            df_filtrado = df[df['fecha'] >= fecha_2014]\n",
    "            self.logger.info(f\"- Filtrando desde 2014-01-01: {len(df_filtrado)}/{len(df)} filas ({(len(df_filtrado)/len(df)*100):.2f}%)\")\n",
    "            \n",
    "            # Crear resultado\n",
    "            resultado = pd.DataFrame()\n",
    "            resultado['fecha'] = df_filtrado['fecha']\n",
    "            \n",
    "            # Verificar y añadir columnas NFCI y ANFCI \n",
    "            columnas_procesadas = []\n",
    "            if 'NFCI' in df_filtrado.columns:\n",
    "                resultado[f'NFCI_{variable}_{tipo_macro}'] = df_filtrado['NFCI']\n",
    "                columnas_procesadas.append('NFCI')\n",
    "                \n",
    "                # Verificar rango de fechas para NFCI\n",
    "                fecha_min_nfci = df_filtrado.loc[df_filtrado['NFCI'].notna(), 'fecha'].min()\n",
    "                fecha_max_nfci = df_filtrado.loc[df_filtrado['NFCI'].notna(), 'fecha'].max()\n",
    "                self.logger.info(f\"- Columna NFCI: Rango de fechas {fecha_min_nfci.strftime('%Y-%m-%d')} a {fecha_max_nfci.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            if 'ANFCI' in df_filtrado.columns:\n",
    "                resultado[f'ANFCI_{variable}_{tipo_macro}'] = df_filtrado['ANFCI']\n",
    "                columnas_procesadas.append('ANFCI')\n",
    "                \n",
    "                # Verificar rango de fechas para ANFCI\n",
    "                fecha_min_anfci = df_filtrado.loc[df_filtrado['ANFCI'].notna(), 'fecha'].min()\n",
    "                fecha_max_anfci = df_filtrado.loc[df_filtrado['ANFCI'].notna(), 'fecha'].max()\n",
    "                self.logger.info(f\"- Columna ANFCI: Rango de fechas {fecha_min_anfci.strftime('%Y-%m-%d')} a {fecha_max_anfci.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Si no se encontraron las columnas específicas, usar todas las numéricas excepto la fecha\n",
    "            if len(columnas_procesadas) == 0:\n",
    "                self.logger.warning(f\"- No se encontraron columnas NFCI o ANFCI. Utilizando columnas numéricas disponibles\")\n",
    "                \n",
    "                for col in df_filtrado.columns:\n",
    "                    if col != 'fecha' and pd.api.types.is_numeric_dtype(df_filtrado[col]):\n",
    "                        prefix = target_col if pd.notna(target_col) else 'NFCI'\n",
    "                        nuevo_nombre = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                        resultado[nuevo_nombre] = df_filtrado[col]\n",
    "                        columnas_procesadas.append(col)\n",
    "                        \n",
    "                        # Verificar rango de fechas para esta columna\n",
    "                        fecha_min_col = df_filtrado.loc[df_filtrado[col].notna(), 'fecha'].min()\n",
    "                        fecha_max_col = df_filtrado.loc[df_filtrado[col].notna(), 'fecha'].max()\n",
    "                        self.logger.info(f\"- Columna {col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            self.logger.info(f\"- Columnas procesadas: {', '.join(columnas_procesadas)}\")\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            resultado = resultado.sort_values('fecha')\n",
    "            \n",
    "            # Calcular fechas mínima y máxima\n",
    "            fecha_min = resultado['fecha'].min()\n",
    "            fecha_max = resultado['fecha'].max()\n",
    "            \n",
    "            # Registrar estadísticas\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or 'NFCI',\n",
    "                'total_filas': len(resultado),\n",
    "                'valores_validos': len(resultado),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {len(resultado)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            return variable, resultado\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "            \n",
    "    def cargar_resultado_script(self, output_file, variable, target_col):\n",
    "        \"\"\"\n",
    "        Carga el archivo de resultado generado por el script y lo estandariza\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Cargando resultado desde {output_file}\")\n",
    "            \n",
    "            # Comprobar si el archivo existe\n",
    "            if not os.path.exists(output_file):\n",
    "                self.logger.error(f\"El archivo de salida {output_file} no existe\")\n",
    "                return None\n",
    "            \n",
    "            # Cargar el archivo según su extensión\n",
    "            if output_file.endswith('.csv'):\n",
    "                df = pd.read_csv(output_file)\n",
    "            elif output_file.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(output_file)\n",
    "            else:\n",
    "                self.logger.error(f\"Formato de archivo no soportado: {output_file}\")\n",
    "                return None\n",
    "            \n",
    "            # Identificar columna de fecha\n",
    "            fecha_col = self.fecha_column_map.get(variable, None)\n",
    "            if fecha_col not in df.columns:\n",
    "                for col in df.columns:\n",
    "                    if any(palabra in col.lower() for palabra in ['date', 'fecha', 'time', 'día']):\n",
    "                        fecha_col = col\n",
    "                        break\n",
    "            \n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar la columna de fecha en {output_file}\")\n",
    "                return None\n",
    "            \n",
    "            # Crear un nuevo DataFrame con las columnas estandarizadas\n",
    "            result_df = pd.DataFrame()\n",
    "            \n",
    "            # Convertir y estandarizar la columna de fecha\n",
    "            result_df['fecha'] = df[fecha_col].apply(convertir_fecha)\n",
    "            result_df = result_df.dropna(subset=['fecha'])\n",
    "            \n",
    "            # Renombrar las columnas de datos con el patrón estándar\n",
    "            for col in df.columns:\n",
    "                if col != fecha_col:\n",
    "                    new_name = f\"{target_col}_{variable}_{col}\"\n",
    "                    result_df[new_name] = df[col]\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            result_df = result_df.sort_values('fecha')\n",
    "            \n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al cargar el resultado del script: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def procesar_archivo(self, config_row):\n",
    "        \"\"\"\n",
    "        Procesa un archivo para una configuración dada según los parámetros del archivo Excel\n",
    "        \"\"\"\n",
    "        variable = config_row['Variable']\n",
    "        tipo_macro = config_row['Tipo Macro']\n",
    "        target_col = config_row['TARGET']\n",
    "        tipo_preprocesamiento = config_row.get('Tipo de Preprocesamiento Según la Fuente', 'Normal')\n",
    "        \n",
    "        self.logger.info(f\"\\nProcesando: {variable} ({tipo_macro})\")\n",
    "        self.logger.info(f\"- Columna TARGET: {target_col}\")\n",
    "        self.logger.info(f\"- Tipo de preprocesamiento: {tipo_preprocesamiento}\")\n",
    "        \n",
    "        # Determinar el procesador específico basado en la variable\n",
    "        if variable == 'US_Empire_State_Index':\n",
    "            return self.procesar_empire_state_manualmente(variable, tipo_macro, target_col)\n",
    "        elif variable == 'AAII_Investor_Sentiment':\n",
    "            return self.procesar_aaii_investor_sentiment(variable, tipo_macro, target_col)\n",
    "        elif variable == 'Put_Call_Ratio_SPY':\n",
    "            return self.procesar_put_call_ratio(variable, tipo_macro, target_col) \n",
    "        elif variable == 'Chicago_Fed_NFCI':\n",
    "            return self.procesar_chicago_fed_manualmente(variable, tipo_macro, target_col)\n",
    "        else:\n",
    "            # Si no hay un método específico, intentamos un procesamiento genérico\n",
    "            self.logger.warning(f\"No existe procesador específico para {variable}, intentando procesamiento genérico\")\n",
    "            return self.procesar_archivo_generico(variable, tipo_macro, target_col)\n",
    "    \n",
    "    def generar_indice_diario(self):\n",
    "        \"\"\"\n",
    "        Genera un DataFrame con índice diario entre fechas mínima y máxima globales\n",
    "        \"\"\"\n",
    "        # Filtrar para usar solo DataFrames válidos (no None)\n",
    "        dfs_validos = {var: df for var, df in self.datos_procesados.items() if df is not None}\n",
    "        \n",
    "        if not dfs_validos:\n",
    "            self.logger.error(\"No hay datos procesados válidos para generar el índice diario\")\n",
    "            return None\n",
    "            \n",
    "        # Calcular fechas mínima y máxima considerando solo los DataFrames válidos\n",
    "        self.fecha_min_global = None\n",
    "        self.fecha_max_global = None\n",
    "        \n",
    "        for variable, df in dfs_validos.items():\n",
    "            fecha_min = df['fecha'].min()\n",
    "            fecha_max = df['fecha'].max()\n",
    "            \n",
    "            if self.fecha_min_global is None or fecha_min < self.fecha_min_global:\n",
    "                self.fecha_min_global = fecha_min\n",
    "                \n",
    "            if self.fecha_max_global is None or fecha_max > self.fecha_max_global:\n",
    "                self.fecha_max_global = fecha_max\n",
    "        \n",
    "        self.logger.info(\"\\nGenerando índice temporal diario...\")\n",
    "        self.logger.info(f\"- Rango de fechas global: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        \n",
    "        # Generar todas las fechas diarias\n",
    "        todas_fechas = pd.date_range(start=self.fecha_min_global, end=self.fecha_max_global, freq='D')\n",
    "        self.indice_diario = pd.DataFrame({'fecha': todas_fechas})\n",
    "        \n",
    "        self.logger.info(f\"- Total de fechas diarias generadas: {len(self.indice_diario)}\")\n",
    "        return self.indice_diario\n",
    "    \n",
    "    def combinar_datos(self):\n",
    "        \"\"\"\n",
    "        Combina todos los indicadores procesados con el índice diario\n",
    "        \"\"\"\n",
    "        # Filtrar para usar solo DataFrames válidos (no None)\n",
    "        dfs_validos = {var: df for var, df in self.datos_procesados.items() if df is not None}\n",
    "        \n",
    "        if not dfs_validos:\n",
    "            self.logger.error(\"No hay datos procesados válidos para combinar\")\n",
    "            return None\n",
    "            \n",
    "        if self.indice_diario is None:\n",
    "            self.logger.error(\"No se ha generado el índice diario\")\n",
    "            return None\n",
    "            \n",
    "        self.logger.info(\"\\nCombinando datos con índice diario...\")\n",
    "        \n",
    "        # Comenzar con el índice diario\n",
    "        df_combinado = self.indice_diario.copy()\n",
    "        \n",
    "        # Para cada indicador, realizar el merge y aplicar ffill\n",
    "        for variable, df in dfs_validos.items():\n",
    "            self.logger.info(f\"- Combinando: {variable}\")\n",
    "            \n",
    "            # Normalizar la columna de fecha\n",
    "            df['fecha'] = pd.to_datetime(df['fecha']).dt.normalize()\n",
    "            \n",
    "            # Realizar merge con el índice diario\n",
    "            df_combinado = pd.merge(df_combinado, df, on='fecha', how='left')\n",
    "            \n",
    "            # Aplicar forward fill (ffill) para todas las columnas de datos\n",
    "            for col in df.columns:\n",
    "                if col != 'fecha':\n",
    "                    df_combinado[col] = df_combinado[col].ffill()\n",
    "        \n",
    "        self.df_combinado = df_combinado\n",
    "        self.logger.info(f\"- DataFrame combinado: {len(df_combinado)} filas, {len(df_combinado.columns)} columnas\")\n",
    "        \n",
    "        return self.df_combinado\n",
    "    \n",
    "    def analizar_cobertura_final(self):\n",
    "        \"\"\"\n",
    "        Genera un informe detallado de cobertura final\n",
    "        \"\"\"\n",
    "        # Filtrar para usar solo estadísticas de DataFrames válidos (no None)\n",
    "        stats_validos = {var: stats for var, stats in self.estadisticas.items() \n",
    "                        if var in self.datos_procesados and self.datos_procesados[var] is not None}\n",
    "        \n",
    "        if not stats_validos or self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos combinados o estadísticas para analizar\")\n",
    "            return\n",
    "            \n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE COBERTURA FINAL\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        \n",
    "        total_indicadores = len(stats_validos)\n",
    "        total_dias = len(self.indice_diario)\n",
    "        \n",
    "        self.logger.info(f\"Total indicadores procesados: {total_indicadores}\")\n",
    "        self.logger.info(f\"Rango de fechas: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Total días en la serie: {total_dias}\")\n",
    "        self.logger.info(\"\\nCobertura por indicador:\")\n",
    "        \n",
    "        for variable, stats in stats_validos.items():\n",
    "            # Calcular cobertura final después del ffill\n",
    "            valores_finales = 0\n",
    "            columnas_var = 0\n",
    "            for col in self.df_combinado.columns:\n",
    "                if col != 'fecha' and variable in col:\n",
    "                    valores_finales += self.df_combinado[col].notna().sum()\n",
    "                    columnas_var += 1\n",
    "            \n",
    "            if columnas_var > 0:\n",
    "                cobertura_final = (valores_finales / (total_dias * columnas_var)) * 100\n",
    "            else:\n",
    "                cobertura_final = 0\n",
    "            \n",
    "            # Actualizar estadísticas\n",
    "            self.estadisticas[variable]['cobertura_final'] = cobertura_final\n",
    "            \n",
    "            self.logger.info(f\"- {variable}: {cobertura_final:.2f}%\")\n",
    "    \n",
    "    def generar_estadisticas_valores(self):\n",
    "        \"\"\"\n",
    "        Genera estadísticas descriptivas de los valores para cada indicador\n",
    "        \"\"\"\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos combinados para analizar\")\n",
    "            return\n",
    "            \n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"ESTADÍSTICAS DE VALORES\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        \n",
    "        for col in self.df_combinado.columns:\n",
    "            if col == 'fecha':\n",
    "                continue\n",
    "                \n",
    "            serie = self.df_combinado[col].dropna()\n",
    "            \n",
    "            if len(serie) == 0:\n",
    "                self.logger.warning(f\"La columna {col} no tiene valores\")\n",
    "                continue\n",
    "            \n",
    "            # Calcular estadísticas básicas\n",
    "            stats = {\n",
    "                'min': serie.min(),\n",
    "                'max': serie.max(),\n",
    "                'mean': serie.mean(),\n",
    "                'median': serie.median(),\n",
    "                'std': serie.std()\n",
    "            }\n",
    "            \n",
    "            self.logger.info(f\"\\nEstadísticas para {col}:\")\n",
    "            self.logger.info(f\"- Min: {stats['min']}\")\n",
    "            self.logger.info(f\"- Max: {stats['max']}\")\n",
    "            self.logger.info(f\"- Media: {stats['mean']}\")\n",
    "            self.logger.info(f\"- Mediana: {stats['median']}\")\n",
    "            self.logger.info(f\"- Desv. Estándar: {stats['std']}\")\n",
    "            \n",
    "            # Guardar estadísticas en el diccionario\n",
    "            variable = next((var for var in self.estadisticas.keys() if var in col), None)\n",
    "            if variable:\n",
    "                # Añadir estadísticas para esta columna\n",
    "                col_stats = {f\"{col}_min\": stats['min'],\n",
    "                           f\"{col}_max\": stats['max'],\n",
    "                           f\"{col}_mean\": stats['mean'],\n",
    "                           f\"{col}_median\": stats['median'],\n",
    "                           f\"{col}_std\": stats['std']}\n",
    "                self.estadisticas[variable].update(col_stats)\n",
    "    \n",
    "    def guardar_resultados(self, output_file='datos_economicos_other_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Guarda los resultados procesados en un archivo Excel\n",
    "        \"\"\"\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos para guardar\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "            self.logger.info(\"GUARDANDO RESULTADOS\")\n",
    "            self.logger.info(\"=\" * 50)\n",
    "            \n",
    "            self.logger.info(f\"Guardando resultados en: {output_file}\")\n",
    "            \n",
    "            # Crear un writer de Excel\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                # Guardar el DataFrame combinado en la primera hoja\n",
    "                self.logger.info(f\"Guardando DataFrame combinado en '{output_file}'...\")\n",
    "                self.df_combinado.to_excel(writer, sheet_name='Datos_Combinados', index=False)\n",
    "                \n",
    "                # Guardar estadísticas en una segunda hoja\n",
    "                self.logger.info(\"Guardando estadísticas de los indicadores...\")\n",
    "                \n",
    "                # Convertir diccionario de estadísticas a DataFrame\n",
    "                stats_data = []\n",
    "                for variable, stats in self.estadisticas.items():\n",
    "                    row = {\n",
    "                        'Variable': variable,\n",
    "                        'Tipo_Macro': stats.get('tipo_macro', ''),\n",
    "                        'Columna_TARGET': stats.get('columna_target', ''),\n",
    "                        'Total_Filas_Original': stats.get('total_filas', 0),\n",
    "                        'Valores_Validos_Original': stats.get('valores_validos', 0),\n",
    "                        'Cobertura_Final_%': stats.get('cobertura_final', 0),\n",
    "                        'Fecha_Min': stats.get('fecha_min', ''),\n",
    "                        'Fecha_Max': stats.get('fecha_max', '')\n",
    "                    }\n",
    "                    # Añadir estadísticas específicas por columna si existen\n",
    "                    for k, v in stats.items():\n",
    "                        if any(k.startswith(f\"{col}_\") for col in ['min', 'max', 'mean', 'median', 'std']):\n",
    "                            row[k] = v\n",
    "                    stats_data.append(row)\n",
    "                    \n",
    "                df_stats = pd.DataFrame(stats_data)\n",
    "                df_stats.to_excel(writer, sheet_name='Estadisticas', index=False)\n",
    "                \n",
    "                # Guardar metadatos\n",
    "                metadata = {\n",
    "                    'Proceso': ['OtherDataProcessor'],\n",
    "                    'Fecha de proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                    'Total indicadores': [len(self.estadisticas)],\n",
    "                    'Periodo': [f\"{self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\"],\n",
    "                    'Total días': [len(self.indice_diario)]\n",
    "                }\n",
    "                pd.DataFrame(metadata).to_excel(writer, sheet_name='Metadatos')\n",
    "                \n",
    "                # Guardar la configuración utilizada\n",
    "                if self.config_data is not None:\n",
    "                    self.config_data.to_excel(writer, sheet_name='Configuración', index=False)\n",
    "            \n",
    "            self.logger.info(f\"Archivo guardado exitosamente: {output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al guardar resultados: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ejecutar_proceso_completo(self, output_file='datos_economicos_other_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Ejecuta el proceso completo de preprocesamiento de forma paramétrica basada en Data Engineering.xlsx\n",
    "        \"\"\"\n",
    "        inicio = time.time()\n",
    "        self.logger.info(\"Iniciando proceso completo OtherDataProcessor...\")\n",
    "        \n",
    "        # 1. Leer configuración\n",
    "        self.leer_configuracion()\n",
    "        if self.config_data is None or len(self.config_data) == 0:\n",
    "            return False\n",
    "        \n",
    "        # Mostrar resumen de configuraciones encontradas\n",
    "        self.logger.info(\"\\nResumen de configuraciones a procesar:\")\n",
    "        for idx, row in self.config_data.iterrows():\n",
    "            self.logger.info(f\"- {row['Variable']} (Tipo Macro: {row['Tipo Macro']}, TARGET: {row['TARGET']})\")\n",
    "        \n",
    "        # 2. Procesar cada archivo\n",
    "        for _, config_row in self.config_data.iterrows():\n",
    "            variable, df_procesado = self.procesar_archivo(config_row)\n",
    "            self.datos_procesados[variable] = df_procesado\n",
    "        \n",
    "        # Verificar si se procesó al menos un archivo correctamente\n",
    "        archivos_correctos = sum(1 for df in self.datos_procesados.values() if df is not None)\n",
    "        if archivos_correctos == 0:\n",
    "            self.logger.error(\"No se pudo procesar correctamente ningún archivo\")\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(f\"\\nArchivos procesados correctamente: {archivos_correctos}/{len(self.datos_procesados)}\")\n",
    "        \n",
    "        # 3. Generar índice diario\n",
    "        self.generar_indice_diario()\n",
    "        if self.indice_diario is None:\n",
    "            return False\n",
    "        \n",
    "        # 4. Combinar datos\n",
    "        self.combinar_datos()\n",
    "        if self.df_combinado is None:\n",
    "            return False\n",
    "        \n",
    "        # 5. Analizar cobertura final\n",
    "        self.analizar_cobertura_final()\n",
    "        \n",
    "        # 6. Generar estadísticas de valores\n",
    "        self.generar_estadisticas_valores()\n",
    "        \n",
    "        # 7. Guardar resultados\n",
    "        resultado = self.guardar_resultados(output_file)\n",
    "        \n",
    "        # 8. Mostrar resumen final\n",
    "        fin = time.time()\n",
    "        tiempo_ejecucion = fin - inicio\n",
    "        \n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE EJECUCIÓN\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        self.logger.info(f\"Proceso: OtherDataProcessor\")\n",
    "        self.logger.info(f\"Tiempo de ejecución: {tiempo_ejecucion:.2f} segundos\")\n",
    "        self.logger.info(f\"Archivos procesados: {len(self.datos_procesados)}\")\n",
    "        self.logger.info(f\"Archivos con error: {sum(1 for df in self.datos_procesados.values() if df is None)}\")\n",
    "        self.logger.info(f\"Archivos procesados correctamente: {archivos_correctos}\")\n",
    "        self.logger.info(f\"Periodo de datos: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Datos combinados: {len(self.df_combinado)} filas, {len(self.df_combinado.columns)} columnas\")\n",
    "        self.logger.info(f\"Archivo de salida: {output_file}\")\n",
    "        self.logger.info(f\"Estado: {'COMPLETADO' if resultado else 'ERROR'}\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        \n",
    "        return resultado\n",
    "\n",
    "\n",
    "# Función principal para ejecutar el proceso\n",
    "def ejecutar_otherdataprocessor(config_file='Data Engineering.xlsx',\n",
    "                                output_file='datos_economicos_other_procesados.xlsx',\n",
    "                                data_root='data/Macro/raw',\n",
    "                                log_file='otherdataprocessor.log'):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso OtherDataProcessor\n",
    "    \n",
    "    Args:\n",
    "        config_file (str): Ruta al archivo de configuración\n",
    "        output_file (str): Ruta al archivo de salida\n",
    "        data_root (str): Directorio raíz donde se encuentran los subdirectorios de datos\n",
    "        log_file (str): Ruta al archivo de log\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si el proceso se completó exitosamente, False en caso contrario\n",
    "    \"\"\"\n",
    "    procesador = OtherDataProcessor(config_file, data_root, log_file)\n",
    "    return procesador.ejecutar_proceso_completo(output_file)\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    resultado = ejecutar_otherdataprocessor()\n",
    "    print(f\"Proceso {'completado exitosamente' if resultado else 'finalizado con errores'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 11:49:12,707 [INFO] ================================================================================\n",
      "2025-03-31 11:49:12,707 [INFO] INICIANDO PROCESO: OtherDataProcessor\n",
      "2025-03-31 11:49:12,708 [INFO] Archivo de configuración: Data Engineering.xlsx\n",
      "2025-03-31 11:49:12,708 [INFO] Directorio raíz de datos: data/Macro/raw\n",
      "2025-03-31 11:49:12,710 [INFO] Fecha y hora: 2025-03-31 11:49:12\n",
      "2025-03-31 11:49:12,710 [INFO] ================================================================================\n",
      "2025-03-31 11:49:12,710 [INFO] Iniciando proceso completo OtherDataProcessor...\n",
      "2025-03-31 11:49:12,711 [INFO] Leyendo archivo de configuración...\n",
      "2025-03-31 11:49:12,736 [INFO] Se encontraron 4 configuraciones para procesar con fuente 'Other'\n",
      "2025-03-31 11:49:12,736 [INFO] \n",
      "Resumen de configuraciones a procesar:\n",
      "2025-03-31 11:49:12,737 [INFO] - US_Empire_State_Index (Tipo Macro: business_confidence, TARGET: nan)\n",
      "2025-03-31 11:49:12,739 [INFO] - AAII_Investor_Sentiment (Tipo Macro: consumer_confidence, TARGET: nan)\n",
      "2025-03-31 11:49:12,740 [INFO] - Put_Call_Ratio_SPY (Tipo Macro: consumer_confidence, TARGET: nan)\n",
      "2025-03-31 11:49:12,742 [INFO] - Chicago_Fed_NFCI (Tipo Macro: leading_economic_index, TARGET: ANFCI)\n",
      "2025-03-31 11:49:12,742 [INFO] \n",
      "Procesando: US_Empire_State_Index (business_confidence)\n",
      "2025-03-31 11:49:12,744 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:49:12,744 [INFO] - Tipo de preprocesamiento: Normal\n",
      "2025-03-31 11:49:12,745 [INFO] Procesando US_Empire_State_Index manualmente\n",
      "2025-03-31 11:49:12,745 [INFO] - Archivo de entrada: data/Macro/raw\\business_confidence\\US_Empire_State_Index.csv\n",
      "2025-03-31 11:49:12,748 [INFO] - Archivo cargado: 285 filas, 24 columnas\n",
      "2025-03-31 11:49:12,751 [INFO] - Rango de fechas: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:49:12,751 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:49:12,753 [INFO] - Columnas encontradas y procesadas: GACDISA, AWCDISA\n",
      "2025-03-31 11:49:12,756 [INFO] - Columna ESI_GACDISA_US_Empire_State_Index_business_confidence: 285 valores no nulos (100.00%), rango: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:49:12,757 [INFO] - Columna ESI_AWCDISA_US_Empire_State_Index_business_confidence: 285 valores no nulos (100.00%), rango: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:49:12,757 [INFO] - US_Empire_State_Index: 285 filas procesadas manualmente, periodo: 2001-07-31 a 2025-03-31\n",
      "2025-03-31 11:49:12,759 [INFO] \n",
      "Procesando: AAII_Investor_Sentiment (consumer_confidence)\n",
      "2025-03-31 11:49:12,759 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:49:12,759 [INFO] - Tipo de preprocesamiento: nan\n",
      "2025-03-31 11:49:12,760 [INFO] Procesando AAII_Investor_Sentiment manualmente\n",
      "2025-03-31 11:49:12,760 [INFO] - Archivo de entrada: data/Macro/raw\\consumer_confidence\\AAII_Investor_Sentiment.xlsx\n",
      "2025-03-31 11:49:12,762 [INFO] Intentando leer Excel con engine: openpyxl\n",
      "2025-03-31 11:49:12,927 [INFO] - Archivo cargado: 1966 filas, 13 columnas\n",
      "2025-03-31 11:49:12,929 [INFO] - Columnas disponibles: Reported Date, Bullish, Neutral, Bearish, Total, Bullish 8-week Mov Avg, Bull-Bear Spread, Bullish Average, Bullish Average +St. Dev.,  Bullish Average - St. Dev., S&P 500 Weekly High, S&P 500 Weekly Low, S&P 500 Weekly Close\n",
      "2025-03-31 11:49:12,930 [INFO] - Usando columna 'Reported Date' como fecha\n",
      "2025-03-31 11:49:12,930 [INFO] - Columnas encontradas: Bearish, Bull-Bear Spread, Bullish\n",
      "2025-03-31 11:49:12,936 [INFO] - Rango de fechas total: 1987-06-26 a 2025-03-20\n",
      "2025-03-31 11:49:12,938 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:49:12,941 [INFO] - Columna AAII_Bearish_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:49:12,942 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:49:12,945 [INFO] - Columna AAII_Bull-Bear Spread_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:49:12,947 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:49:12,950 [INFO] - Columna AAII_Bullish_AAII_Investor_Sentiment_consumer_confidence: Rango de fechas 1987-07-24 a 2025-03-20\n",
      "2025-03-31 11:49:12,950 [INFO] - Valores disponibles: 1963/1966 (99.85%)\n",
      "2025-03-31 11:49:12,951 [INFO] - AAII_Investor_Sentiment: 1966 filas procesadas manualmente, periodo: 1987-06-26 a 2025-03-20\n",
      "2025-03-31 11:49:12,953 [INFO] \n",
      "Procesando: Put_Call_Ratio_SPY (consumer_confidence)\n",
      "2025-03-31 11:49:12,953 [INFO] - Columna TARGET: nan\n",
      "2025-03-31 11:49:12,954 [INFO] - Tipo de preprocesamiento: nan\n",
      "2025-03-31 11:49:12,954 [INFO] Procesando Put_Call_Ratio_SPY manualmente\n",
      "2025-03-31 11:49:12,956 [INFO] - Archivo de entrada: data/Macro/raw\\consumer_confidence\\Put_Call_Ratio_SPY.csv\n",
      "2025-03-31 11:49:13,091 [INFO] - Archivo cargado: 114936 filas, 13 columnas\n",
      "2025-03-31 11:49:13,091 [INFO] - Columnas disponibles: date, act_symbol, expiration, strike, call_put, bid, ask, vol, delta, gamma, theta, vega, rho\n",
      "2025-03-31 11:49:13,093 [INFO] - Usando columna 'date' como fecha/hora\n",
      "2025-03-31 11:49:13,117 [WARNING] - No se encontraron columnas 'put_volume' y 'call_volume' estándar. Se aplicará fallback usando columnas numéricas.\n",
      "2025-03-31 11:49:13,142 [INFO] - Put_Call_Ratio_SPY: 895 filas tras agrupar fallback, periodo: 2019-02-09 a 2025-03-25\n",
      "2025-03-31 11:49:13,145 [INFO] \n",
      "Procesando: Chicago_Fed_NFCI (leading_economic_index)\n",
      "2025-03-31 11:49:13,146 [INFO] - Columna TARGET: ANFCI\n",
      "2025-03-31 11:49:13,148 [INFO] - Tipo de preprocesamiento: Normal\n",
      "2025-03-31 11:49:13,148 [INFO] Procesando Chicago_Fed_NFCI manualmente\n",
      "2025-03-31 11:49:13,148 [INFO] - Archivo de entrada: data/Macro/raw\\leading_economic_index\\Chicago_Fed_NFCI.csv\n",
      "2025-03-31 11:49:13,158 [INFO] - Archivo cargado: 2829 filas, 7 columnas\n",
      "2025-03-31 11:49:13,158 [INFO] - Columnas disponibles: Friday_of_Week, NFCI, ANFCI, Risk, Credit, Leverage, Nonfinancial_Leverage\n",
      "2025-03-31 11:49:13,164 [INFO] - Usando columna 'Friday_of_Week' como fecha\n",
      "2025-03-31 11:49:13,167 [INFO] - Rango de fechas total: 1971-01-08 a 2025-03-21\n",
      "2025-03-31 11:49:13,169 [INFO] - ¿Tiene datos desde 2014 o antes?: Sí\n",
      "2025-03-31 11:49:13,170 [INFO] - Filtrando desde 2014-01-01: 586/2829 filas (20.71%)\n",
      "2025-03-31 11:49:13,175 [INFO] - Columna NFCI: Rango de fechas 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:49:13,178 [INFO] - Columna ANFCI: Rango de fechas 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:49:13,178 [INFO] - Columnas procesadas: NFCI, ANFCI\n",
      "2025-03-31 11:49:13,181 [INFO] - Chicago_Fed_NFCI: 586 filas procesadas manualmente, periodo: 2014-01-03 a 2025-03-21\n",
      "2025-03-31 11:49:13,183 [INFO] \n",
      "Archivos procesados correctamente: 4/4\n",
      "2025-03-31 11:49:13,183 [INFO] \n",
      "Generando índice temporal diario...\n",
      "2025-03-31 11:49:13,184 [INFO] - Rango de fechas global: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:49:13,186 [INFO] - Total de fechas diarias generadas: 13794\n",
      "2025-03-31 11:49:13,187 [INFO] \n",
      "Combinando datos con índice diario...\n",
      "2025-03-31 11:49:13,189 [INFO] - Combinando: US_Empire_State_Index\n",
      "2025-03-31 11:49:13,193 [INFO] - Combinando: AAII_Investor_Sentiment\n",
      "2025-03-31 11:49:13,204 [INFO] - Combinando: Put_Call_Ratio_SPY\n",
      "2025-03-31 11:49:13,220 [INFO] - Combinando: Chicago_Fed_NFCI\n",
      "2025-03-31 11:49:13,227 [INFO] - DataFrame combinado: 13794 filas, 17 columnas\n",
      "2025-03-31 11:49:13,227 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:49:13,229 [INFO] RESUMEN DE COBERTURA FINAL\n",
      "2025-03-31 11:49:13,229 [INFO] ==================================================\n",
      "2025-03-31 11:49:13,229 [INFO] Total indicadores procesados: 3\n",
      "2025-03-31 11:49:13,231 [INFO] Rango de fechas: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:49:13,231 [INFO] Total días en la serie: 13794\n",
      "2025-03-31 11:49:13,232 [INFO] \n",
      "Cobertura por indicador:\n",
      "2025-03-31 11:49:13,232 [INFO] - US_Empire_State_Index: 62.67%\n",
      "2025-03-31 11:49:13,234 [INFO] - AAII_Investor_Sentiment: 99.80%\n",
      "2025-03-31 11:49:13,235 [INFO] - Chicago_Fed_NFCI: 29.77%\n",
      "2025-03-31 11:49:13,235 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:49:13,238 [INFO] ESTADÍSTICAS DE VALORES\n",
      "2025-03-31 11:49:13,240 [INFO] ==================================================\n",
      "2025-03-31 11:49:13,243 [INFO] \n",
      "Estadísticas para ESI_GACDISA_US_Empire_State_Index_business_confidence:\n",
      "2025-03-31 11:49:13,244 [INFO] - Min: -79.9\n",
      "2025-03-31 11:49:13,244 [INFO] - Max: 39.0\n",
      "2025-03-31 11:49:13,246 [INFO] - Media: 6.604580682475419\n",
      "2025-03-31 11:49:13,248 [INFO] - Mediana: 7.8\n",
      "2025-03-31 11:49:13,249 [INFO] - Desv. Estándar: 15.716616407659977\n",
      "2025-03-31 11:49:13,252 [INFO] \n",
      "Estadísticas para ESI_AWCDISA_US_Empire_State_Index_business_confidence:\n",
      "2025-03-31 11:49:13,252 [INFO] - Min: -63.2\n",
      "2025-03-31 11:49:13,254 [INFO] - Max: 33.5\n",
      "2025-03-31 11:49:13,255 [INFO] - Media: 1.2108733371891272\n",
      "2025-03-31 11:49:13,257 [INFO] - Mediana: 1.3\n",
      "2025-03-31 11:49:13,257 [INFO] - Desv. Estándar: 10.23365502980844\n",
      "2025-03-31 11:49:13,258 [INFO] \n",
      "Estadísticas para AAII_Bearish_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:49:13,260 [INFO] - Min: 0.06\n",
      "2025-03-31 11:49:13,261 [INFO] - Max: 0.7027\n",
      "2025-03-31 11:49:13,263 [INFO] - Media: 0.3105507494551794\n",
      "2025-03-31 11:49:13,263 [INFO] - Mediana: 0.2988\n",
      "2025-03-31 11:49:13,264 [INFO] - Desv. Estándar: 0.09745327291339351\n",
      "2025-03-31 11:49:13,267 [INFO] \n",
      "Estadísticas para AAII_Bull-Bear Spread_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:49:13,269 [INFO] - Min: -0.54\n",
      "2025-03-31 11:49:13,269 [INFO] - Max: 0.6286\n",
      "2025-03-31 11:49:13,271 [INFO] - Media: 0.06615073056806625\n",
      "2025-03-31 11:49:13,272 [INFO] - Mediana: 0.07468750000000002\n",
      "2025-03-31 11:49:13,274 [INFO] - Desv. Estándar: 0.18114104664392988\n",
      "2025-03-31 11:49:13,275 [INFO] \n",
      "Estadísticas para AAII_Bullish_AAII_Investor_Sentiment_consumer_confidence:\n",
      "2025-03-31 11:49:13,277 [INFO] - Min: 0.12\n",
      "2025-03-31 11:49:13,277 [INFO] - Max: 0.75\n",
      "2025-03-31 11:49:13,278 [INFO] - Media: 0.3767014800232457\n",
      "2025-03-31 11:49:13,278 [INFO] - Mediana: 0.373016\n",
      "2025-03-31 11:49:13,278 [INFO] - Desv. Estándar: 0.10098032765975683\n",
      "2025-03-31 11:49:13,281 [INFO] \n",
      "Estadísticas para PutCall_strike_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,283 [INFO] - Min: 223.02564102564102\n",
      "2025-03-31 11:49:13,283 [INFO] - Max: 612.6603773584906\n",
      "2025-03-31 11:49:13,284 [INFO] - Media: 404.668114090602\n",
      "2025-03-31 11:49:13,286 [INFO] - Mediana: 405.0263157894737\n",
      "2025-03-31 11:49:13,286 [INFO] - Desv. Estándar: 87.93213244314363\n",
      "2025-03-31 11:49:13,289 [INFO] \n",
      "Estadísticas para PutCall_bid_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,289 [INFO] - Min: 9.744833333333334\n",
      "2025-03-31 11:49:13,290 [INFO] - Max: 38.27791666666667\n",
      "2025-03-31 11:49:13,290 [INFO] - Media: 26.29920874911389\n",
      "2025-03-31 11:49:13,292 [INFO] - Mediana: 28.854599999999998\n",
      "2025-03-31 11:49:13,292 [INFO] - Desv. Estándar: 7.413850149086833\n",
      "2025-03-31 11:49:13,294 [INFO] \n",
      "Estadísticas para PutCall_ask_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,294 [INFO] - Min: 9.886\n",
      "2025-03-31 11:49:13,295 [INFO] - Max: 38.78576388888889\n",
      "2025-03-31 11:49:13,295 [INFO] - Media: 26.60199191275792\n",
      "2025-03-31 11:49:13,297 [INFO] - Mediana: 29.264769230769232\n",
      "2025-03-31 11:49:13,297 [INFO] - Desv. Estándar: 7.48820818076355\n",
      "2025-03-31 11:49:13,300 [INFO] \n",
      "Estadísticas para PutCall_vol_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,300 [INFO] - Min: 0.10756969696969697\n",
      "2025-03-31 11:49:13,301 [INFO] - Max: 1.0840395833333334\n",
      "2025-03-31 11:49:13,301 [INFO] - Media: 0.26006733207554783\n",
      "2025-03-31 11:49:13,301 [INFO] - Mediana: 0.24875655737704916\n",
      "2025-03-31 11:49:13,303 [INFO] - Desv. Estándar: 0.08703340900993432\n",
      "2025-03-31 11:49:13,304 [INFO] \n",
      "Estadísticas para PutCall_delta_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,304 [INFO] - Min: -0.1892878205128205\n",
      "2025-03-31 11:49:13,304 [INFO] - Max: 0.13888653846153848\n",
      "2025-03-31 11:49:13,306 [INFO] - Media: 0.016195936242762034\n",
      "2025-03-31 11:49:13,306 [INFO] - Mediana: 0.00973051948051948\n",
      "2025-03-31 11:49:13,307 [INFO] - Desv. Estándar: 0.038078736335686224\n",
      "2025-03-31 11:49:13,309 [INFO] \n",
      "Estadísticas para PutCall_gamma_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,309 [INFO] - Min: 0.00313972602739726\n",
      "2025-03-31 11:49:13,310 [INFO] - Max: 0.018022727272727274\n",
      "2025-03-31 11:49:13,313 [INFO] - Media: 0.0073453931604935095\n",
      "2025-03-31 11:49:13,315 [INFO] - Mediana: 0.005812328767123288\n",
      "2025-03-31 11:49:13,315 [INFO] - Desv. Estándar: 0.003714755536802568\n",
      "2025-03-31 11:49:13,316 [INFO] \n",
      "Estadísticas para PutCall_theta_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,318 [INFO] - Min: -0.7143673611111111\n",
      "2025-03-31 11:49:13,318 [INFO] - Max: -0.026554545454545452\n",
      "2025-03-31 11:49:13,320 [INFO] - Media: -0.06679269441911516\n",
      "2025-03-31 11:49:13,320 [INFO] - Mediana: -0.058112666666666674\n",
      "2025-03-31 11:49:13,321 [INFO] - Desv. Estándar: 0.04793361473370308\n",
      "2025-03-31 11:49:13,321 [INFO] \n",
      "Estadísticas para PutCall_vega_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,323 [INFO] - Min: 0.08615289855072464\n",
      "2025-03-31 11:49:13,324 [INFO] - Max: 0.35191842105263155\n",
      "2025-03-31 11:49:13,324 [INFO] - Media: 0.17390617048713555\n",
      "2025-03-31 11:49:13,324 [INFO] - Mediana: 0.1629635802469136\n",
      "2025-03-31 11:49:13,326 [INFO] - Desv. Estándar: 0.03985547326367032\n",
      "2025-03-31 11:49:13,326 [INFO] \n",
      "Estadísticas para PutCall_rho_Put_Call_Ratio_SPY_consumer_confidence:\n",
      "2025-03-31 11:49:13,327 [INFO] - Min: -0.08338653846153846\n",
      "2025-03-31 11:49:13,327 [INFO] - Max: 0.1444063829787234\n",
      "2025-03-31 11:49:13,327 [INFO] - Media: 0.03279143559682671\n",
      "2025-03-31 11:49:13,329 [INFO] - Mediana: 0.03569838709677419\n",
      "2025-03-31 11:49:13,329 [INFO] - Desv. Estándar: 0.04133325747106617\n",
      "2025-03-31 11:49:13,332 [INFO] \n",
      "Estadísticas para NFCI_Chicago_Fed_NFCI_leading_economic_index:\n",
      "2025-03-31 11:49:13,332 [INFO] - Min: -0.799666616089861\n",
      "2025-03-31 11:49:13,333 [INFO] - Max: 0.494555538914267\n",
      "2025-03-31 11:49:13,333 [INFO] - Media: -0.4805096337311241\n",
      "2025-03-31 11:49:13,333 [INFO] - Mediana: -0.52381758146561\n",
      "2025-03-31 11:49:13,335 [INFO] - Desv. Estándar: 0.18723518268942319\n",
      "2025-03-31 11:49:13,336 [INFO] \n",
      "Estadísticas para ANFCI_Chicago_Fed_NFCI_leading_economic_index:\n",
      "2025-03-31 11:49:13,336 [INFO] - Min: -0.908607104751089\n",
      "2025-03-31 11:49:13,338 [INFO] - Max: 0.633457159158181\n",
      "2025-03-31 11:49:13,338 [INFO] - Media: -0.47021921372465036\n",
      "2025-03-31 11:49:13,339 [INFO] - Mediana: -0.504665111888294\n",
      "2025-03-31 11:49:13,339 [INFO] - Desv. Estándar: 0.21287358745036755\n",
      "2025-03-31 11:49:13,341 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:49:13,341 [INFO] GUARDANDO RESULTADOS\n",
      "2025-03-31 11:49:13,341 [INFO] ==================================================\n",
      "2025-03-31 11:49:13,342 [INFO] Guardando resultados en: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:49:13,346 [INFO] Guardando DataFrame combinado en 'datos_economicos_other_procesados.xlsx'...\n",
      "2025-03-31 11:49:14,693 [INFO] Guardando estadísticas de los indicadores...\n",
      "2025-03-31 11:49:16,640 [INFO] Archivo guardado exitosamente: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:49:16,642 [INFO] \n",
      "==================================================\n",
      "2025-03-31 11:49:16,642 [INFO] RESUMEN DE EJECUCIÓN\n",
      "2025-03-31 11:49:16,643 [INFO] ==================================================\n",
      "2025-03-31 11:49:16,643 [INFO] Proceso: OtherDataProcessor\n",
      "2025-03-31 11:49:16,643 [INFO] Tiempo de ejecución: 3.93 segundos\n",
      "2025-03-31 11:49:16,645 [INFO] Archivos procesados: 4\n",
      "2025-03-31 11:49:16,645 [INFO] Archivos con error: 0\n",
      "2025-03-31 11:49:16,645 [INFO] Archivos procesados correctamente: 4\n",
      "2025-03-31 11:49:16,646 [INFO] Periodo de datos: 1987-06-26 a 2025-03-31\n",
      "2025-03-31 11:49:16,648 [INFO] Datos combinados: 13794 filas, 17 columnas\n",
      "2025-03-31 11:49:16,648 [INFO] Archivo de salida: datos_economicos_other_procesados.xlsx\n",
      "2025-03-31 11:49:16,649 [INFO] Estado: COMPLETADO\n",
      "2025-03-31 11:49:16,651 [INFO] ==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de logging\n",
    "def configurar_logging(log_file='otherdataprocessor.log'):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger('OtherDataProcessor')\n",
    "\n",
    "# Función para convertir fechas en diversos formatos\n",
    "def convertir_fecha(fecha_str):\n",
    "    \"\"\"\n",
    "    Convierte diversos formatos de fecha a pd.Timestamp.\n",
    "    \"\"\"\n",
    "    if isinstance(fecha_str, (pd.Timestamp, datetime)):\n",
    "        return pd.Timestamp(fecha_str)\n",
    "    \n",
    "    if pd.isna(fecha_str):\n",
    "        return None\n",
    "\n",
    "    if isinstance(fecha_str, str):\n",
    "        fecha_str = fecha_str.strip()\n",
    "        formatos = [\n",
    "            '%d.%m.%Y', '%d/%m/%Y', '%d-%m-%Y',\n",
    "            '%m/%d/%Y', '%Y-%m-%d',\n",
    "            '%Y%m%d', '%d%m%Y'\n",
    "        ]\n",
    "        for fmt in formatos:\n",
    "            try:\n",
    "                return pd.to_datetime(fecha_str, format=fmt)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            if re.search(r'([A-Za-z]+\\s+\\d+,\\s+\\d{4})', fecha_str):\n",
    "                match = re.search(r'([A-Za-z]+\\s+\\d+,\\s+\\d{4})', fecha_str)\n",
    "                return pd.to_datetime(match.group(1))\n",
    "            meses_es = {\n",
    "                'ene': 'Jan', 'feb': 'Feb', 'mar': 'Mar', 'abr': 'Apr',\n",
    "                'may': 'May', 'jun': 'Jun', 'jul': 'Jul', 'ago': 'Aug',\n",
    "                'sep': 'Sep', 'oct': 'Oct', 'nov': 'Nov', 'dic': 'Dec'\n",
    "            }\n",
    "            texto_procesado = fecha_str.lower()\n",
    "            for mes_es, mes_en in meses_es.items():\n",
    "                if mes_es in texto_procesado:\n",
    "                    texto_procesado = texto_procesado.replace(mes_es, mes_en)\n",
    "            return pd.to_datetime(texto_procesado)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(fecha_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class OtherDataProcessor:\n",
    "    \"\"\"\n",
    "    Clase para procesar datos de la fuente \"Other\" usando scripts personalizados.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_file, data_root='data/Macro/raw', log_file='otherdataprocessor.log'):\n",
    "        self.config_file = config_file\n",
    "        self.data_root = data_root\n",
    "        self.logger = configurar_logging(log_file)\n",
    "        self.config_data = None\n",
    "        self.fecha_min_global = None\n",
    "        self.fecha_max_global = None\n",
    "        self.indice_diario = None\n",
    "        self.datos_procesados = {}\n",
    "        self.df_combinado = None\n",
    "        self.estadisticas = {}\n",
    "        \n",
    "        # Mapeo de variables a rutas de archivos conocidas\n",
    "        self.data_paths = {\n",
    "            'US_Empire_State_Index': os.path.join('business_confidence', 'US_Empire_State_Index.csv'),\n",
    "            'AAII_Investor_Sentiment': os.path.join('consumer_confidence', 'AAII_Investor_Sentiment.xls'),\n",
    "            'Put_Call_Ratio_SPY': os.path.join('consumer_confidence', 'Put_Call_Ratio_SPY.csv'),\n",
    "            'Chicago_Fed_NFCI': os.path.join('leading_economic_index', 'Chicago_Fed_NFCI.csv')\n",
    "        }\n",
    "\n",
    "        self.logger.info(\"=\" * 80)\n",
    "        self.logger.info(\"INICIANDO PROCESO: OtherDataProcessor\")\n",
    "        self.logger.info(f\"Archivo de configuración: {config_file}\")\n",
    "        self.logger.info(f\"Directorio raíz de datos: {data_root}\")\n",
    "        self.logger.info(f\"Fecha y hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        self.logger.info(\"=\" * 80)\n",
    "    \n",
    "    def leer_configuracion(self):\n",
    "        \"\"\"\n",
    "        Lee y filtra la configuración del archivo Excel para la fuente \"Other\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Leyendo archivo de configuración...\")\n",
    "            df_config = pd.read_excel(self.config_file)\n",
    "            self.config_data = df_config[df_config['Fuente'] == 'Other'].copy()\n",
    "            num_configs = len(self.config_data)\n",
    "            self.logger.info(f\"Se encontraron {num_configs} configuraciones para procesar con fuente 'Other'\")\n",
    "            if num_configs == 0:\n",
    "                self.logger.warning(\"No se encontraron configuraciones que cumplan los criterios\")\n",
    "                return None\n",
    "            return self.config_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al leer configuración: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def encontrar_ruta_archivo(self, variable, tipo_macro):\n",
    "        \"\"\"\n",
    "        Encuentra la ruta completa del archivo basado en la variable y tipo_macro\n",
    "        \"\"\"\n",
    "        if variable in self.data_paths:\n",
    "            ruta_conocida = os.path.join(self.data_root, self.data_paths[variable])\n",
    "            if os.path.exists(ruta_conocida):\n",
    "                return ruta_conocida\n",
    "        ruta_base = os.path.join(self.data_root, tipo_macro)\n",
    "        for ext in ['.csv', '.xlsx', '.xls']:\n",
    "            nombre_archivo = f\"{variable}{ext}\"\n",
    "            ruta_completa = os.path.join(ruta_base, nombre_archivo)\n",
    "            if os.path.exists(ruta_completa):\n",
    "                return ruta_completa\n",
    "        for root, dirs, files in os.walk(self.data_root):\n",
    "            for file in files:\n",
    "                if file.startswith(variable) and file.endswith(('.csv', '.xlsx', '.xls')):\n",
    "                    return os.path.join(root, file)\n",
    "        return None\n",
    "    \n",
    "    def ejecutar_script(self, script_path, variable, output_path=None):\n",
    "        \"\"\"\n",
    "        Ejecuta un script de Python a través del sistema\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Ejecutando script: {script_path}\")\n",
    "            spec = importlib.util.spec_from_file_location(\"custom_script\", script_path)\n",
    "            module = importlib.util.module_from_spec(spec)\n",
    "            current_dir = os.getcwd()\n",
    "            script_dir = os.path.dirname(os.path.abspath(script_path))\n",
    "            os.chdir(script_dir)\n",
    "            sys.argv = ['']\n",
    "            try:\n",
    "                spec.loader.exec_module(module)\n",
    "                resultado = True\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error al ejecutar el script {script_path}: {str(e)}\")\n",
    "                resultado = False\n",
    "            os.chdir(current_dir)\n",
    "            return resultado\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al cargar el script {script_path}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def procesar_empire_state_manualmente(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Empire State Index manualmente cuando el script no está disponible\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        ruta_directa = os.path.join(self.data_root, 'business_confidence', f\"{variable}.csv\")\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            nuevo_df = pd.DataFrame()\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'survey', 'week']):\n",
    "                    fecha_col = col\n",
    "                    break\n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                fecha_col = df.columns[0]\n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "            if 'surveyDate' in df.columns:\n",
    "                fecha_col = 'surveyDate'\n",
    "            nuevo_df['fecha'] = pd.to_datetime(df[fecha_col])\n",
    "            fecha_min = nuevo_df['fecha'].min()\n",
    "            fecha_max = nuevo_df['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            fecha_2014 = pd.Timestamp('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            if fecha_min > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            if tipo_macro == 'business_confidence' and target_col == 'macro_business_confidence_fe_1month':\n",
    "                if 'GACDISA' in df.columns:\n",
    "                    nuevo_df[f'{target_col}_GACDISA_{variable}_{tipo_macro}'] = df['GACDISA']\n",
    "                    self.logger.info(f\"- Columna GACDISA encontrada y procesada\")\n",
    "                if 'AWCDISA' in df.columns:\n",
    "                    nuevo_df[f'{target_col}_AWCDISA_{variable}_{tipo_macro}'] = df['AWCDISA']\n",
    "                    self.logger.info(f\"- Columna AWCDISA encontrada y procesada\")\n",
    "                self.logger.info(f\"Procesamiento específico para {variable} con TARGET={target_col}\")\n",
    "            else:\n",
    "                columnas_indicador = ['GACDISA', 'AWCDISA', 'headline', 'main_index']\n",
    "                prefix = target_col if pd.notna(target_col) else 'ESI'\n",
    "                columnas_encontradas = []\n",
    "                for col in columnas_indicador:\n",
    "                    if col in df.columns:\n",
    "                        nuevo_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "                        columnas_encontradas.append(col)\n",
    "                if columnas_encontradas:\n",
    "                    self.logger.info(f\"- Columnas encontradas y procesadas: {', '.join(columnas_encontradas)}\")\n",
    "                if len(nuevo_df.columns) == 1:\n",
    "                    columnas_usadas = []\n",
    "                    for col in df.columns:\n",
    "                        if col != fecha_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                            nuevo_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "                            columnas_usadas.append(col)\n",
    "                    if columnas_usadas:\n",
    "                        self.logger.info(f\"- Se usaron columnas numéricas genéricas: {', '.join(columnas_usadas)}\")\n",
    "            nuevo_df = nuevo_df.sort_values('fecha')\n",
    "            for col in nuevo_df.columns:\n",
    "                if col != 'fecha':\n",
    "                    col_no_nulos = nuevo_df[col].notna().sum()\n",
    "                    pct_no_nulos = (col_no_nulos / len(nuevo_df)) * 100\n",
    "                    fecha_min_col = nuevo_df.loc[nuevo_df[col].notna(), 'fecha'].min()\n",
    "                    fecha_max_col = nuevo_df.loc[nuevo_df[col].notna(), 'fecha'].max()\n",
    "                    self.logger.info(f\"- Columna {col}: {col_no_nulos} valores no nulos ({pct_no_nulos:.2f}%), rango: {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "            fecha_min = nuevo_df['fecha'].min()\n",
    "            fecha_max = nuevo_df['fecha'].max()\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or 'ESI',\n",
    "                'total_filas': len(nuevo_df),\n",
    "                'valores_validos': len(nuevo_df),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            self.logger.info(f\"- {variable}: {len(nuevo_df)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            return variable, nuevo_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "    \n",
    "    def procesar_aaii_investor_sentiment(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de AAII Investor Sentiment manualmente\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        ruta_directa = os.path.join(self.data_root, 'consumer_confidence', f\"{variable}.xls\")\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        try:\n",
    "            df = None\n",
    "            engines = ['openpyxl', 'xlrd']\n",
    "            for engine in engines:\n",
    "                try:\n",
    "                    self.logger.info(f\"Intentando leer Excel con engine: {engine}\")\n",
    "                    df = pd.read_excel(input_file, engine=engine)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Error al leer con engine {engine}: {str(e)}\")\n",
    "            if df is None:\n",
    "                try:\n",
    "                    self.logger.info(\"Intentando leer como CSV...\")\n",
    "                    df = pd.read_csv(input_file)\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error al leer como CSV: {str(e)}\")\n",
    "                    return variable, None\n",
    "            if df is None or len(df) == 0:\n",
    "                self.logger.error(f\"No se pudo leer el archivo {input_file} con ningún método\")\n",
    "                return variable, None\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            columna_fecha = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'week', 'period']):\n",
    "                    columna_fecha = col\n",
    "                    self.logger.info(f\"- Usando columna '{col}' como fecha\")\n",
    "                    break\n",
    "            if columna_fecha is None:\n",
    "                columna_fecha = df.columns[0]\n",
    "                self.logger.info(f\"- Usando primera columna '{columna_fecha}' como fecha (fallback)\")\n",
    "            columnas_objetivo = ['Bearish', 'Bull-Bear Spread', 'Bullish']\n",
    "            columnas_encontradas = [col for col in columnas_objetivo if col in df.columns]\n",
    "            columnas_faltantes = [col for col in columnas_objetivo if col not in df.columns]\n",
    "            if columnas_encontradas:\n",
    "                self.logger.info(f\"- Columnas encontradas: {', '.join(columnas_encontradas)}\")\n",
    "            if columnas_faltantes:\n",
    "                self.logger.warning(f\"- Columnas no encontradas: {', '.join(columnas_faltantes)}\")\n",
    "            resultado = pd.DataFrame()\n",
    "            resultado['fecha'] = pd.to_datetime(df[columna_fecha])\n",
    "            fecha_min_total = resultado['fecha'].min()\n",
    "            fecha_max_total = resultado['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas total: {fecha_min_total.strftime('%Y-%m-%d')} a {fecha_max_total.strftime('%Y-%m-%d')}\")\n",
    "            fecha_2014 = pd.to_datetime('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min_total <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            if fecha_min_total > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min_total.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            prefix = target_col if pd.notna(target_col) else 'AAII'\n",
    "            for columna in columnas_encontradas:\n",
    "                nombre_col = f'{prefix}_{columna}_{variable}_{tipo_macro}'\n",
    "                resultado[nombre_col] = df[columna]\n",
    "                fechas_validas = resultado[resultado[nombre_col].notna()]\n",
    "                if len(fechas_validas) > 0:\n",
    "                    fecha_min_col = fechas_validas['fecha'].min()\n",
    "                    fecha_max_col = fechas_validas['fecha'].max()\n",
    "                    self.logger.info(f\"- Columna {nombre_col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                    self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(resultado)} ({len(fechas_validas)/len(resultado)*100:.2f}%)\")\n",
    "            if not columnas_encontradas:\n",
    "                self.logger.warning(f\"- No se encontraron columnas específicas. Usando columnas numéricas disponibles.\")\n",
    "                columnas_usadas = []\n",
    "                for col in df.columns:\n",
    "                    if col != columna_fecha and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                        nombre_col = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                        resultado[nombre_col] = df[col]\n",
    "                        columnas_usadas.append(col)\n",
    "                        fechas_validas = resultado[resultado[nombre_col].notna()]\n",
    "                        if len(fechas_validas) > 0:\n",
    "                            fecha_min_col = fechas_validas['fecha'].min()\n",
    "                            fecha_max_col = fechas_validas['fecha'].max()\n",
    "                            self.logger.info(f\"- Columna {nombre_col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "                            self.logger.info(f\"- Valores disponibles: {len(fechas_validas)}/{len(resultado)} ({len(fechas_validas)/len(resultado)*100:.2f}%)\")\n",
    "                if columnas_usadas:\n",
    "                    self.logger.info(f\"- Se utilizaron columnas numéricas alternativas: {', '.join(columnas_usadas)}\")\n",
    "            resultado = resultado.sort_values('fecha')\n",
    "            fecha_min = resultado['fecha'].min()\n",
    "            fecha_max = resultado['fecha'].max()\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or prefix,\n",
    "                'total_filas': len(resultado),\n",
    "                'valores_validos': len(resultado),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            self.logger.info(f\"- {variable}: {len(resultado)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            return variable, resultado\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "\n",
    "    def procesar_put_call_ratio(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Put/Call Ratio manualmente, agregando intradía a un solo valor diario.\n",
    "        Si se encuentran columnas para 'put_volume' y 'call_volume', se agrupan por día sumándolas\n",
    "        y se calcula el ratio. En caso contrario se utiliza un fallback agrupando las columnas numéricas\n",
    "        (mediante la media).\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        ruta_directa = os.path.join(self.data_root, 'consumer_confidence', f\"{variable}.csv\")\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            \n",
    "            result_df = pd.DataFrame()\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'time', 'periodo']):\n",
    "                    fecha_col = col\n",
    "                    self.logger.info(f\"- Usando columna '{col}' como fecha/hora\")\n",
    "                    break\n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                fecha_col = df.columns[0]\n",
    "                self.logger.info(f\"- Usando primera columna '{fecha_col}' como fecha (fallback)\")\n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "                \n",
    "            df['fecha'] = pd.to_datetime(df[fecha_col], errors='coerce')\n",
    "            df = df.dropna(subset=['fecha'])\n",
    "            df['fecha_dia'] = df['fecha'].dt.normalize()\n",
    "            \n",
    "            # Buscar columnas de volúmenes\n",
    "            col_put = None\n",
    "            col_call = None\n",
    "            posibles_put = ['put_volume', 'Put_Volume', 'volume_puts']\n",
    "            posibles_call = ['call_volume', 'Call_Volume', 'volume_calls']\n",
    "            for c in df.columns:\n",
    "                if c in posibles_put:\n",
    "                    col_put = c\n",
    "                if c in posibles_call:\n",
    "                    col_call = c\n",
    "            \n",
    "            if not col_put or not col_call:\n",
    "                self.logger.warning(\"- No se encontraron columnas 'put_volume' y 'call_volume' estándar. \"\n",
    "                                    \"Se aplicará fallback usando columnas numéricas.\")\n",
    "                result_df['fecha'] = df['fecha_dia']\n",
    "                for col in df.select_dtypes(include=[np.number]).columns:\n",
    "                    if col != 'fecha_dia':\n",
    "                        prefix = target_col if pd.notna(target_col) else 'PutCall'\n",
    "                        nuevo_nombre = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                        result_df[nuevo_nombre] = df[col]\n",
    "                result_df = result_df.groupby('fecha', as_index=False).mean()\n",
    "                result_df = result_df.sort_values('fecha')\n",
    "                fecha_min = result_df['fecha'].min()\n",
    "                fecha_max = result_df['fecha'].max()\n",
    "                self.logger.info(f\"- {variable}: {len(result_df)} filas tras agrupar fallback, periodo: {fecha_min.date()} a {fecha_max.date()}\")\n",
    "                return variable, result_df\n",
    "            else:\n",
    "                df_diario = df.groupby('fecha_dia', as_index=False).agg({\n",
    "                    col_put: 'sum',\n",
    "                    col_call: 'sum'\n",
    "                })\n",
    "                df_diario['put_call_ratio'] = df_diario[col_put] / df_diario[col_call].replace(0, np.nan)\n",
    "                df_diario.rename(columns={'fecha_dia': 'fecha'}, inplace=True)\n",
    "                col_name = f'consumer_confidence_PutCall_{variable}_{tipo_macro}'\n",
    "                df_diario[col_name] = df_diario['put_call_ratio']\n",
    "                df_diario = df_diario.sort_values('fecha')\n",
    "                fecha_min = df_diario['fecha'].min()\n",
    "                fecha_max = df_diario['fecha'].max()\n",
    "                self.logger.info(f\"- {variable}: {len(df_diario)} filas diarias tras agrupar, periodo: {fecha_min.date()} a {fecha_max.date()}\")\n",
    "                result_df = df_diario[['fecha', col_put, col_call, col_name]]\n",
    "                self.estadisticas[variable] = {\n",
    "                    'tipo_macro': tipo_macro,\n",
    "                    'columna_target': target_col or 'PutCall',\n",
    "                    'total_filas': len(result_df),\n",
    "                    'valores_validos': len(result_df),\n",
    "                    'fecha_min': fecha_min,\n",
    "                    'fecha_max': fecha_max\n",
    "                }\n",
    "                return variable, result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "            \n",
    "    def procesar_archivo_generico(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesamiento genérico para cualquier archivo de datos\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} de forma genérica\")\n",
    "        input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "        if input_file is None:\n",
    "            self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "            return variable, None\n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        try:\n",
    "            if input_file.endswith('.csv'):\n",
    "                df = pd.read_csv(input_file)\n",
    "            elif input_file.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(input_file)\n",
    "            else:\n",
    "                self.logger.error(f\"Formato de archivo no soportado: {input_file}\")\n",
    "                return variable, None\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(term in col.lower() for term in ['date', 'time', 'fecha', 'week', 'period']):\n",
    "                    fecha_col = col\n",
    "                    break\n",
    "            if fecha_col is None and len(df.columns) > 0:\n",
    "                try:\n",
    "                    pd.to_datetime(df.iloc[:, 0])\n",
    "                    fecha_col = df.columns[0]\n",
    "                except:\n",
    "                    self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                    return variable, None\n",
    "            result_df = pd.DataFrame()\n",
    "            result_df['fecha'] = pd.to_datetime(df[fecha_col])\n",
    "            result_df = result_df.dropna(subset=['fecha'])\n",
    "            prefix = target_col if pd.notna(target_col) else variable.split('_')[0]\n",
    "            for col in df.columns:\n",
    "                if col != fecha_col and pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    result_df[f'{prefix}_{col}_{variable}_{tipo_macro}'] = df[col]\n",
    "            result_df = result_df.sort_values('fecha')\n",
    "            fecha_min = result_df['fecha'].min()\n",
    "            fecha_max = result_df['fecha'].max()\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or prefix,\n",
    "                'total_filas': len(result_df),\n",
    "                'valores_validos': len(result_df),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            self.logger.info(f\"- {variable}: {len(result_df)} filas procesadas genéricamente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            return variable, result_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} genéricamente: {str(e)}\")\n",
    "            return variable, None\n",
    "            \n",
    "    def procesar_chicago_fed_manualmente(self, variable, tipo_macro, target_col):\n",
    "        \"\"\"\n",
    "        Procesa los datos de Chicago Fed NFCI manualmente cuando el script no está disponible\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Procesando {variable} manualmente\")\n",
    "        ruta_directa = os.path.join(self.data_root, 'leading_economic_index', f\"{variable}.csv\")\n",
    "        if not os.path.exists(ruta_directa):\n",
    "            input_file = self.encontrar_ruta_archivo(variable, tipo_macro)\n",
    "            if input_file is None:\n",
    "                self.logger.error(f\"Archivo de entrada no encontrado para {variable}\")\n",
    "                return variable, None\n",
    "        else:\n",
    "            input_file = ruta_directa\n",
    "        self.logger.info(f\"- Archivo de entrada: {input_file}\")\n",
    "        try:\n",
    "            df = pd.read_csv(input_file)\n",
    "            self.logger.info(f\"- Archivo cargado: {len(df)} filas, {len(df.columns)} columnas\")\n",
    "            self.logger.info(f\"- Columnas disponibles: {', '.join(df.columns.tolist())}\")\n",
    "            if 'Friday_of_Week' in df.columns:\n",
    "                df['fecha'] = pd.to_datetime(df['Friday_of_Week'], format='%m/%d/%Y')\n",
    "                self.logger.info(f\"- Usando columna 'Friday_of_Week' como fecha\")\n",
    "            else:\n",
    "                for col in df.columns:\n",
    "                    if any(term in col.lower() for term in ['date', 'week', 'period']):\n",
    "                        df['fecha'] = pd.to_datetime(df[col])\n",
    "                        self.logger.info(f\"- Usando columna '{col}' como fecha\")\n",
    "                        break\n",
    "            if 'fecha' not in df.columns:\n",
    "                self.logger.error(f\"No se pudo identificar columna de fecha en {input_file}\")\n",
    "                return variable, None\n",
    "            fecha_min_total = df['fecha'].min()\n",
    "            fecha_max_total = df['fecha'].max()\n",
    "            self.logger.info(f\"- Rango de fechas total: {fecha_min_total.strftime('%Y-%m-%d')} a {fecha_max_total.strftime('%Y-%m-%d')}\")\n",
    "            fecha_2014 = pd.to_datetime('2014-01-01')\n",
    "            tiene_desde_2014 = fecha_min_total <= fecha_2014\n",
    "            self.logger.info(f\"- ¿Tiene datos desde 2014 o antes?: {'Sí' if tiene_desde_2014 else 'No'}\")\n",
    "            if fecha_min_total > fecha_2014:\n",
    "                self.logger.warning(f\"- ATENCIÓN: Los datos comienzan en {fecha_min_total.strftime('%Y-%m-%d')}, después de 2014-01-01\")\n",
    "            df_filtrado = df[df['fecha'] >= fecha_2014]\n",
    "            self.logger.info(f\"- Filtrando desde 2014-01-01: {len(df_filtrado)}/{len(df)} filas ({(len(df_filtrado)/len(df)*100):.2f}%)\")\n",
    "            resultado = pd.DataFrame()\n",
    "            resultado['fecha'] = df_filtrado['fecha']\n",
    "            columnas_procesadas = []\n",
    "            if 'NFCI' in df_filtrado.columns:\n",
    "                resultado[f'NFCI_{variable}_{tipo_macro}'] = df_filtrado['NFCI']\n",
    "                columnas_procesadas.append('NFCI')\n",
    "                fecha_min_nfci = df_filtrado.loc[df_filtrado['NFCI'].notna(), 'fecha'].min()\n",
    "                fecha_max_nfci = df_filtrado.loc[df_filtrado['NFCI'].notna(), 'fecha'].max()\n",
    "                self.logger.info(f\"- Columna NFCI: Rango de fechas {fecha_min_nfci.strftime('%Y-%m-%d')} a {fecha_max_nfci.strftime('%Y-%m-%d')}\")\n",
    "            if 'ANFCI' in df_filtrado.columns:\n",
    "                resultado[f'ANFCI_{variable}_{tipo_macro}'] = df_filtrado['ANFCI']\n",
    "                columnas_procesadas.append('ANFCI')\n",
    "                fecha_min_anfci = df_filtrado.loc[df_filtrado['ANFCI'].notna(), 'fecha'].min()\n",
    "                fecha_max_anfci = df_filtrado.loc[df_filtrado['ANFCI'].notna(), 'fecha'].max()\n",
    "                self.logger.info(f\"- Columna ANFCI: Rango de fechas {fecha_min_anfci.strftime('%Y-%m-%d')} a {fecha_max_anfci.strftime('%Y-%m-%d')}\")\n",
    "            if len(columnas_procesadas) == 0:\n",
    "                self.logger.warning(f\"- No se encontraron columnas NFCI o ANFCI. Utilizando columnas numéricas disponibles\")\n",
    "                for col in df_filtrado.columns:\n",
    "                    if col != 'fecha' and pd.api.types.is_numeric_dtype(df_filtrado[col]):\n",
    "                        prefix = target_col if pd.notna(target_col) else 'NFCI'\n",
    "                        nuevo_nombre = f'{prefix}_{col}_{variable}_{tipo_macro}'\n",
    "                        resultado[nuevo_nombre] = df_filtrado[col]\n",
    "                        columnas_procesadas.append(col)\n",
    "                        fecha_min_col = df_filtrado.loc[df_filtrado[col].notna(), 'fecha'].min()\n",
    "                        fecha_max_col = df_filtrado.loc[df_filtrado[col].notna(), 'fecha'].max()\n",
    "                        self.logger.info(f\"- Columna {col}: Rango de fechas {fecha_min_col.strftime('%Y-%m-%d')} a {fecha_max_col.strftime('%Y-%m-%d')}\")\n",
    "            self.logger.info(f\"- Columnas procesadas: {', '.join(columnas_procesadas)}\")\n",
    "            resultado = resultado.sort_values('fecha')\n",
    "            fecha_min = resultado['fecha'].min()\n",
    "            fecha_max = resultado['fecha'].max()\n",
    "            self.estadisticas[variable] = {\n",
    "                'tipo_macro': tipo_macro,\n",
    "                'columna_target': target_col or 'NFCI',\n",
    "                'total_filas': len(resultado),\n",
    "                'valores_validos': len(resultado),\n",
    "                'fecha_min': fecha_min,\n",
    "                'fecha_max': fecha_max\n",
    "            }\n",
    "            self.logger.info(f\"- {variable}: {len(resultado)} filas procesadas manualmente, periodo: {fecha_min.strftime('%Y-%m-%d')} a {fecha_max.strftime('%Y-%m-%d')}\")\n",
    "            return variable, resultado\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al procesar {variable} manualmente: {str(e)}\")\n",
    "            return variable, None\n",
    "    \n",
    "    def cargar_resultado_script(self, output_file, variable, target_col):\n",
    "        \"\"\"\n",
    "        Carga el archivo de resultado generado por el script y lo estandariza\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Cargando resultado desde {output_file}\")\n",
    "            if not os.path.exists(output_file):\n",
    "                self.logger.error(f\"El archivo de salida {output_file} no existe\")\n",
    "                return None\n",
    "            if output_file.endswith('.csv'):\n",
    "                df = pd.read_csv(output_file)\n",
    "            elif output_file.endswith(('.xlsx', '.xls')):\n",
    "                df = pd.read_excel(output_file)\n",
    "            else:\n",
    "                self.logger.error(f\"Formato de archivo no soportado: {output_file}\")\n",
    "                return None\n",
    "            fecha_col = None\n",
    "            for col in df.columns:\n",
    "                if any(palabra in col.lower() for palabra in ['date', 'fecha', 'time', 'día']):\n",
    "                    fecha_col = col\n",
    "                    break\n",
    "            if fecha_col is None:\n",
    "                self.logger.error(f\"No se pudo identificar la columna de fecha en {output_file}\")\n",
    "                return None\n",
    "            result_df = pd.DataFrame()\n",
    "            result_df['fecha'] = df[fecha_col].apply(convertir_fecha)\n",
    "            result_df = result_df.dropna(subset=['fecha'])\n",
    "            for col in df.columns:\n",
    "                if col != fecha_col:\n",
    "                    new_name = f\"{target_col}_{variable}_{col}\"\n",
    "                    result_df[new_name] = df[col]\n",
    "            result_df = result_df.sort_values('fecha')\n",
    "            return result_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al cargar el resultado del script: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def procesar_archivo(self, config_row):\n",
    "        \"\"\"\n",
    "        Procesa un archivo para una configuración dada según los parámetros del archivo Excel\n",
    "        \"\"\"\n",
    "        variable = config_row['Variable']\n",
    "        tipo_macro = config_row['Tipo Macro']\n",
    "        target_col = config_row['TARGET']\n",
    "        tipo_preprocesamiento = config_row.get('Tipo de Preprocesamiento Según la Fuente', 'Normal')\n",
    "        self.logger.info(f\"\\nProcesando: {variable} ({tipo_macro})\")\n",
    "        self.logger.info(f\"- Columna TARGET: {target_col}\")\n",
    "        self.logger.info(f\"- Tipo de preprocesamiento: {tipo_preprocesamiento}\")\n",
    "        if variable == 'US_Empire_State_Index':\n",
    "            return self.procesar_empire_state_manualmente(variable, tipo_macro, target_col)\n",
    "        elif variable == 'AAII_Investor_Sentiment':\n",
    "            return self.procesar_aaii_investor_sentiment(variable, tipo_macro, target_col)\n",
    "        elif variable == 'Put_Call_Ratio_SPY':\n",
    "            return self.procesar_put_call_ratio(variable, tipo_macro, target_col) \n",
    "        elif variable == 'Chicago_Fed_NFCI':\n",
    "            return self.procesar_chicago_fed_manualmente(variable, tipo_macro, target_col)\n",
    "        else:\n",
    "            self.logger.warning(f\"No existe procesador específico para {variable}, intentando procesamiento genérico\")\n",
    "            return self.procesar_archivo_generico(variable, tipo_macro, target_col)\n",
    "    \n",
    "    def generar_indice_diario(self):\n",
    "        \"\"\"\n",
    "        Genera un DataFrame con índice diario entre fechas mínima y máxima globales\n",
    "        \"\"\"\n",
    "        dfs_validos = {var: df for var, df in self.datos_procesados.items() if df is not None}\n",
    "        if not dfs_validos:\n",
    "            self.logger.error(\"No hay datos procesados válidos para generar el índice diario\")\n",
    "            return None\n",
    "        self.fecha_min_global = None\n",
    "        self.fecha_max_global = None\n",
    "        for variable, df in dfs_validos.items():\n",
    "            fecha_min = df['fecha'].min()\n",
    "            fecha_max = df['fecha'].max()\n",
    "            if self.fecha_min_global is None or fecha_min < self.fecha_min_global:\n",
    "                self.fecha_min_global = fecha_min\n",
    "            if self.fecha_max_global is None or fecha_max > self.fecha_max_global:\n",
    "                self.fecha_max_global = fecha_max\n",
    "        self.logger.info(\"\\nGenerando índice temporal diario...\")\n",
    "        self.logger.info(f\"- Rango de fechas global: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        todas_fechas = pd.date_range(start=self.fecha_min_global, end=self.fecha_max_global, freq='D')\n",
    "        self.indice_diario = pd.DataFrame({'fecha': todas_fechas})\n",
    "        self.logger.info(f\"- Total de fechas diarias generadas: {len(self.indice_diario)}\")\n",
    "        return self.indice_diario\n",
    "    \n",
    "    def combinar_datos(self):\n",
    "        \"\"\"\n",
    "        Combina todos los indicadores procesados con el índice diario\n",
    "        \"\"\"\n",
    "        dfs_validos = {var: df for var, df in self.datos_procesados.items() if df is not None}\n",
    "        if not dfs_validos:\n",
    "            self.logger.error(\"No hay datos procesados válidos para combinar\")\n",
    "            return None\n",
    "        if self.indice_diario is None:\n",
    "            self.logger.error(\"No se ha generado el índice diario\")\n",
    "            return None\n",
    "        self.logger.info(\"\\nCombinando datos con índice diario...\")\n",
    "        df_combinado = self.indice_diario.copy()\n",
    "        # Para cada indicador, se normaliza la fecha, se agrupa para asegurar un único registro diario y se hace merge\n",
    "        for variable, df in dfs_validos.items():\n",
    "            self.logger.info(f\"- Combinando: {variable}\")\n",
    "            df['fecha'] = pd.to_datetime(df['fecha']).dt.normalize()\n",
    "            df_unique = df.groupby('fecha', as_index=False).last()\n",
    "            df_combinado = pd.merge(df_combinado, df_unique, on='fecha', how='left')\n",
    "            for col in df_unique.columns:\n",
    "                if col != 'fecha':\n",
    "                    df_combinado[col] = df_combinado[col].ffill()\n",
    "        self.df_combinado = df_combinado\n",
    "        self.logger.info(f\"- DataFrame combinado: {len(df_combinado)} filas, {len(df_combinado.columns)} columnas\")\n",
    "        return self.df_combinado\n",
    "    \n",
    "    def analizar_cobertura_final(self):\n",
    "        \"\"\"\n",
    "        Genera un informe detallado de cobertura final\n",
    "        \"\"\"\n",
    "        stats_validos = {var: stats for var, stats in self.estadisticas.items() \n",
    "                        if var in self.datos_procesados and self.datos_procesados[var] is not None}\n",
    "        if not stats_validos or self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos combinados o estadísticas para analizar\")\n",
    "            return\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE COBERTURA FINAL\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        total_indicadores = len(stats_validos)\n",
    "        total_dias = len(self.indice_diario)\n",
    "        self.logger.info(f\"Total indicadores procesados: {total_indicadores}\")\n",
    "        self.logger.info(f\"Rango de fechas: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Total días en la serie: {total_dias}\")\n",
    "        self.logger.info(\"\\nCobertura por indicador:\")\n",
    "        for variable, stats in stats_validos.items():\n",
    "            valores_finales = 0\n",
    "            columnas_var = 0\n",
    "            for col in self.df_combinado.columns:\n",
    "                if col != 'fecha' and variable in col:\n",
    "                    valores_finales += self.df_combinado[col].notna().sum()\n",
    "                    columnas_var += 1\n",
    "            if columnas_var > 0:\n",
    "                cobertura_final = (valores_finales / (total_dias * columnas_var)) * 100\n",
    "            else:\n",
    "                cobertura_final = 0\n",
    "            self.estadisticas[variable]['cobertura_final'] = cobertura_final\n",
    "            self.logger.info(f\"- {variable}: {cobertura_final:.2f}%\")\n",
    "    \n",
    "    def generar_estadisticas_valores(self):\n",
    "        \"\"\"\n",
    "        Genera estadísticas descriptivas de los valores para cada indicador\n",
    "        \"\"\"\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos combinados para analizar\")\n",
    "            return\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"ESTADÍSTICAS DE VALORES\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        for col in self.df_combinado.columns:\n",
    "            if col == 'fecha':\n",
    "                continue\n",
    "            serie = self.df_combinado[col].dropna()\n",
    "            if len(serie) == 0:\n",
    "                self.logger.warning(f\"La columna {col} no tiene valores\")\n",
    "                continue\n",
    "            stats = {\n",
    "                'min': serie.min(),\n",
    "                'max': serie.max(),\n",
    "                'mean': serie.mean(),\n",
    "                'median': serie.median(),\n",
    "                'std': serie.std()\n",
    "            }\n",
    "            self.logger.info(f\"\\nEstadísticas para {col}:\")\n",
    "            self.logger.info(f\"- Min: {stats['min']}\")\n",
    "            self.logger.info(f\"- Max: {stats['max']}\")\n",
    "            self.logger.info(f\"- Media: {stats['mean']}\")\n",
    "            self.logger.info(f\"- Mediana: {stats['median']}\")\n",
    "            self.logger.info(f\"- Desv. Estándar: {stats['std']}\")\n",
    "            variable = next((var for var in self.estadisticas.keys() if var in col), None)\n",
    "            if variable:\n",
    "                col_stats = {f\"{col}_min\": stats['min'],\n",
    "                             f\"{col}_max\": stats['max'],\n",
    "                             f\"{col}_mean\": stats['mean'],\n",
    "                             f\"{col}_median\": stats['median'],\n",
    "                             f\"{col}_std\": stats['std']}\n",
    "                self.estadisticas[variable].update(col_stats)\n",
    "    \n",
    "    def guardar_resultados(self, output_file='datos_economicos_other_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Guarda los resultados procesados en un archivo Excel\n",
    "        \"\"\"\n",
    "        if self.df_combinado is None:\n",
    "            self.logger.error(\"No hay datos para guardar\")\n",
    "            return False\n",
    "        try:\n",
    "            self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "            self.logger.info(\"GUARDANDO RESULTADOS\")\n",
    "            self.logger.info(\"=\" * 50)\n",
    "            self.logger.info(f\"Guardando resultados en: {output_file}\")\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                self.logger.info(f\"Guardando DataFrame combinado en '{output_file}'...\")\n",
    "                self.df_combinado.to_excel(writer, sheet_name='Datos_Combinados', index=False)\n",
    "                self.logger.info(\"Guardando estadísticas de los indicadores...\")\n",
    "                stats_data = []\n",
    "                for variable, stats in self.estadisticas.items():\n",
    "                    row = {\n",
    "                        'Variable': variable,\n",
    "                        'Tipo_Macro': stats.get('tipo_macro', ''),\n",
    "                        'Columna_TARGET': stats.get('columna_target', ''),\n",
    "                        'Total_Filas_Original': stats.get('total_filas', 0),\n",
    "                        'Valores_Validos_Original': stats.get('valores_validos', 0),\n",
    "                        'Cobertura_Final_%': stats.get('cobertura_final', 0),\n",
    "                        'Fecha_Min': stats.get('fecha_min', ''),\n",
    "                        'Fecha_Max': stats.get('fecha_max', '')\n",
    "                    }\n",
    "                    for k, v in stats.items():\n",
    "                        if any(k.startswith(f\"{col}_\") for col in ['min', 'max', 'mean', 'median', 'std']):\n",
    "                            row[k] = v\n",
    "                    stats_data.append(row)\n",
    "                df_stats = pd.DataFrame(stats_data)\n",
    "                df_stats.to_excel(writer, sheet_name='Estadisticas', index=False)\n",
    "                metadata = {\n",
    "                    'Proceso': ['OtherDataProcessor'],\n",
    "                    'Fecha de proceso': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "                    'Total indicadores': [len(self.estadisticas)],\n",
    "                    'Periodo': [f\"{self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\"],\n",
    "                    'Total días': [len(self.indice_diario)]\n",
    "                }\n",
    "                pd.DataFrame(metadata).to_excel(writer, sheet_name='Metadatos')\n",
    "                if self.config_data is not None:\n",
    "                    self.config_data.to_excel(writer, sheet_name='Configuración', index=False)\n",
    "            self.logger.info(f\"Archivo guardado exitosamente: {output_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al guardar resultados: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def ejecutar_proceso_completo(self, output_file='datos_economicos_other_procesados.xlsx'):\n",
    "        \"\"\"\n",
    "        Ejecuta el proceso completo de preprocesamiento de forma paramétrica basada en Data Engineering.xlsx\n",
    "        \"\"\"\n",
    "        inicio = time.time()\n",
    "        self.logger.info(\"Iniciando proceso completo OtherDataProcessor...\")\n",
    "        self.leer_configuracion()\n",
    "        if self.config_data is None or len(self.config_data) == 0:\n",
    "            return False\n",
    "        self.logger.info(\"\\nResumen de configuraciones a procesar:\")\n",
    "        for idx, row in self.config_data.iterrows():\n",
    "            self.logger.info(f\"- {row['Variable']} (Tipo Macro: {row['Tipo Macro']}, TARGET: {row['TARGET']})\")\n",
    "        for _, config_row in self.config_data.iterrows():\n",
    "            variable, df_procesado = self.procesar_archivo(config_row)\n",
    "            self.datos_procesados[variable] = df_procesado\n",
    "        archivos_correctos = sum(1 for df in self.datos_procesados.values() if df is not None)\n",
    "        if archivos_correctos == 0:\n",
    "            self.logger.error(\"No se pudo procesar correctamente ningún archivo\")\n",
    "            return False\n",
    "        self.logger.info(f\"\\nArchivos procesados correctamente: {archivos_correctos}/{len(self.datos_procesados)}\")\n",
    "        self.generar_indice_diario()\n",
    "        if self.indice_diario is None:\n",
    "            return False\n",
    "        self.combinar_datos()\n",
    "        if self.df_combinado is None:\n",
    "            return False\n",
    "        self.analizar_cobertura_final()\n",
    "        self.generar_estadisticas_valores()\n",
    "        resultado = self.guardar_resultados(output_file)\n",
    "        fin = time.time()\n",
    "        tiempo_ejecucion = fin - inicio\n",
    "        self.logger.info(\"\\n\" + \"=\" * 50)\n",
    "        self.logger.info(\"RESUMEN DE EJECUCIÓN\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        self.logger.info(f\"Proceso: OtherDataProcessor\")\n",
    "        self.logger.info(f\"Tiempo de ejecución: {tiempo_ejecucion:.2f} segundos\")\n",
    "        self.logger.info(f\"Archivos procesados: {len(self.datos_procesados)}\")\n",
    "        self.logger.info(f\"Archivos con error: {sum(1 for df in self.datos_procesados.values() if df is None)}\")\n",
    "        self.logger.info(f\"Archivos procesados correctamente: {archivos_correctos}\")\n",
    "        self.logger.info(f\"Periodo de datos: {self.fecha_min_global.strftime('%Y-%m-%d')} a {self.fecha_max_global.strftime('%Y-%m-%d')}\")\n",
    "        self.logger.info(f\"Datos combinados: {len(self.df_combinado)} filas, {len(self.df_combinado.columns)} columnas\")\n",
    "        self.logger.info(f\"Archivo de salida: {output_file}\")\n",
    "        self.logger.info(f\"Estado: {'COMPLETADO' if resultado else 'ERROR'}\")\n",
    "        self.logger.info(\"=\" * 50)\n",
    "        return resultado\n",
    "\n",
    "# Función principal para ejecutar el proceso\n",
    "def ejecutar_otherdataprocessor(config_file='Data Engineering.xlsx',\n",
    "                                output_file='datos_economicos_other_procesados.xlsx',\n",
    "                                data_root='data/Macro/raw',\n",
    "                                log_file='otherdataprocessor.log'):\n",
    "    \"\"\"\n",
    "    Ejecuta el proceso OtherDataProcessor\n",
    "    \n",
    "    Args:\n",
    "        config_file (str): Ruta al archivo de configuración\n",
    "        output_file (str): Ruta al archivo de salida\n",
    "        data_root (str): Directorio raíz donde se encuentran los subdirectorios de datos\n",
    "        log_file (str): Ruta al archivo de log\n",
    "        \n",
    "    Returns:\n",
    "        bool: True si el proceso se completó exitosamente, False en caso contrario\n",
    "    \"\"\"\n",
    "    procesador = OtherDataProcessor(config_file, data_root, log_file)\n",
    "    return procesador.ejecutar_proceso_completo(output_file)\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    resultado = ejecutar_otherdataprocessor()\n",
    "    print(f\"Proceso {'completado exitosamente' if resultado else 'finalizado con errores'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorizar Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 12:10:21,602 [INFO] Archivo Excel cargado exitosamente.\n",
      "2025-03-31 12:10:21,603 [INFO] Iniciando la categorización de columnas...\n",
      "2025-03-31 12:10:21,606 [INFO] Proceso completado exitosamente.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnas encontradas para Sin categoría:\n",
      "fecha\n",
      "\n",
      "columnas encontradas para bond:\n",
      "PRICE_Australia_10Y_Bond_bond\n",
      "PRICE_Italy_10Y_Bond_bond\n",
      "PRICE_Japan_10Y_Bond_bond\n",
      "PRICE_UK_10Y_Bond_bond\n",
      "PRICE_Germany_10Y_Bond_bond\n",
      "PRICE_Canada_10Y_Bond_bond\n",
      "PRICE_China_10Y_Bond_bond\n",
      "DGS10_US_10Y_Treasury_bond\n",
      "DGS2_US_2Y_Treasury_bond\n",
      "AAA_Corporate_Bond_AAA_Spread_bond\n",
      "BAA10YM_Corporate_Bond_BBB_Spread_bond\n",
      "BAMLH0A0HYM2_High_Yield_Bond_Spread_bond\n",
      "\n",
      "columnas encontradas para commodities:\n",
      "PRICE_CrudeOil_WTI_commodities\n",
      "PRICE_Gold_Spot_commodities\n",
      "PRICE_Silver_Spot_commodities\n",
      "PRICE_Copper_Futures_commodities\n",
      "PRICE_Platinum_Spot_commodities\n",
      "\n",
      "columnas encontradas para exchange_rate:\n",
      "PRICE_EUR_USD_Spot_exchange_rate\n",
      "PRICE_GBP_USD_Spot_exchange_rate\n",
      "PRICE_JPY_USD_Spot_exchange_rate\n",
      "PRICE_CNY_USD_Spot_exchange_rate\n",
      "PRICE_AUD_USD_Spot_exchange_rate\n",
      "PRICE_CAD_USD_Spot_exchange_rate\n",
      "PRICE_MXN_USD_Spot_exchange_rate\n",
      "PRICE_EUR_GBP_Cross_exchange_rate\n",
      "\n",
      "columnas encontradas para index_pricing:\n",
      "ULTIMO_S&P500_Index_index_pricing\n",
      "ULTIMO_NASDAQ_Composite_index_pricing\n",
      "ULTIMO_Russell_2000_index_pricing\n",
      "ULTIMO_FTSE_100_index_pricing\n",
      "ULTIMO_Nikkei_225_index_pricing\n",
      "ULTIMO_DAX_30_index_pricing\n",
      "PRICE_Shanghai_Composite_index_pricing\n",
      "ULTIMO_VIX_VolatilityIndex_index_pricing\n",
      "Price_Dollar_Index_DXY_index_pricing\n",
      "\n",
      "columnas encontradas para business_confidence:\n",
      "ESI_GACDISA_US_Empire_State_Index_business_confidence\n",
      "ESI_AWCDISA_US_Empire_State_Index_business_confidence\n",
      "Actual_US_ISM_Manufacturing_business_confidence\n",
      "Actual_US_ISM_Services_business_confidence\n",
      "Actual_US_Philly_Fed_Index_business_confidence\n",
      "Actual_France_Business_Climate_business_confidence\n",
      "Actual_EuroZone_Business_Climate_business_confidence\n",
      "\n",
      "columnas encontradas para consumer_confidence:\n",
      "AAII_Bearish_AAII_Investor_Sentiment_consumer_confidence\n",
      "AAII_Bull-Bear Spread_AAII_Investor_Sentiment_consumer_confidence\n",
      "AAII_Bullish_AAII_Investor_Sentiment_consumer_confidence\n",
      "PutCall_strike_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_bid_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_ask_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_vol_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_delta_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_gamma_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_theta_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_vega_Put_Call_Ratio_SPY_consumer_confidence\n",
      "PutCall_rho_Put_Call_Ratio_SPY_consumer_confidence\n",
      "Actual_US_Consumer_Confidence_consumer_confidence\n",
      "CSCICP02EZM460S_EuroZone_Consumer_Confidence_consumer_confidence\n",
      "CSCICP02CHQ460S_Switzerland_Consumer_Confidence_consumer_confidence\n",
      "UMCSENT_Michigan_Consumer_Sentiment_consumer_confidence\n",
      "\n",
      "columnas encontradas para leading_economic_index:\n",
      "NFCI_Chicago_Fed_NFCI_leading_economic_index\n",
      "ANFCI_Chicago_Fed_NFCI_leading_economic_index\n",
      "Actual_US_ConferenceBoard_LEI_leading_economic_index\n",
      "Actual_Japan_Leading_Indicator_leading_economic_index\n",
      "\n",
      "columnas encontradas para economics:\n",
      "Actual_China_PMI_Manufacturing_economics\n",
      "CPIAUCSL_US_CPI_economics\n",
      "CPILFESL_US_Core_CPI_economics\n",
      "PCE_US_PCE_economics\n",
      "PCEPILFE_US_Core_PCE_economics\n",
      "PPIACO_US_PPI_economics\n",
      "INDPRO_US_Industrial_Production_MoM_economics\n",
      "CSUSHPINSA_US_CaseShiller_HomePrice_economics\n",
      "GDP_US_GDP_Growth_economics\n",
      "TCU_US_Capacity_Utilization_economics\n",
      "PERMIT_US_Building_Permits_economics\n",
      "HOUST_US_Housing_Starts_economics\n",
      "FEDFUNDS_US_FedFunds_Rate_economics\n",
      "ECBDFR_ECB_Deposit_Rate_economics\n",
      "WALCL_Fed_Balance_Sheet_economics\n",
      "\n",
      "columnas encontradas para car_registrations:\n",
      "DNKSLRTCR03GPSAM_Denmark_Car_Registrations_MoM_car_registrations\n",
      "USASLRTCR03GPSAM_US_Car_Registrations_MoM_car_registrations\n",
      "ZAFSLRTCR03GPSAM_SouthAfrica_Car_Registrations_MoM_car_registrations\n",
      "GBRSLRTCR03GPSAM_United_Kingdom_Car_Registrations_MoM_car_registrations\n",
      "ESPSLRTCR03GPSAM_Spain_Car_Registrations_MoM_car_registrations\n",
      "\n",
      "columnas encontradas para comm_loans:\n",
      "BUSLOANS_US_Commercial_Loans_comm_loans\n",
      "CREACBM027NBOG_US_RealEstate_Commercial_Loans_comm_loans\n",
      "TOTALSL_US_Consumer_Credit_comm_loans\n",
      "\n",
      "columnas encontradas para unemployment_rate:\n",
      "PRICE_US_Unemployment_Rate_unemployment_rate\n",
      "PRICE_US_Nonfarm_Payrolls_unemployment_rate\n",
      "PRICE_US_Initial_Jobless_Claims_unemployment_rate\n",
      "PRICE_US_JOLTS_unemployment_rate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración básica de logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Definición de patrones regex para cada categoría (buscando al final de la cadena)\n",
    "regex_patterns = {\n",
    "    'bond': re.compile(r\"_bond$\"),\n",
    "    'business_confidence': re.compile(r\"_business_confidence$\"),\n",
    "    'car_registrations': re.compile(r\"_car_registrations$\"),\n",
    "    'comm_loans': re.compile(r\"_comm_loans$\"),\n",
    "    'commodities': re.compile(r\"_commodities$\"),\n",
    "    'consumer_confidence': re.compile(r\"_consumer_confidence$\"),\n",
    "    'economics': re.compile(r\"_economics$\"),\n",
    "    'exchange_rate': re.compile(r\"_exchange_rate$\"),\n",
    "    'exports': re.compile(r\"_exports$\"),\n",
    "    'index_pricing': re.compile(r\"_index_pricing$\"),\n",
    "    'leading_economic_index': re.compile(r\"_leading_economic_index$\"),\n",
    "    'unemployment_rate': re.compile(r\"_unemployment_rate$\")\n",
    "}\n",
    "\n",
    "def categorize_column(col_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Determina la categoría de una columna basándose en el patrón que aparece al final del nombre.\n",
    "    Retorna la categoría si hay coincidencia o \"Sin categoría\" en caso contrario.\n",
    "    \"\"\"\n",
    "    for category, pattern in regex_patterns.items():\n",
    "        if pattern.search(col_name):\n",
    "            return category\n",
    "    return \"Sin categoría\"\n",
    "\n",
    "def main():\n",
    "    # Intentar leer el archivo Excel\n",
    "    try:\n",
    "        df = pd.read_excel(\"MERGEDEXCELS.xlsx\")\n",
    "        columns = df.columns.tolist()\n",
    "        logging.info(\"Archivo Excel cargado exitosamente.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error al leer el archivo Excel: %s\", e)\n",
    "        return\n",
    "\n",
    "    logging.info(\"Iniciando la categorización de columnas...\")\n",
    "\n",
    "    # Agrupar columnas por categoría\n",
    "    grouped_columns = {}\n",
    "    for col in columns:\n",
    "        category = categorize_column(col)\n",
    "        grouped_columns.setdefault(category, []).append(col)\n",
    "\n",
    "    # Imprimir el resultado en el formato solicitado\n",
    "    for category, cols in grouped_columns.items():\n",
    "        print(f\"columnas encontradas para {category}:\")\n",
    "        for col in cols:\n",
    "            print(col)\n",
    "        print()  # Línea en blanco para separar grupos\n",
    "\n",
    "    logging.info(\"Proceso completado exitosamente.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
