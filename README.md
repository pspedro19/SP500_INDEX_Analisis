# ğŸ“Š SP500 Index Analysis - Data Transformation Pipeline

[![CI](https://github.com/pspedro19/SP500_INDEX_Analisis/actions/workflows/ci.yml/badge.svg)](https://github.com/pspedro19/SP500_INDEX_Analisis/actions/workflows/ci.yml)

Este proyecto aplica una serie de transformaciones modulares y secuenciales sobre datos macroeconÃ³micos para generar datasets entrenables para modelos predictivos, orientados al anÃ¡lisis del Ã­ndice S&P500.

El proceso inicia con `pipelines/ml/00_step_preprocess.py`, que **no se ejecuta**
automÃ¡ticamente al correr `run_pipeline.py`. Este paso debe lanzarse de forma
manual o a travÃ©s del comando `sp500 preprocess`. Posteriormente,
`run_pipeline.py` orquesta los pasos 1â€“10 (incluido el 7.5) desde la carpeta
`pipelines/`.

---

## ğŸ“ Estructura del Proyecto

```
SP500_index_analysis/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/        # Datos crudos originales (0_raw)
â”‚   â”œâ”€â”€ interim/    # Transformaciones intermedias (1_preprocess*)
â”‚   â”œâ”€â”€ features/   # Insumos para modelado (2_model_input)
â”‚   â”œâ”€â”€ processed/  # Datos limpios o finales
â”‚   â””â”€â”€ samples/    # Subconjuntos de prueba
â”‚   # Al ejecutar el pipeline se crearÃ¡n `3_trainingdata/`,
â”‚   # `4_results/` y `5_metrics/` en esta carpeta.
â”‚
â”œâ”€â”€ pipelines/      # Scripts de procesamiento por paso
â”œâ”€â”€ notebooks/      # Jupyter notebooks exploratorios
â”œâ”€â”€ logs/           # Registros de ejecuciÃ³n
â”œâ”€â”€ run_pipeline.py # Orquestador principal del pipeline
â”œâ”€â”€ .gitignore
â””â”€â”€ requirements.txt
```

Las carpetas `models/` y `data/4_results/` se generan automÃ¡ticamente
al ejecutar el pipeline y no estÃ¡n versionadas.

---

## â–¶ï¸ EjecuciÃ³n del pipeline

```bash
python run_pipeline.py
```

### Uso de la CLI

Instala el paquete en modo editable y luego ejecuta los comandos `sp500` para
invocar los distintos servicios:

```bash
sp500 preprocess  # preprocesamiento de datos
sp500 train       # entrenamiento de modelos
sp500 infer       # generar predicciones
sp500 backtest   # ejecutar backtests
```

Para ver todas las opciones disponibles tambiÃ©n puedes ejecutar:

```bash
sp500 --help
```
### Ejecutar pasos individualmente

Cada script puede ejecutarse por separado:
```bash
python pipelines/ml/00_step_preprocess.py                 # Paso 0
python pipelines/ml/01_step_merge_excels.py               # Paso 1
python pipelines/ml/02_step_generate_categories.py        # Paso 2
python pipelines/ml/03_step_clean_columns.py              # Paso 3
python pipelines/ml/04_step_transform_features.py         # Paso 4
python pipelines/ml/05_step_remove_relations.py           # Paso 5
python pipelines/ml/06_step_fpi_selection.py              # Paso 6
python pipelines/ml/06a_step_filtro_20days.py             # Paso 6a
python pipelines/ml/07_step_train_models.py               # Paso 7
python pipelines/ml/07a_step_apply_inverse_transform.py   # Paso 7a
python pipelines/ml/07b_step_ensemble.py                  # Paso 7b
python pipelines/ml/07c_step_Calculo_Valor_SP500.py       # Paso 7c
python pipelines/ml/07d_step_Transform_to_PowerBI.py      # Paso 7d
python pipelines/ml/08_step_prepare_output.py             # Paso 8
python pipelines/ml/09_step_backtest.py                   # Paso 9
python pipelines/ml/10_step_inference.py                  # Paso 10
```

---

## ğŸ§© DescripciÃ³n de cada paso

### ğŸŸ£ Paso 0 - Preprocesamiento Inicial
**Script:** `pipelines/ml/00_step_preprocess.py`
- Limpia estructuras base y normaliza nombres y fechas.  
- **Input:** Archivos `.xlsx` o `.csv` de `data/raw/`  
- **Output:** Archivos estandarizados a `data/processed/`

---

### ğŸ”µ Paso 1 - UniÃ³n de Archivos
**Script:** `pipelines/ml/01_step_merge_excels.py`
- Une mÃºltiples archivos Excel en uno solo.  
- **Input:** Archivos macroeconÃ³micos  
- **Output:** `MERGEDEXCELS.xlsx`

---

### ğŸ”µ Paso 2 - GeneraciÃ³n de CategorÃ­as
**Script:** `pipelines/ml/02_step_generate_categories.py`
- Clasifica columnas en categorÃ­as econÃ³micas.  
- **Output:** `MERGEDEXCELS_CATEGORIZADO.xlsx`

---

### ğŸ”µ Paso 3 - Limpieza de Nombres de Columnas
**Script:** `pipelines/ml/03_step_clean_columns.py`
- Elimina redundancias y mejora la trazabilidad de variables.  
- **Output:** `MERGEDEXCELS_CATEGORIZADO_LIMPIO.xlsx`

---

### ğŸŸ  Paso 4 - Transformaciones e Indicadores
**Script:** `pipelines/ml/04_step_transform_features.py`
- Aplica indicadores tÃ©cnicos como:
  - MoM, YoY, medias mÃ³viles, z-score, log-retornos, RSI, Bollinger Bands.  
- **Output:** Datos enriquecidos por categorÃ­a.

---

### ğŸŸ  Paso 5 - EliminaciÃ³n de Relaciones Redundantes
**Script:** `pipelines/ml/05_step_remove_relations.py`
- Elimina multicolinealidad con VIF, y variables con alta correlaciÃ³n o baja varianza.  
- **Output:** Dataset reducido.

---

### ğŸŸ¡ Paso 6 - SelecciÃ³n de Variables Relevantes (FPI)
**Script:** `pipelines/ml/06_step_fpi_selection.py`
- Selecciona variables clave usando Feature Permutation Importance con CatBoost y validaciÃ³n temporal.
- **Output:** `EUR_final_FPI.xlsx`

---

### ğŸŸ¡ Paso 6a - Filtro de 20 dÃ­as
**Script:** `pipelines/ml/06a_step_filtro_20days.py`
- Reduce el ruido eliminando filas muy cercanas en el tiempo.
- **Output:** Dataset filtrado.

---

### ğŸŸ¢ Paso 7 - Entrenamiento de Modelos
**Script:** `pipelines/ml/07_step_train_models.py`
- Entrena y optimiza modelos CatBoost, LightGBM, XGBoost, MLP, SVM con Optuna.
- **Output:**
  - Modelos `.pkl` en `models/` (carpeta creada al ejecutar el pipeline)
  - Predicciones en `data/4_results/all_models_predictions.csv`

### ğŸŸ¢ Paso 7a - Aplicar TransformaciÃ³n Inversa
**Script:** `pipelines/ml/07a_step_apply_inverse_transform.py`
- Restaura las predicciones a su escala original.
- **Output:** `predicciones_reales.csv`

### ğŸŸ¢ Paso 7b - Ensamble de Modelos
**Script:** `pipelines/ml/07b_step_ensemble.py`
- Combina las predicciones de los modelos base con un enfoque greedy.
- **Output:** `ensemble_greedy.pkl` y `ensemble_info.json`

### ğŸŸ¢ Paso 7c - CÃ¡lculo del Valor del S&P500
**Script:** `pipelines/ml/07c_step_Calculo_Valor_SP500.py`
- Convierte los retornos pronosticados en valores del Ã­ndice.
- **Output:** `valor_sp500.csv`

### ğŸŸ¢ Paso 7d - Formato compatible con Power BI
**Script:** `pipelines/ml/07d_step_Transform_to_PowerBI.py`
- Adapta los CSV al formato regional espaÃ±ol para Power BI.
- **Output:** `data/4_results/archivo_powerbi_es.csv`

---

### ğŸŸ¢ Paso 8 - PreparaciÃ³n de Resultados para Dashboard
**Script:** `pipelines/ml/08_step_prepare_output.py`
- Convierte los resultados a formato `.csv` compatible con Power BI (formato espaÃ±ol).
- **Output:** `data/4_results/archivo_para_powerbi.csv`

### ğŸŸ¢ Paso 9 - Backtest de Estrategias
**Script:** `pipelines/ml/09_step_backtest.py`
- EvalÃºa el desempeÃ±o histÃ³rico de las predicciones.
- **Output:** mÃ©tricas y grÃ¡ficos en `data/5_metrics/`.

### ğŸŸ¢ Paso 10 - Inferencia
**Script:** `pipelines/ml/10_step_inference.py`
- Genera pronÃ³sticos usando los modelos entrenados.
- **Output:** `predictions_api.json` y visualizaciones de forecast.

---

## ğŸ›¡ï¸ Seguridad y control de versiones

- Variables sensibles estÃ¡n definidas en `.env` (excluido con `.gitignore`)
- Los datos, los modelos y los logs generados se excluyen del versionado
- Usa ramas `feature/` o `refactor/` para nuevas funcionalidades

---

## âš™ï¸ Requisitos

```bash
pip install -r requirements.txt
```

---

## âœ… IntegraciÃ³n continua

Este proyecto utiliza **GitHub Actions** para ejecutar automÃ¡ticamente las
pruebas y las tareas de linting en cada commit. El flujo de trabajo principal se
encuentra en `.github/workflows/ci.yml` y ejecuta `pip install -e .[dev]`,
`ruff check .`, `black --check .` y `pytest`.

### Ejecutar pruebas localmente

Para lanzar la misma suite de pruebas que corre en CI, desde la raÃ­z del
repositorio ejecuta:

```bash
make test
```

---

## ğŸ§  Autor

Proyecto desarrollado por Pedro para anÃ¡lisis avanzado del Ã­ndice S&P500 usando datos macroeconÃ³micos estructurados y modelado basado en aprendizaje automÃ¡tico.

## âŒ Scripts eliminados

El script `pipelines/ml/limpiar_ultimo_nan.py` se eliminÃ³ del repositorio al no formar parte del flujo de procesamiento actual.
